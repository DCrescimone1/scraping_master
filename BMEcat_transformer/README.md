# BMEcat_transformer â€“ DABAG Spec Scraper

## Overview

Extracts product IDs (SUPPLIER_PID) from BMEcat XML files, searches DABAG.ch, scrapes product pages in DE/FR/IT, extracts specification tables, prints formatted tables in the terminal, and saves results to JSON for future XML transformation.

## Features

- **BMEcat SUPPLIER_PID extraction** from local XML files
- **Configurable scraping backend**: Firecrawl or Playwright
- **Multi-language scraping**: de, fr, it
- **Specification table parsing** via BeautifulSoup
- **Readable terminal tables** using `tabulate`
- **JSON output** saved to `outputs/`
- **Graceful error handling** with warnings
- **XML UDX specs extraction** from Original XML (`UDX.EDXF.*` fields)
- **AI-powered feature matching** (optional, via Grok) maps unstructured XML text to structured features

## Requirements

- Python 3.9+
- See project-level `requirements.txt` for dependencies

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

Set values in root `.env` and/or environment variables. See `BMEcat_transformer/config.py`.

Environment:

```bash
# Required for Firecrawl mode
FIRECRAWL_API_KEY=fc-your-key

# Optional overrides
SCRAPING_METHOD=firecrawl   # or: playwright
DABAG_BASE_URL=https://www.dabag.ch
BME_OUTPUT_DIR=outputs/

# Optional: Enable AI-powered XML specs extraction/matching
GROK_API_KEY=xa-your-key
GROK_MODEL=grok-4-fast-reasoning
```

Selection logic:
- `SCRAPING_METHOD=firecrawl` â†’ uses `easy_rich/src/web_scraper.py`
- `SCRAPING_METHOD=playwright` â†’ uses `manual_scrape/src/web_scraper.py`

## Usage

From `BMEcat_transformer/` directory:

```bash
# Main scraper
python3 scripts/main.py /path/to/your_bmecat.xml

# Or JSON input with a simple array of SUPPLIER_PIDs
python3 scripts/main.py /path/to/manual_ids.json

# Feature extraction
python3 scripts/extract_features.py

# Testing imports
python3 test_imports.py
```

You can also run without args and enter the path when prompted.

### Input File Formats

- **XML Format**: Extracts `SUPPLIER_PID` values from BMEcat XML files (see `core/xml_reader.py` for details).
- **JSON Format**: Simple array of strings. Example:

```json
[
  "ID1",
  "ID2",
  "ID3"
]
```

## Product ID Extraction

The system extracts product identifiers from BMEcat XML files with the following priority:

1. **SUPPLIER_AID** (checked first)
2. **SUPPLIER_PID** (fallback if AID not found)

Both tags are treated as equivalent product identifiers. The extractor:
- Uses regex for robustness with malformed XML
- Deduplicates automatically
- Preserves order of first appearance
- Works with or without XML namespaces

**Example XML tags recognized:**
```xml
<SUPPLIER_AID>207603216S</SUPPLIER_AID>
<SUPPLIER_PID>DCG405NT-XJ</SUPPLIER_PID>
```

Note: Variable names in code still use `SUPPLIER_PID` for backward compatibility.

## Output

- Terminal shows a per-product table with columns `[Spec Label, DE, FR, IT]`.
- A timestamped JSON file is saved to `outputs/`.

Example JSON structure:

```json
{
  "PROD123": {
    "SUPPLIER_PID": "PROD123",
    "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=...",
    "languages": {
      "de": {"Leistung": "1200 W"},
      "fr": {"Puissance": "1200 W"},
      "it": {"Potenza": "1200 W"}
    }
  }
}
```

## Master JSON Tracking

The scraper maintains a persistent `master_bmecat_dabag.json` file in the `outputs/` directory that accumulates all scraped products across runs.

### How It Works

1. **First Run**: All products are scraped and added to the master JSON
2. **Subsequent Runs**:
   - If a product ID already exists, you'll see a comparison and be prompted to update or skip
   - New products are automatically added
   - Updates are tracked with timestamps

### Master JSON Structure

```json
{
  "metadata": {
    "created_at": "2025-10-16T10:00:00",
    "last_updated": "2025-10-16T14:30:00",
    "total_products": 150
  },
  "products": {
    "PRODUCT_ID": {
      "scraped_at": "2025-10-16T10:15:00",
      "updated_at": "2025-10-16T14:25:00",
      "product_url": "https://...",
      "languages": {
        "de": { },
        "fr": { },
        "it": { }
      }
    }
  }
}
```

### Backup System

- Automatically maintains 2 backup versions of the master JSON
- Backups are named: `master_bmecat_dabag.json.backup1` and `master_bmecat_dabag.json.backup2`
- Each save rotates backups automatically

### Per-Run Exports

In addition to the master JSON, each run still creates a timestamped export file (e.g., `bmecat_dabag_results_20251016_104334.json`) containing only the products processed in that specific run.

### Configuration

You can customize master JSON behavior in `.env`:

```bash
MASTER_JSON_FILENAME=master_bmecat_dabag.json
MASTER_JSON_BACKUP_COUNT=2
```

## Project Structure

```
BMEcat_transformer/
â”œâ”€â”€ config.py
â”œâ”€â”€ README.md
â”œâ”€â”€ test_imports.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ extract_features.py
â”‚
â”œâ”€â”€ inputs/
â”‚   â””â”€â”€ example_supplier_ids.json
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ input_handler.py
â”‚   â”œâ”€â”€ xml_reader.py
â”‚   â””â”€â”€ master_json_manager.py
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dabag_scraper.py
â”‚   â””â”€â”€ table_extractor.py
â”‚
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ feature_extractor.py
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ output_formatter.py
â”‚
â””â”€â”€ ui/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ user_prompt.py
```

## Dependencies

Managed at project root in `requirements.txt`:

- `beautifulsoup4`, `tabulate`, `requests`, `python-dotenv`
- `firecrawl-py` (for Firecrawl mode)
- `playwright` (for Playwright mode)
- `pyyaml>=6.0.1` (AI prompt configuration)

## Logging

The project uses a centralized logging system via `utils/logger.py`.

**Usage:**
```python
from utils.logger import setup_logger

logger = setup_logger(__name__)
logger.info("Information message")
logger.debug("Debug message")
logger.warning("Warning message")
logger.error("Error message")
```

**Log Levels:**
- DEBUG: Detailed diagnostic information
- INFO: General informational messages (default)
- WARNING: Warning messages
- ERROR: Error messages

**Configuration:**
- Logs are written to stdout by default.
- To enable file logging:
```python
logger = setup_logger(__name__, log_file="logs/app.log")
```

## ðŸ”§ Feature Name Extractor

Extract a database of unique feature names (fname) across all languages from the master JSON.

### Purpose
Creates a reference CSV file showing how each product feature is named in German (de), French (fr), and Italian (it).

### Usage

```bash
cd BMEcat_transformer
python3 scripts/extract_features.py
```

### Output
- **File**: `outputs/unique_features.csv`
- **Format**: 6 columns (fname_de, fvalue_de, fname_fr, fvalue_fr, fname_it, fvalue_it)
- **Content**: Unique feature mappings with example values

### Example Output
```csv
fname_de,fvalue_de,fname_fr,fvalue_fr,fname_it,fvalue_it
Spannung,18 V,tension,18 V,tensione,18 V
Akku-Typ,--,type d'accu,--,tipo di batteria,--
```

### Notes
- Features are matched by position (assumes same order across languages)
- Only first occurrence is stored as example value
- Requires `master_bmecat_dabag.json` to exist (run main scraper first)

## Comparison Table Generation

Generate comparison tables merging Original XML (DEWALT), DABAG XML (multi-language), web-scraped data (from master JSON), and AI mappings (placeholder).

### Usage

```bash
python3 scripts/create_comparison_tables.py \
  --original data/DEWALT_BMEcat_Original.xml \
  --dabag data/DEWALT_Version_DABAG.xml

# With auto-scraping
python3 scripts/create_comparison_tables.py \
  --original data/DEWALT_BMEcat_Original.xml \
  --dabag data/DEWALT_Version_DABAG.xml \
  --auto-scrape
```

### Input Requirements

- `original_xml`: BMEcat XML (original source). Features read from `FDESCR`, `FVALUE`, `FUNIT` per `<FEATURE>` within `<PRODUCT>` by `SUPPLIER_PID`.
- `dabag_xml`: BMEcat XML (DABAG enriched). Features read per language via `FNAME lang=deu|fra|ita`, `FVALUE lang=...`, and `FUNIT`.
- Optional `--auto-scrape`: If enabled, missing `SUPPLIER_PID`s in the master JSON will be scraped using `scrapers/dabag_scraper.py` and appended/updated.

### Output

- Individual tables:
  - Directory: `outputs/comparison_tables/`
  - File: `comparison_{supplier_id}_{lang}.json`
  - Columns: `original_fname, original_fvalue, dabag_fname, dabag_fvalue, web_fname, web_fvalue, ai_fname, ai_fvalue`
  - Units are saved separately per row (`original_funit`, `dabag_funit`).
- Master catalog:
  - File: `outputs/comparison_tables/master_comparison_catalog.json`
  - Structure:

```json
{
  "products": {
    "SUPPLIER_PID": {
      "languages": {
        "de": { "columns": [...], "rows": [...], "units": [...] },
        "fr": { "columns": [...], "rows": [...], "units": [...] },
        "it": { "columns": [...], "rows": [...], "units": [...] }
      }
    }
  }
}
```

### Example

After running the command, expect per-language files like:

```
outputs/comparison_tables/comparison_123456_de.json
outputs/comparison_tables/comparison_123456_fr.json
outputs/comparison_tables/comparison_123456_it.json
```

and a consolidated:

```
outputs/comparison_tables/master_comparison_catalog.json
```

