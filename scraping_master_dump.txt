Folder structure:
.
‚îú‚îÄ‚îÄ BMEcat_transformer
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ core
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_table_builder.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ input_handler.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ master_json_manager.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ xml_readers
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ dabag_xml_reader.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ original_supplier_id_extractor.py
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ original_xml_reader.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ count_products.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ inputs
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ example_supplier_ids.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ output
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ output_formatter.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processors
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ai_feature_matcher.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ feature_extractor.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ feature_mapper.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ xml_specs_extractor.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ prompts
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ xml_specs_mapping.yaml
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ scrapers
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dabag_scraper.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ table_extractor.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ scripts
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ create_comparison_tables.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ extract_features_StandAlone.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_debug.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_debug2.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_imports.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ test_original_xml.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ui
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ user_prompt.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualize_tables.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ dabag_ch_20251016_161607
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ www_dabag_ch__data.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ www_dabag_ch_.md
‚îú‚îÄ‚îÄ doc_processor
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ bmecat_parser.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ firecrawl_parser.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ json_generator.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ llm_processor.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ pdf_handler.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ user_prompt.py
‚îú‚îÄ‚îÄ easy_rich
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ serp_api_client.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ web_scraper.py
‚îú‚îÄ‚îÄ manual_scrape
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ install_browsers.sh
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ browser_utils.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ browsers
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ browser_chromium.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ browser_factory.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ browser_firefox.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ content_extractor.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ serp_api_client.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ web_scraper.py
‚îú‚îÄ‚îÄ outputs
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ai_generated_features.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251016_104334.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251016_105350.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251016_113936.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251016_114927.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251017_112227.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251017_171520.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bmecat_dabag_results_20251017_172053.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_tables
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCG405NT-XJ_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCG405NT-XJ_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCG405NT-XJ_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCH273NT-XJ_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCH273NT-XJ_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCH273NT-XJ_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCK329P2T-QW_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCK329P2T-QW_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCK329P2T-QW_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCS573NT-XJ_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCS573NT-XJ_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DCS573NT-XJ_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1952-QZ_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1952-QZ_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1952-QZ_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1953-QZ_de.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1953-QZ_fr.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ comparison_DT1953-QZ_it.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ master_comparison_catalog.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ master_bmecat_dabag.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ master_bmecat_dabag.json.backup1
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ master_bmecat_dabag.json.backup2
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ scraped_text
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DCG405NT-XJ.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DCH273NT-XJ.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DCK329P2T-QW.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DCS573NT-XJ.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ DT1952-QZ.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ DT1953-QZ.txt
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Stihl_Version DABAG_summary_20251010_183123.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Stihl_Version DABAG_summary_20251013_100932.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ unique_features.csv
‚îú‚îÄ‚îÄ project_dump.sh
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ scraping_master_dump.txt
‚îî‚îÄ‚îÄ store_steampowered_com_spider_man_20250926_171735
    ‚îú‚îÄ‚îÄ spider_man_results_data.json
    ‚îî‚îÄ‚îÄ spider_man_results.md

25 directories, 111 files


--- File Contents ---


------------------------------------------------- ./doc_processor/config.py --------------------------------------------------

"""
Configuration settings for Document Processor.
Follows same pattern as easy_rich and manual_scrape configs.
"""

import os
from dotenv import load_dotenv

# Load environment variables from root .env
load_dotenv()

# API Configuration
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")  # Used for URL-based PDF parsing
GROK_API_KEY = os.getenv("GROK_API_KEY")  # xAI API key
GROK_MODEL = os.getenv("GROK_MODEL", "grok-beta")  # Default model

# Output Settings
OUTPUT_DIR = "outputs/"
FILENAME_PATTERN = "{original_name}_summary_{timestamp}.json"

# JSON Template - AI fills these fields
DOCUMENT_TEMPLATE = {
    "executive_summary": "",
    "document_type": "",
    "key_topics": [],
    "technical_details": {
        "technologies_mentioned": [],
        "requirements": [],
        "constraints": []
    },
    "entities": {
        "people": [],
        "organizations": [],
        "products": [],
        "dates": []
    },
    "action_items": [],
    "decisions_made": [],
    "open_questions": [],
    "complexity_assessment": "",
    "estimated_read_time_minutes": 0,
    "critical_sections": []
}

# User Questions - asked in terminal after AI processing
USER_QUESTIONS = [
    "What is the primary purpose of this document?",
    "Who is the intended audience? (e.g., developers, executives)",
    "Project or client name (if applicable):",
    "Any critical deadlines or milestones mentioned?",
    "Specific areas you want highlighted or tracked:",
    "Additional context or notes:"
]

# Validation
if not FIRECRAWL_API_KEY:
    raise ValueError("FIRECRAWL_API_KEY not found in environment variables")

if not GROK_API_KEY:
    raise ValueError("GROK_API_KEY not found in environment variables")


------------------------------------------------- ./doc_processor/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Document Processor - PDF & XML Analysis Tool
Parses PDFs/XMLs, analyzes with AI, collects user context, outputs structured JSON.
"""

import sys
import time
from pathlib import Path

# Import configuration
import config

# Import modules
from src.pdf_handler import PDFHandler
from src.firecrawl_parser import FirecrawlParser
from src.llm_processor import LLMProcessor
from src.user_prompt import UserPrompt
from src.json_generator import JSONGenerator


def get_file_path() -> str:
    """Get file path (PDF or XML) from command line or user input."""
    if len(sys.argv) > 1:
        return sys.argv[1]
    else:
        print("üìÑ Document Processor (PDF & XML)")
        print("="*60)
        file_path = input("Enter path to PDF or XML file: ").strip()
        return file_path


def main():
    """Main orchestration function."""
    start_time = time.time()
    
    try:
        # Get file path
        file_path = get_file_path()
        
        # PHASE 1: Validate file
        print("\nüîç Validating file...")
        pdf_handler = PDFHandler()
        
        if not pdf_handler.validate_file(file_path):
            print("‚ùå Validation failed. Exiting.")
            return
        
        file_info = pdf_handler.get_file_info(file_path)
        file_type = file_info.get('file_type', 'pdf')
        print(f"‚úÖ Valid {file_type.upper()}: {file_info['filename']} ({file_info['size_mb']} MB)")
        
        # PHASE 2: Parse document (auto-detects URL vs local, PDF vs XML)
        print(f"\nüìÑ Extracting content from {file_type.upper()}...")
        parser = FirecrawlParser(config.FIRECRAWL_API_KEY)
        
        parse_result = parser.parse_document(file_path, file_type)
        if not parse_result:
            print(f"‚ùå Failed to extract content from {file_type.upper()}. Exiting.")
            return
        
        markdown_text = parser.get_parsed_content(parse_result)
        file_info["word_count"] = parse_result.get("word_count", 0)
        print(f"‚úÖ Extracted {file_info['word_count']} words")
        
        # PHASE 3: Process with Grok AI
        print("\nü§ñ Analyzing with LLM (xAI Grok by default)...")
        grok = LLMProcessor(config.GROK_API_KEY, config.GROK_MODEL, base_url="https://api.x.ai/v1")
        
        ai_analysis = grok.process_document(markdown_text, config.DOCUMENT_TEMPLATE)
        if not ai_analysis:
            print("‚ùå AI processing failed. Exiting.")
            return
        
        # PHASE 4: Collect user context
        print("\nüìù Collecting additional context...")
        user_prompt = UserPrompt(config.USER_QUESTIONS)
        
        raw_answers = user_prompt.ask_questions()
        user_context = user_prompt.format_for_json(raw_answers)
        
        # PHASE 5: Generate output
        print("\nüíæ Generating JSON output...")
        
        # Calculate processing time
        processing_time = round(time.time() - start_time, 2)
        
        processing_meta = {
            "grok_model": config.GROK_MODEL,
            "parsing_method": parse_result.get("method", "unknown"),
            "file_type": file_type,
            "total_processing_time_seconds": processing_time
        }
        
        generator = JSONGenerator(config.OUTPUT_DIR)
        
        final_output = generator.create_output(
            ai_data=ai_analysis,
            user_answers=user_context,
            file_metadata=file_info,
            processing_metadata=processing_meta
        )
        
        output_path = generator.save_to_file(final_output, file_info["filename"])
        
        if output_path:
            # Display summary
            generator.display_summary(final_output)
            print(f"‚úÖ Processing complete! Saved to: {output_path}")
            print(f"‚è±Ô∏è  Total time: {processing_time}s")
        else:
            print("‚ùå Failed to save output file.")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Process interrupted by user. Exiting.")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


------------------------------------------------- ./doc_processor/src/pdf_handler.py --------------------------------------------------

"""PDF and XML file validation and handling."""

import os
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict


class PDFHandler:
    """Validates and prepares PDF and XML files for processing."""
    
    def __init__(self):
        """Initialize file handler for PDF and XML files."""
        pass
    
    # Supported file extensions
    SUPPORTED_EXTENSIONS = {'.pdf', '.xml'}
    
    def validate_file(self, file_path: str) -> bool:
        """
        Validate that input is either a valid local PDF/XML file or a PDF/XML URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
        """
        try:
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Basic sanity: must have netloc and path ending with supported extension
                if not parsed.netloc:
                    print(f"‚ùå Invalid URL: {file_path}")
                    return False
                path_lower = parsed.path.lower()
                if not any(path_lower.endswith(ext) for ext in self.SUPPORTED_EXTENSIONS):
                    print(f"‚ùå URL does not point to a supported file (PDF or XML): {file_path}")
                    return False
                return True

            # Local file case
            path = Path(file_path)

            # Check exists
            if not path.exists():
                print(f"‚ùå File not found: {file_path}")
                return False

            # Check is file (not directory)
            if not path.is_file():
                print(f"‚ùå Not a file: {file_path}")
                return False

            # Check supported extension
            if path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
                print(f"‚ùå Not a supported file type (PDF or XML): {file_path}")
                return False

            # Check readable
            if not os.access(file_path, os.R_OK):
                print(f"‚ùå File not readable: {file_path}")
                return False

            return True

        except Exception as e:
            print(f"‚ùå Error validating file: {e}")
            return False

    def get_file_type(self, file_path: str) -> str:
        """
        Determine file type (pdf or xml).
        
        Args:
            file_path: Path to file or URL
            
        Returns:
            'pdf' or 'xml' or 'unknown'
        """
        try:
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                path_lower = parsed.path.lower()
            else:
                path_lower = str(file_path).lower()
            
            if path_lower.endswith('.pdf'):
                return 'pdf'
            elif path_lower.endswith('.xml'):
                return 'xml'
            else:
                return 'unknown'
        except Exception:
            return 'unknown'
    
    def get_file_info(self, file_path: str) -> Dict[str, object]:
        """
        Extract file metadata for local PDF/XML or URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
            Dict with file info
        """
        try:
            file_type = self.get_file_type(file_path)
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Derive filename from URL path
                name = Path(parsed.path).name or f"document.{file_type}"
                return {
                    "filename": name,
                    "file_path": file_path,
                    "size_mb": 0,
                    "file_type": file_type
                }
            
            # Local file case
            path = Path(file_path)
            size_mb = path.stat().st_size / (1024 * 1024)
            
            return {
                "filename": path.name,
                "file_path": str(path.absolute()),
                "size_mb": round(size_mb, 2),
                "file_type": file_type
            }
        except Exception as e:
            print(f"‚ö†Ô∏è  Error getting file info: {e}")
            return {
                "filename": "unknown",
                "file_path": file_path,
                "size_mb": 0,
                "file_type": "unknown"
            }

------------------------------------------------- ./doc_processor/src/firecrawl_parser.py --------------------------------------------------

"""Firecrawl PDF/XML parsing integration."""

from typing import Optional, Dict
from firecrawl import Firecrawl
import fitz  # PyMuPDF
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom
from .bmecat_parser import BMEcatParser

class FirecrawlParser:
    """Handles PDF and XML to text conversion using Firecrawl and local parsers."""

    def __init__(self, api_key: str):
        """
        Initialize Firecrawl client.

        Args:
            api_key: Firecrawl API key
        """
        if not api_key:
            raise ValueError("Firecrawl API key is required")

        self.firecrawl = Firecrawl(api_key=api_key)

    def _is_url(self, path: str) -> bool:
        """
        Check if input is a URL or local file path.

        Args:
            path: Input path or URL

        Returns:
            True if URL, False if local path
        """
        return path.startswith("http://") or path.startswith("https://")

    def _parse_local_pdf(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local PDF file using PyMuPDF.

        Args:
            file_path: Path to local PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("üìÑ Parsing local PDF with PyMuPDF...")

            # Open PDF
            doc = fitz.open(file_path)

            # Extract text from all pages
            markdown_content = ""
            for page_num, page in enumerate(doc, start=1):
                text = page.get_text()
                markdown_content += f"\n\n## Page {page_num}\n\n{text}"

            page_count = len(doc)
            doc.close()

            if not markdown_content or len(markdown_content) < 50:
                print("‚ö†Ô∏è  Warning: Extracted content is very short")
                return None

            # Count words
            word_count = len(markdown_content.split())

            print(f"‚úÖ Extracted {word_count:,} words from {page_count} pages")

            return {
                "markdown": markdown_content.strip(),
                "word_count": word_count,
                "page_count": page_count,
                "method": "pymupdf",
            }

        except Exception as e:
            print(f"‚ùå Error parsing local PDF: {e}")
            return None

    def _parse_local_xml(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local XML file using enhanced BMEcat parser.

        Args:
            file_path: Path to local XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("üìÑ Parsing BMEcat XML with enhanced parser...")
            
            # Use BMEcat-specific parser
            bmecat_parser = BMEcatParser()
            parsed_data = bmecat_parser.parse_bmecat_file(file_path)
            
            if not parsed_data:
                print("‚ö†Ô∏è  Failed to parse BMEcat XML")
                return None
            
            # Convert to markdown for AI processing
            markdown_content = bmecat_parser.to_markdown(parsed_data)
            word_count = len(markdown_content.split())
            
            print(f"‚úÖ Extracted {word_count:,} words")
            print(f"   - {parsed_data['metadata']['total_catalog_structures']} catalog structures")
            print(f"   - {parsed_data['metadata']['root_categories']} root categories")
            
            return {
                "markdown": markdown_content,
                "word_count": word_count,
                "method": "bmecat_parser"
            }
        
        except Exception as e:
            print(f"‚ùå Error parsing BMEcat XML: {e}")
            return None

    def _parse_url_pdf(self, pdf_url: str) -> Optional[Dict[str, object]]:
        """
        Parse PDF from URL using Firecrawl.

        Args:
            pdf_url: URL to PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("üìÑ Parsing PDF from URL with Firecrawl...")

            # Use Firecrawl with PDF parser explicitly
            result = self.firecrawl.scrape(
                pdf_url, formats=["markdown"], parsers=["pdf"]
            )

            # Extract markdown content
            markdown_content = getattr(result, "markdown", "")

            if not markdown_content or len(markdown_content) < 50:
                print(" Warning: Extracted content is very short")
                return None

            # Count words for info
            word_count = len(markdown_content.split())

            print(f"‚úÖ Extracted {word_count:,} words")

            return {
                "markdown": markdown_content,
                "word_count": word_count,
                "raw_result": result,
                "method": "firecrawl",
            }

        except Exception as e:
            print(f"‚ùå Error parsing PDF from URL: {e}")
            return None

    def _parse_url_xml(self, url: str) -> Optional[Dict[str, object]]:
        """
        Parse XML from URL.
        
        Args:
            url: URL to XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            import requests
            
            # Fetch XML from URL
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # Parse XML
            root = ET.fromstring(response.content)
            
            # Convert to pretty-printed string
            xml_string = ET.tostring(root, encoding='unicode')
            pretty_xml = minidom.parseString(xml_string).toprettyxml(indent="  ")
            
            # Combine structure content (pretty XML only)
            markdown_text = f"# XML Document Structure\n\n```xml\n{pretty_xml}\n```"
            
            word_count = len(pretty_xml.split())
            
            return {
                "markdown": markdown_text,
                "word_count": word_count,
                "method": "xml_url_parser"
            }
            
        except Exception as e:
            print(f"‚ùå Error parsing XML URL: {e}")
            return None



    def parse_document(self, file_path: str, file_type: str = 'pdf') -> Optional[Dict[str, object]]:
        """
        Parse document (PDF or XML) based on type and location.

        Args:
            file_path: Path to local file OR URL to file
            file_type: 'pdf' or 'xml' (default: 'pdf')

        Returns:
            Dict with parsed content or None if failed
        """
        is_url = self._is_url(file_path)

        # Route to appropriate parser
        if file_type == 'pdf':
            if is_url:
                print("üåê Detected PDF URL - using Firecrawl")
                return self._parse_url_pdf(file_path)
            else:
                print("üíæ Detected local PDF - using PyMuPDF")
                return self._parse_local_pdf(file_path)
        elif file_type == 'xml':
            if is_url:
                print("üåê Detected XML URL - using XML parser")
                return self._parse_url_xml(file_path)
            else:
                print("üíæ Detected local XML - using XML parser")
                return self._parse_local_xml(file_path)
        else:
            print(f"‚ùå Unsupported file type: {file_type}")
            return None

    def get_parsed_content(self, parse_result: Dict) -> str:
        """
        Extract markdown text from parse result.
        
        Args:
            parse_result: Result from parse_document()
        
        Returns:
            Markdown text content
        """
        if not parse_result:
            return ""
        return parse_result.get("markdown", "")

------------------------------------------------- ./doc_processor/src/user_prompt.py --------------------------------------------------

"""Terminal user interaction for collecting context."""

from typing import Dict, List


class UserPrompt:
    """Handles interactive terminal questions to collect user context."""
    
    def __init__(self, questions: List[str]):
        """
        Initialize with list of questions.
        
        Args:
            questions: List of question strings
        """
        self.questions = questions
    
    def ask_questions(self) -> Dict[str, str]:
        """
        Present questions to user in terminal and collect answers.
        
        Returns:
            Dict mapping question index to answer
        """
        print("\n" + "="*60)
        print("üìù Additional Context Needed")
        print("="*60)
        print("(Press Enter to skip optional questions)\n")
        
        answers = {}
        total = len(self.questions)
        
        for i, question in enumerate(self.questions, 1):
            # Display question with progress
            print(f"Question {i}/{total}:")
            print(f"  {question}")
            
            # Get user input
            try:
                answer = input("> ").strip()
                
                # Store answer (even if empty)
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": answer if answer else "N/A"
                }
                
                print()  # Blank line for readability
                
            except KeyboardInterrupt:
                print("\n\n‚ö†Ô∏è  Input interrupted. Skipping remaining questions.")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading input: {e}")
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": "N/A"
                }
        
        print("="*60)
        print("‚úÖ Context collection complete\n")
        
        return answers
    
    def format_for_json(self, answers: Dict) -> Dict[str, str]:
        """
        Format answers for JSON output structure.
        
        Args:
            answers: Raw answers dict from ask_questions()
            
        Returns:
            Cleaned dict for user_context section
        """
        formatted = {}
        
        for key, value in answers.items():
            # Create simple key from question
            question_text = value["question"]
            answer_text = value["answer"]
            
            # Use simplified key
            clean_key = question_text.split("?")[0].lower().replace(" ", "_")[:30]
            formatted[clean_key] = answer_text
        
        return formatted

------------------------------------------------- ./doc_processor/src/json_generator.py --------------------------------------------------

"""JSON output file generation."""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional


class JSONGenerator:
    """Creates structured JSON output files."""
    
    def __init__(self, output_dir: str = "outputs/"):
        """
        Initialize generator.
        
        Args:
            output_dir: Directory for output files
        """
        self.output_dir = output_dir
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """Create output directory if it doesn't exist."""
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def create_output(
        self,
        ai_data: Dict,
        user_answers: Dict,
        file_metadata: Dict,
        processing_metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Combine all data into final JSON structure.
        
        Args:
            ai_data: AI analysis results
            user_answers: User context answers
            file_metadata: Source file info
            processing_metadata: Optional processing stats
            
        Returns:
            Complete output dict
        """
        timestamp = datetime.now().isoformat()
        
        output = {
            "document_info": {
                "source_file": file_metadata.get("filename", "unknown"),
                "file_path": file_metadata.get("file_path", ""),
                "processed_at": timestamp,
                "file_size_mb": file_metadata.get("size_mb", 0),
                "word_count": file_metadata.get("word_count", 0)
            },
            "ai_analysis": ai_data,
            "user_context": user_answers,
            "processing_metadata": processing_metadata or {}
        }
        
        return output
    
    def save_to_file(self, data: Dict, original_filename: str) -> str:
        """
        Save JSON data to timestamped file.
        
        Args:
            data: JSON data to save
            original_filename: Original PDF filename
            
        Returns:
            Path to saved file
        """
        try:
            # Generate filename
            base_name = Path(original_filename).stem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"{base_name}_summary_{timestamp}.json"
            output_path = os.path.join(self.output_dir, output_filename)
            
            # Write JSON file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Saved to: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error saving file: {e}")
            return ""
    
    def display_summary(self, data: Dict):
        """
        Display key information in terminal.
        
        Args:
            data: Complete output data
        """
        print("\n" + "="*60)
        print("üìä Document Summary")
        print("="*60)
        
        # Executive summary
        if "ai_analysis" in data:
            summary = data["ai_analysis"].get("executive_summary", "N/A")
            print(f"\nSummary:\n{summary[:200]}...")
            
            # Key topics
            topics = data["ai_analysis"].get("key_topics", [])
            if topics:
                print(f"\nKey Topics: {', '.join(topics[:5])}")
            
            # Complexity
            complexity = data["ai_analysis"].get("complexity_assessment", "N/A")
            print(f"Complexity: {complexity}")
        
        print("\n" + "="*60 + "\n")

------------------------------------------------- ./doc_processor/src/bmecat_parser.py --------------------------------------------------

"""
BMEcat XML Parser - Specialized parser for BMEcat catalog format.
Captures all attributes, relationships, and multi-language content.
"""

import xml.etree.ElementTree as ET
import codecs
from typing import Dict, List, Optional, Any


class BMEcatParser:
    """Enhanced parser specifically designed for BMEcat format."""
    
    def __init__(self):
        """Initialize the BMEcat parser."""
        self.namespaces = {
            'bme': 'http://www.bmecat.org/bmecat/2005fd'
        }
    
    def parse_bmecat_file(self, file_path: str) -> Dict[str, Any]:
        """
        Parse BMEcat XML file and extract all structured data.
        
        Args:
            file_path: Path to BMEcat XML file
            
        Returns:
            Dictionary with complete structured data or None if failed
        """
        try:
            # Read and sanitize XML to handle BOM and leading whitespace before declaration
            with open(file_path, 'rb') as f:
                raw = f.read()

            # Remove UTF-8 BOM if present
            if raw.startswith(codecs.BOM_UTF8):
                raw = raw[len(codecs.BOM_UTF8):]

            # Decode and strip leading whitespace/newlines before the first '<'
            text = raw.decode('utf-8', errors='replace')
            text = text.lstrip()  # remove leading spaces/newlines/tabs

            # Parse from string (handles cases where declaration isn't at pos 0 originally)
            root = ET.fromstring(text)
            
            # Handle namespace
            if root.tag.startswith('{'):
                ns = root.tag.split('}')[0] + '}'
            else:
                ns = ''
            
            result = {
                'header': self._parse_header(root, ns),
                'catalog_groups': self._parse_catalog_groups(root, ns),
                'metadata': {
                    'total_catalog_structures': 0,
                    'root_categories': 0,
                    'node_categories': 0,
                    'leaf_categories': 0
                }
            }
            
            # Count structures
            for group in result['catalog_groups']:
                for struct in group['structures']:
                    result['metadata']['total_catalog_structures'] += 1
                    struct_type = struct.get('type', '')
                    if struct_type == 'root':
                        result['metadata']['root_categories'] += 1
                    elif struct_type == 'node':
                        result['metadata']['node_categories'] += 1
                    elif struct_type == 'leaf':
                        result['metadata']['leaf_categories'] += 1
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error parsing BMEcat file: {e}")
            return None
    
    def _parse_header(self, root: ET.Element, ns: str) -> Dict[str, Any]:
        """Extract header information."""
        header = {}
        header_elem = root.find(f'.//{ns}HEADER')
        
        if header_elem is not None:
            # Catalog info
            catalog_elem = header_elem.find(f'{ns}CATALOG')
            if catalog_elem is not None:
                header['catalog'] = {
                    'languages': [lang.text for lang in catalog_elem.findall(f'{ns}LANGUAGE')],
                    'catalog_id': self._get_text(catalog_elem, f'{ns}CATALOG_ID'),
                    'catalog_version': self._get_text(catalog_elem, f'{ns}CATALOG_VERSION')
                }
            
            # Supplier info
            supplier_elem = header_elem.find(f'{ns}SUPPLIER')
            if supplier_elem is not None:
                header['supplier'] = {
                    'name': self._get_text(supplier_elem, f'{ns}SUPPLIER_NAME')
                }
            
            # Parties info
            parties = []
            for party in header_elem.findall(f'.//{ns}PARTY'):
                party_id = party.find(f'{ns}PARTY_ID')
                if party_id is not None:
                    parties.append({
                        'id': party_id.text,
                        'type': party_id.get('type')
                    })
            header['parties'] = parties
        
        return header
    
    def _parse_catalog_groups(self, root: ET.Element, ns: str) -> List[Dict[str, Any]]:
        """Extract all catalog group systems."""
        groups = []
        
        for group_system in root.findall(f'.//{ns}CATALOG_GROUP_SYSTEM'):
            group_data = {
                'group_system_id': self._get_text(group_system, f'{ns}GROUP_SYSTEM_ID'),
                'group_system_name': self._get_text(group_system, f'{ns}GROUP_SYSTEM_NAME'),
                'structures': []
            }
            
            # Parse all catalog structures
            for struct in group_system.findall(f'{ns}CATALOG_STRUCTURE'):
                structure_data = self._parse_catalog_structure(struct, ns)
                group_data['structures'].append(structure_data)
            
            groups.append(group_data)
        
        return groups
    
    def _parse_catalog_structure(self, struct: ET.Element, ns: str) -> Dict[str, Any]:
        """Parse a single CATALOG_STRUCTURE element with all details."""
        data = {
            'type': struct.get('type'),
            'group_id': self._get_text(struct, f'{ns}GROUP_ID'),
            'parent_id': self._get_text(struct, f'{ns}PARENT_ID'),
            'group_order': self._get_text(struct, f'{ns}GROUP_ORDER'),
            'names': {},
            'descriptions': {}
        }
        
        # Get all GROUP_NAME elements (multi-language)
        for name_elem in struct.findall(f'{ns}GROUP_NAME'):
            lang = name_elem.get('lang', 'default')
            data['names'][lang] = name_elem.text
        
        # Get all GROUP_DESCRIPTION elements (multi-language)
        for desc_elem in struct.findall(f'{ns}GROUP_DESCRIPTION'):
            lang = desc_elem.get('lang', 'default')
            # Preserve full description text including line breaks
            desc_text = desc_elem.text or ''
            data['descriptions'][lang] = desc_text.strip()
        
        return data
    
    def _get_text(self, element: ET.Element, path: str) -> Optional[str]:
        """Safely extract text from element."""
        elem = element.find(path)
        return elem.text if elem is not None else None
    
    def to_markdown(self, parsed_data: Dict[str, Any]) -> str:
        """
        Convert parsed BMEcat data to well-structured markdown for AI processing.
        
        Args:
            parsed_data: Output from parse_bmecat_file()
            
        Returns:
            Formatted markdown string
        """
        md_lines = []
        
        # Header section
        md_lines.append("# BMEcat Catalog Document\n")
        
        if parsed_data.get('header'):
            md_lines.append("## Catalog Information\n")
            header = parsed_data['header']
            
            if 'catalog' in header:
                cat = header['catalog']
                md_lines.append(f"**Catalog ID:** {cat.get('catalog_id', 'N/A')}")
                md_lines.append(f"**Version:** {cat.get('catalog_version', 'N/A')}")
                md_lines.append(f"**Languages:** {', '.join(cat.get('languages', []))}\n")
            
            if 'supplier' in header:
                md_lines.append(f"**Supplier:** {header['supplier'].get('name', 'N/A')}\n")
        
        # Metadata
        if parsed_data.get('metadata'):
            meta = parsed_data['metadata']
            md_lines.append("## Catalog Statistics\n")
            md_lines.append(f"- Total catalog structures: {meta['total_catalog_structures']}")
            md_lines.append(f"- Root categories: {meta['root_categories']}")
            md_lines.append(f"- Node categories: {meta['node_categories']}")
            md_lines.append(f"- Leaf categories: {meta['leaf_categories']}\n")
        
        # Catalog structures
        md_lines.append("## Catalog Structure Details\n")
        
        for group in parsed_data.get('catalog_groups', []):
            md_lines.append(f"### Group System: {group.get('group_system_name', 'N/A')}\n")
            md_lines.append(f"**System ID:** {group.get('group_system_id', 'N/A')}\n")
            
            for idx, struct in enumerate(group.get('structures', []), 1):
                md_lines.append(f"#### Structure {idx}: {struct.get('type', 'unknown').upper()}\n")
                md_lines.append(f"**Group ID:** `{struct.get('group_id', 'N/A')}` ")
                md_lines.append(f"**Parent ID:** `{struct.get('parent_id', 'N/A')}` ")
                md_lines.append(f"**Order:** {struct.get('group_order', 'N/A')}\n")
                
                # Names (all languages)
                if struct.get('names'):
                    md_lines.append("**Names:**")
                    for lang, name in struct['names'].items():
                        md_lines.append(f"- [{lang}] {name}")
                    md_lines.append("")
                
                # Descriptions (all languages)
                if struct.get('descriptions'):
                    md_lines.append("**Descriptions:**")
                    for lang, desc in struct['descriptions'].items():
                        if desc:  # Only show if description exists
                            md_lines.append(f"\n*[{lang}]*")
                            md_lines.append(desc)
                    md_lines.append("\n---\n")
        
        return "\n".join(md_lines)

------------------------------------------------- ./doc_processor/src/llm_processor.py --------------------------------------------------

"""Generic LLM API integration for document analysis (xAI Grok-compatible)."""

import json
from typing import Dict, Optional
import requests


class LLMProcessor:
    """Processes documents using xAI's Grok API via direct HTTP calls."""

    def __init__(self, api_key: str, model_name: str = "grok-4-fast-reasoning", base_url: str = "https://api.x.ai/v1"):
        """Initialize LLM processor.

        Args:
            api_key: xAI API key
            model_name: Grok model to use
            base_url: xAI API base URL
        """
        if not api_key:
            raise ValueError("LLM API key is required")

        self.api_key = api_key
        self.model = model_name
        self.base_url = base_url.rstrip('/')
    
    def create_analysis_prompt(self, text_content: str, template: Dict) -> str:
        """
        Create prompt for the LLM to analyze the document and fill the template.
        """
        template_str = json.dumps(template, indent=2)
        
        prompt = f"""Analyze the following document and fill the JSON template with accurate information extracted from the text.

INSTRUCTIONS:
- Be concise but thorough
- Extract only information present in the document
- Use "N/A" for fields where information is not available
- For lists, provide the most relevant items
- Estimate complexity as: Simple, Medium, or Complex

DOCUMENT CONTENT:
{text_content[:15000]}  

JSON TEMPLATE TO FILL:
{template_str}

Return ONLY the filled JSON, no other text."""
        
        return prompt
    
    def process_document(self, markdown_text: str, template: Dict) -> Optional[Dict]:
        """
        Send document to the LLM for analysis and return the filled JSON template.
        """
        try:
            print("ü§ñ Processing with LLM (xAI Grok)...")

            # Create prompt
            prompt = self.create_analysis_prompt(markdown_text, template)

            # Build request
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a document analysis AI that extracts structured information and fills JSON templates accurately.",
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ],
                "temperature": 0.3,
                "max_tokens": 4000,
            }

            resp = requests.post(url, headers=headers, json=payload, timeout=60)

            if resp.status_code != 200:
                print(f"‚ùå Grok API error: {resp.status_code} - {resp.text[:500]}")
                return None

            data = resp.json()

            # Extract assistant message content
            try:
                content = data["choices"][0]["message"]["content"]
            except Exception:
                print("‚ùå Unexpected response format from Grok API")
                return None
            
            # Parse JSON response
            try:
                filled_template = json.loads(content)
                print("‚úÖ AI analysis complete")
                return filled_template
            except json.JSONDecodeError:
                print("‚ö†Ô∏è  Warning: Response was not valid JSON, attempting to extract...")
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = content[start:end]
                    filled_template = json.loads(json_str)
                    print("‚úÖ AI analysis complete (extracted JSON)")
                    return filled_template
                else:
                    print("‚ùå Could not parse AI response as JSON")
                    return None
        except Exception as e:
            print(f"‚ùå Error processing with LLM: {e}")
            return None

------------------------------------------------- ./easy_rich/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

------------------------------------------------- ./easy_rich/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    ‚îî‚îÄ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("ü•∑ Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./easy_rich/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
from firecrawl import Firecrawl
from dotenv import load_dotenv
import re
from urllib.parse import urlparse, urljoin

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        DEFAULT_PROXY_MODE, BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    DEFAULT_PROXY_MODE = "auto"
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize Firecrawl client."""
        load_dotenv()
        api_key = os.getenv("FIRECRAWL_API_KEY")
        if not api_key:
            raise ValueError("FIRECRAWL_API_KEY not found in environment variables")
        self.firecrawl = Firecrawl(api_key=api_key)

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_proxy(self, url: str, proxy_mode: str = DEFAULT_PROXY_MODE) -> Optional[Dict[str, str]]:
        """
        Scrape content with specified proxy mode.
        
        Args:
            url: URL to scrape
            proxy_mode: Proxy mode ("auto", "basic", "stealth")
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            result = self.firecrawl.scrape(
                url,
                proxy=proxy_mode,
                formats=[
                    "markdown",
                    {
                        "type": "json",
                        "prompt": "Extract key information from this page including title, main content summary, key points, and any important data like prices, dates, or contact information.",
                    },
                ],
            )

            # Access attributes from Firecrawl Document object safely
            has_metadata = hasattr(result, "metadata") and result.metadata is not None
            title = getattr(result.metadata, "title", "No title found") if has_metadata else "No title found"
            status_code = getattr(result.metadata, "statusCode", "Unknown") if has_metadata else "Unknown"
            markdown_content = getattr(result, "markdown", "")
            structured_data = getattr(result, "json", {})

            return {
                "url": url,
                "title": title,
                "markdown_content": markdown_content,
                "structured_data": structured_data,
                "status_code": str(status_code),
            }

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Firecrawl with bot detection and stealth fallback.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # First attempt with default proxy
        scraped_content = self.scrape_with_proxy(url, DEFAULT_PROXY_MODE)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # Prompt for stealth retry
            if self.prompt_stealth_retry():
                print(STEALTH_TRYING_MSG)
                stealth_content = self.scrape_with_proxy(url, "stealth")
                if stealth_content:
                    print("‚úÖ Success with stealth mode!")
                    return stealth_content
                else:
                    print("‚ùå Stealth mode also failed")
                    return None
            else:
                print("‚è≠Ô∏è  Skipping stealth mode")
                return None
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Save scraped content to both markdown and JSON files.

        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            # Save markdown content
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./easy_rich/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None

------------------------------------------------- ./manual_scrape/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

# Browser Configuration
BROWSER = {
    'default': 'firefox',  # Options: 'chromium', 'firefox'
    'engines': {
        'chromium': {
            'timeout': 30000,
            'stealth_mode': True
        },
        'firefox': {
            'timeout': 30000,
            'stealth_mode': True
        }
    }
}

------------------------------------------------- ./manual_scrape/install_browsers.sh --------------------------------------------------

#!/bin/bash
echo "üöÄ Installing Playwright browsers..."
playwright install
playwright install chromium firefox
echo "‚úÖ Both Chromium and Firefox installed successfully!"

------------------------------------------------- ./manual_scrape/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    ‚îî‚îÄ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("ü•∑ Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./manual_scrape/src/content_extractor.py --------------------------------------------------

"""Extract article content from webpages using Playwright."""

import time
from bs4 import BeautifulSoup
from .browser_utils import handle_any_popups, add_human_pause, extract_readable_text

class ContentExtractor:
    """Extract article content from a webpage."""
    
    def __init__(self):
        pass
    
    def extract_article_content(self, page, url, article, domain, source, user_email=None):
        """Extract article content from the page.
        
        Args:
            page: The Playwright page object
            url: The URL of the article
            article: The article metadata
            domain: The domain of the article
            source: The source of the article
            user_email: Optional user email for validation context
            
        Returns:
            tuple: (success, article_content, title, rejection_reason)
        """
        print("Extracting article content...")
                
        try:
            # Start with aggressive popup handling first
            print("Applying aggressive popup/cookie handling approach...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Perform progressive scrolling for content exposure
            print("Performing progressive scrolling for content exposure...")
            self._perform_progressive_scrolling(page)
            
            # Handle any popups that appeared after scrolling
            print("Handling any new popups after scrolling...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Wait for page to be fully loaded with a timeout
            print("Waiting for page ready state before content extraction...")
            try:
                page.wait_for_load_state("networkidle", timeout=10000)
                print("Page ready state reached")
            except Exception as e:
                print(f"Error waiting for page ready state (continuing anyway): {e}")
            
            # Extract content with BeautifulSoup
            article_content = self._extract_content_with_soup(page)
            
            # Get character count
            chars_count = len(article_content) if article_content else 0
            print(f"Extracted {chars_count} chars with content extraction")
            
            # Include the full content in logs when it's less than 100 characters
            if article_content and chars_count < 100:
                print(f"Content (under 100 chars): {article_content}")
            
            # If we have substantial content (>5500 chars), consider it valid
            if article_content and len(article_content) > 5500:
                title = self._extract_title(page, article)
                print(f"Article has substantial content ({len(article_content)} chars), accepting")
                return True, article_content, title, None
            
            # Continue with validation for articles with less content
            if article_content and len(article_content) > 150:
                title = self._extract_title(page, article)
            
                # Return the content for validation
                if len(article_content) < 1600:
                    print(f"Content too short: {len(article_content)} chars (minimum 1600)")
                    
                    # Try a more aggressive approach as fallback
                    try:
                        print("Attempting more aggressive content extraction as fallback...")
                        
                        fallback_content = page.evaluate("""() => {
                            const bodyText = document.body.innerText;
                            return bodyText || '';
                        }""")
                        
                        if fallback_content and len(fallback_content) > 1600:
                            print(f"Fallback extraction succeeded, got {len(fallback_content)} chars")
                            return True, fallback_content, title, None
                    except Exception as fallback_error:
                        print(f"Fallback extraction failed: {fallback_error}")
                    
                    rejection_reason = f"Content too short: {len(article_content)} chars (minimum 1600)"
                    return False, "", title, rejection_reason
                
                return True, article_content, title, None
            else:
                rejection_reason = "Content too short: 0 chars"
                print("Content extraction failed: Empty or very short content")
                return False, "", article.get("title", ""), rejection_reason
            
        except Exception as e:
            print(f"Error extracting content: {e}")
            import traceback
            print(f"Extraction error traceback: {traceback.format_exc()}")
            rejection_reason = f"Error: {str(e)}"
            return False, "", article.get("title", ""), rejection_reason

    def _perform_progressive_scrolling(self, page):
        """Perform progressive scrolling to expose all content."""
        try:
            page_height = page.evaluate("document.body.scrollHeight")
            view_height = page.evaluate("window.innerHeight")
            
            scroll_steps = min(4, max(2, int(page_height / view_height)))
            print(f"Performing {scroll_steps} scroll steps to load all content...")
            
            for i in range(1, scroll_steps + 1):
                scroll_position = (i * page_height) / scroll_steps
                page.evaluate(f"window.scrollTo(0, {scroll_position})")
                page.wait_for_timeout(1000)
            
            # Go back to top before extraction
            page.evaluate("window.scrollTo(0, 0)")
            page.wait_for_timeout(1000)
            
        except Exception as e:
            print(f"Error during progressive scrolling: {e}")

    def _extract_content_with_soup(self, page):
        """Extract content using BeautifulSoup parsing."""
        try:
            # Get page HTML and parse with BeautifulSoup
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract readable text
            return extract_readable_text(soup)
            
        except Exception as e:
            print(f"Error during content extraction: {e}")
            return ""

    def _extract_title(self, page, article):
        """Extract title from the page."""
        try:
            # Try to get title from page
            page_title = page.title()
            if page_title and len(page_title.strip()) > 0:
                return page_title.strip()
                
            # Fallback to original title
            return article.get("title", "")
            
        except Exception as e:
            print(f"Error extracting title: {e}")
            return article.get("title", "")

------------------------------------------------- ./manual_scrape/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
import re
from urllib.parse import urlparse, urljoin
from playwright.sync_api import sync_playwright

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

from .browsers.browser_factory import BrowserFactory
from .content_extractor import ContentExtractor


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize web scraper with Playwright."""
        self.content_extractor = ContentExtractor()

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_playwright(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content with Playwright browser.
        
        Args:
            url: URL to scrape
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            with sync_playwright() as p:
                # Create browser instance
                browser_config = BrowserFactory.create()
                browser = browser_config.launch(p, headless=True)
                
                # Create context with stealth settings
                user_agents = browser_config.get_user_agents()
                import random
                context = browser.new_context(
                    user_agent=random.choice(user_agents),
                    viewport={"width": 1280, "height": 1200}
                )
                page = context.new_page()
                
                # Set timeout
                page.set_default_navigation_timeout(60000)
                
                try:
                    # Navigate to URL
                    response = page.goto(url, wait_until="domcontentloaded")
                    
                    if response and response.status >= 400:
                        print(f"HTTP error {response.status} while accessing content")
                        return None
                    
                    # Extract domain and create article dict
                    domain = urlparse(url).netloc
                    article = {"title": f"Article from {domain}"}
                    
                    # Extract content
                    success, article_content, title, rejection_reason = self.content_extractor.extract_article_content(
                        page, url, article, domain, domain
                    )
                    
                    if not success:
                        print(f"Content extraction failed: {rejection_reason}")
                        return None
                    
                    return {
                        "url": url,
                        "title": title,
                        "markdown_content": article_content,
                        "structured_data": {},
                        "status_code": str(response.status if response else 200),
                    }
                    
                finally:
                    page.close()
                    context.close()
                    browser.close()

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Playwright with bot detection.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # Scrape with Playwright
        scraped_content = self.scrape_with_playwright(url)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # For now, just return None - stealth retry could be implemented later
            print("‚≠ê Bot detection encountered")
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./manual_scrape/src/browser_utils.py --------------------------------------------------

"""Browser utilities for popup handling, data extraction, and stealth operations."""

import time
import random
from bs4 import BeautifulSoup

def get_human_delay(min_ms=800, max_ms=2500):
    """Get a random delay that mimics human behavior."""
    return random.randint(min_ms, max_ms)

def add_human_pause(page, min_ms=1000, max_ms=3000):
    """Add a human-like pause with mouse movement."""
    try:
        viewport = page.viewport_size
        if viewport:
            x = random.randint(100, viewport['width'] - 100)
            y = random.randint(100, viewport['height'] - 100)
            page.mouse.move(x, y)
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))
    except Exception:
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))

def handle_cookies_and_consent(page, timeout=15000):
    """Handle cookies and consent dialogs with Playwright selectors."""
    print("Handling cookies and consent with Playwright selectors...")
    
    start_time = time.time()
    max_time_ms = timeout
    
    try:
        if (time.time() - start_time) * 1000 > max_time_ms:
            print(f"Cookie handling timed out after {timeout}ms")
            return False
        
        # Priority 1: Explicit accept buttons
        accept_buttons = [
            "button:has-text('I accept all cookies')",
            "button:has-text('Accept all cookies')", 
            "button:has-text('Accept All')",
            "button:has-text('Accept')",
            "button:has-text('Accetta tutto')",  # Italian
            "button:has-text('Accetto')",       # Italian
        ]
        
        # Priority 2: Close/dismiss buttons  
        close_buttons = [
            "button:has-text('Close')",
            "button:has-text('Chiudi')",        # Italian
            "button:has-text('√ó')",
            "button[aria-label*='close']",
            "button[class*='close']",
        ]
        
        # Priority 3: Generic attribute-based selectors
        generic_buttons = [
            "[id*='accept'] button:not([href]):not(a)",
            "[class*='accept'] button:not([href]):not(a)", 
            "[id*='cookie'] button:not([href]):not(a)",
            "[class*='cookie'] button:not([href]):not(a)",
            "[data-testid*='accept'] button:not([href]):not(a)",
        ]
        
        # Combine all selectors in priority order
        button_selectors = accept_buttons + close_buttons + generic_buttons
        
        clicked = False
        
        # Try each selector with timeout check
        for selector in button_selectors:
            if (time.time() - start_time) * 1000 > max_time_ms:
                print(f"Cookie handling timed out during selector iteration")
                return False
            
            try:
                if page.query_selector(selector):
                    print(f"Found popup button with selector: {selector}")
                    page.click(selector, timeout=2000)
                    print(f"Successfully clicked popup button with selector: {selector}")
                    clicked = True
                    break
            except Exception:
                continue
        
        if clicked:
            print("Cookie/consent button clicked, waiting for page to stabilize...")
            remaining_time = max(0, max_time_ms - (time.time() - start_time) * 1000)
            if remaining_time > 1000:
                page.wait_for_timeout(min(2000, int(remaining_time)))
            return True
        
        return False
        
    except Exception as e:
        print(f"Error handling cookies and consent: {e}")
        return False

def handle_any_popups(page, aggressive=False):
    """Handle any type of popup, consent dialog, or cookie banner."""
    print(f"Trying {'aggressive ' if aggressive else ''}popup handling...")
    
    # Step 1: Try button clicking with human delays
    add_human_pause(page, 500, 1200)
    
    button_clicked = handle_cookies_and_consent(page, timeout=8000 if aggressive else 5000)
    
    if button_clicked:
        print("‚úÖ Successfully handled popup with button clicking")
        add_human_pause(page, 1000, 2000)
        return True
    
    # Step 2: DOM removal fallback if aggressive
    if aggressive:
        print("Using DOM removal fallback...")
        add_human_pause(page, 1000, 2000)
        
        try:
            removed_count = page.evaluate("""() => {
                const removeElements = (selector) => {
                    const elements = document.querySelectorAll(selector);
                    let removed = 0;
                    elements.forEach(el => {
                        el.remove();
                        removed++;
                    });
                    return removed;
                };
                
                let removedCount = 0;
                
                // Remove elements with high z-index that could be overlays
                document.querySelectorAll('*').forEach(el => {
                    const style = window.getComputedStyle(el);
                    const zIndex = parseInt(style.zIndex);
                    if (zIndex > 999) {
                        const position = style.position;
                        if (position === 'fixed' || position === 'absolute') {
                            el.remove();
                            removedCount++;
                        }
                    }
                });
                
                // Remove common cookie/consent banners
                removedCount += removeElements('[class*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[class*="consent"]:not(html):not(body)');
                removedCount += removeElements('[class*="popup"]:not(html):not(body)');
                removedCount += removeElements('[class*="banner"]:not(html):not(body)');
                removedCount += removeElements('[class*="overlay"]:not(html):not(body)');
                removedCount += removeElements('[class*="modal"]:not(html):not(body)');
                removedCount += removeElements('[id*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[id*="consent"]:not(html):not(body)');
                
                document.body.style.overflow = 'auto';
                document.documentElement.style.overflow = 'auto';
                
                return removedCount;
            }""")
            
            if removed_count > 0:
                print("DOM removal: removed ${removed_count} elements")
                add_human_pause(page, 1500, 3000)
                return True
        except Exception as e:
            print(f"DOM removal failed: {e}")
    
    return False

def extract_readable_text(soup):
    """Extract readable text from BeautifulSoup object."""
    # Remove unwanted elements
    for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
        element.decompose()
    
    # Try to find main content areas
    content_selectors = [
        'article', 
        '[role="main"]', 
        '.content', 
        '#content',
        '.post-content',
        '.article-body',
        '.entry-content'
    ]
    
    for selector in content_selectors:
        content = soup.select_one(selector)
        if content:
            text = content.get_text(separator=' ', strip=True)
            if len(text) > 500:  # Only return if substantial content
                return text
    
    # Fallback to body text
    return soup.get_text(separator=' ', strip=True)

------------------------------------------------- ./manual_scrape/src/browsers/browser_firefox.py --------------------------------------------------

"""Firefox browser implementation."""

from typing import List

class FirefoxBrowser:
    """Firefox browser configuration and launch handler."""

    name = "firefox"

    def get_launch_args(self) -> List[str]:
        """Get Firefox-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--no-first-run',
            '--disable-default-browser-check',
            '--disable-infobars'
        ]

    def launch(self, playwright_instance, headless: bool = False):
        """Launch Firefox browser with stealth configuration."""
        print(f"üöÄ Launching Firefox browser (headless: {headless})")
        print(f"üîß Firefox launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.firefox.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"‚úÖ Firefox browser launched successfully")
            return browser
        except Exception as e:
            print(f"‚ùå Failed to launch Firefox browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Firefox-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/browsers/__init__.py --------------------------------------------------

"""Browser factory and configuration modules."""

------------------------------------------------- ./manual_scrape/src/browsers/browser_factory.py --------------------------------------------------

"""Factory class for creating browser instances."""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import BROWSER

from .browser_chromium import ChromiumBrowser
from .browser_firefox import FirefoxBrowser

class BrowserFactory:
    """Factory class for creating browser instances."""

    _browsers = {
        'chromium': ChromiumBrowser,
        'firefox': FirefoxBrowser
    }

    @classmethod
    def create(cls, browser_name=None):
        """Create a browser instance based on configuration or specified name."""
        if browser_name is None:
            browser_name = BROWSER.get('default', 'firefox')
            print(f"üåê Browser auto-selected from config: {browser_name}")
        else:
            print(f"üåê Browser explicitly requested: {browser_name}")

        browser_class = cls._browsers.get(browser_name)
        if not browser_class:
            raise ValueError(f"Unsupported browser: {browser_name}. Available: {list(cls._browsers.keys())}")

        browser_instance = browser_class()
        print(f"‚úÖ Browser instance created: {browser_instance.name}")
        return browser_instance

    @classmethod
    def get_available_browsers(cls):
        """Get list of available browser names."""
        return list(cls._browsers.keys())
    
    @classmethod
    def verify_browser_config(cls):
        """Verify current browser configuration and return details."""
        try:
            config_browser = BROWSER.get('default', 'firefox')
            print(f"üìã Config browser setting: {config_browser}")
            
            browser_instance = cls.create()
            print(f"üìã Actual browser created: {browser_instance.name}")
            
            return {
                'config_browser': config_browser,
                'actual_browser': browser_instance.name,
                'available_browsers': cls.get_available_browsers()
            }
        except Exception as e:
            print(f"‚ùå Browser config verification failed: {e}")
            return None

------------------------------------------------- ./manual_scrape/src/browsers/browser_chromium.py --------------------------------------------------

"""Chromium browser implementation."""

from typing import List

class ChromiumBrowser:
    """Chromium browser configuration and launch handler."""

    name = "chromium"

    def get_launch_args(self) -> List[str]:
        """Get Chromium-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-dev-shm-usage',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--disable-features=TranslateUI',
            '--disable-ipc-flooding-protection',
            '--no-first-run',
            '--force-device-scale-factor=1',
            '--disable-default-apps'
        ]

    def launch(self, playwright_instance, headless: bool = True):
        """Launch Chromium browser with stealth configuration."""
        print(f"üöÄ Launching Chromium browser (headless: {headless})")
        print(f"üîß Chromium launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.chromium.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"‚úÖ Chromium browser launched successfully")
            return browser
        except Exception as e:
            print(f"‚ùå Failed to launch Chromium browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Chromium-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None

------------------------------------------------- ./outputs/unique_features.csv --------------------------------------------------

fname_de,fvalue_de,fname_fr,fvalue_fr,fname_it,fvalue_it
Ader-Querschnitt,1 mm¬≤,section des conducteurs,1 mm¬≤,sezione del conduttore,1 mm¬≤
Ader-Quer¬≠schnitt,1.5 mm¬≤,section des conducteurs,1.5 mm¬≤,sezione del conduttore,1.5 mm¬≤
Adern,3 St√ºck,fils,3 pi√®ce,vene,3 pezzo
Akku,-- St√ºck,accu,-- pi√®ce,batteria,-- pezzo
Akku-Kapazit√§t,-- Ah,capacit√© de l‚Äôaccu,-- Ah,capacit√† batteria,-- Ah
Akku-System,AP-System,syst√®me √† accu,syst√®me AP,sistema a batterie,sistema AP
Akku-Typ,--,type d‚Äôaccu,--,tipo di batteria,--
Antrieb,Vario,entra√Ænement,vario,azionamento,vario
Arbeitsdruck max.,1.5-2.5 bar,pression de travail max.,1.5-2.5 bar,pressione di lavoro mas.,1.5-2.5 bar
Auf¬≠bau,rund,structure,ronde,struttura,rotondo
Aussen-√ò,216 mm,√ò ex¬≠t√©¬≠rieur,216 mm,√ò esterno,216 mm
Beh√§ltervolumen,12 l,volume du r√©cipient,12 l,volume del recipiente,12 l
Blendschutz,nein,protection contre l'√©blouissement,non,,
Bohrleistung Beton,24 mm,cap. b√©ton,24 mm,cap. calcestruzzo,24 mm
Boh¬≠rungs-√ò,30 mm,√ò per√ßage,30 mm,√ò foro,30 mm
Breite,400 mm,largeur,400 mm,larghezza,400 mm
Drehstopp,ja,arr√™t de frappe,oui,arresto di rotazione,s√¨
Drehzahl,5500 min-1,vitesse,5500 min-1,numero di giri,5500 min-1
Ein-/Ausschalter,ja,interrupteur marche/arr√™t,oui,interruttore on/off,s√¨
Einsatz¬≠bereich,innen,secteur d'op√©ration,int√©rieur,campo d'impiego,interno
Einzelschlagenergie,2.1 J (EPTA),energie de frappe,2.1 J (EPTA),energia percussione singola,2.1 J (EPTA)
Fangvolumen,52 l,volume de r√©ception,52 l,volume di raccolta,52 l
Farbe,schwarz,couleur,noir,colore,nero
Farbe Geh√§use,weiss,couleur du bo√Ætier,blanc,colore della scatola,bianco
Funktion,√úberlastschutz,fonction,protection anti-surchage,funzione,protezione dal sovraccarico
Geh√§use,Kunststoff,bo√Ætier,mat. synth√©tique,scatola,plastica
Geh√§use¬≠material,Kunststoff,mat√©riau du bo√Ætier,mat. synth√©tique,materiale involucro,plastica
Gewicht,2.50 kg,poids,2.50 kg,peso,2.50 kg
Gewicht (ohne Akkupack),7.6 kg,poids (sans accu),7.6 kg,peso (senza pacco batterie),7.6 kg
Hubraum,27.2 cm¬≥,cylindr√©e,27.2 cm¬≥,cilindrata,27.2 cm¬≥
H√∂he,80 mm,hauteur,80 mm,altezza,80 mm
IP Schutzart,20,type de protection IP,20,tipo di protezione IP,20
IP Schutzart Steckdosen,20,type de protection IP prises,20,tipo di protezione IP prese,20
IP Schutzart Stecker,20,type de protection IP prise,20,tipo di protezione IP spina,20
Inhalt,5 l,contenu,5 l,contenuto,5 l
Kabelfarbe,weiss,couleur c√¢ble,blanc,colore cavo,bianco
Kabell√§nge,1.5 m,longueur du c√¢ble,1.5 m,lunghezza cavo,1.5 m
Kabelqualit√§t,TD,qualit√© du c√¢ble,TD,qualit√† cavo,TD
Kabel¬≠qualit√§t,TD,qualit√© du c√¢ble,TD,qualit√† cavo,TD
Kabel¬≠qualit√§t HAR,H05RR-F,qualit√© du c√¢ble HAR,H05RR-F,qualit√† cavo HAR,H05RR-F
Kinderschutz,ja,protection pour enfants,oui,protezione per bambini,s√¨
LED-Leuchte,mit,lampe DEL,avec,lampada LED,con
Ladeger√§t,--,chargeur,--,caricabatterie,--
Leerlaufdrehzahl,0-1100 min-1,nombre de tours √† vide,0-1100 min-1,numero di giri a vuoto,0-1100 min-1
Leiter¬≠art,feindr,type de conducteur,fil d'acier fin,tipo di conduttore,filo d'acciaio fine
Luftmenge,3284 l / min.,d√©bit air,3284 l / min.,volume d‚Äôaria,3284 l / min.
Motortechnik,2-MIX,technique du moteur,2-MIX,tecnica del motore,2-MIX
"Nenn-√ò DN (Schlauch, Rohr, D√ºse)",32 mm,"√ò nominal DN (tuyau, tube, buse)",32 mm,"√ò nominale DN (tubo flessibile, tubo, ugello)",32 mm
Orig. Nr.,SA04-011-7311,no. orig.,SA04-011-7311,no. orig.,SA04-011-7311
Saugkraft / Vakuum,200 mbar,puissance d‚Äôaspiration / sous-vide,200 mbar,forza d‚Äôaspirazione / sotto vuoto,200 mbar
Saugleistung mit Rundd√ºse,770 m¬≥ / h,capacit√© d‚Äôaspiration avec buse ronde,770 m¬≥ / h,capacit√† d‚Äôaspirazione con ugello rotondo,770 m¬≥ / e
Saugschlauchl√§nge,2.4 m,longueur du tuyau d‚Äôaspiration,2.4 m,lunghezza tubo aspirazione,2.4 m
Schalldruckpegel,82 dB(A),niveau de pression acoustique,82 dB(A),livello di pressione acustica,82 dB(A)
Schalldruckpegel (10 m) (LpA),89 dB(A),niveau de pression acoustique (10 m) (LpA),89 dB(A),livello di pressione acustica (10 m) (LpA),89 dB(A)
Schallleistungspegel (LwA),100 dB(A),niveau de puissance acoustique (LwA),100 dB(A),livello di potenza acustica (LwA),100 dB(A)
Scheiben-√ò,125 mm,√ò disques,125 mm,√ò dischi,125 mm
Schei¬≠ben¬≠beschich¬≠tung,"kratzfest, beschlagfrei",rev√™tement des verre,"r√©sistant aux rayures, antibu√©e",rivestimento del disco,"antigraffi, antiappannante"
Schei¬≠ben¬≠material,PC (Polycarbonat),mat√©riau des verres,polycarbonate (PC),materiale del disco,polycarbonato (PC)
Schienenl√§nge,40 cm,longueur du rail,40 cm,lunghezza rotaia,40 cm
Schlagzahl,0-4600 min-1,fr√©quence de frappe,0-4600 min-1,frequenza colpo al minuto,0-4600 min-1
Schnittbreite,460 mm,largeur de coupe,460 mm,larghezza del taglio,460 mm
Schnitth√∂he,20-100 mm,hauteur de coupe,20-100 mm,altezza taglio,20-100 mm
Schnittleistung 45¬∞,51 mm,profondeur de coupe 45¬∞,51 mm,profondit√† taglio 45¬∞,51 mm
Schnittleistung 90¬∞,67 mm,profondeur de coupe √† 90¬∞,67 mm,profondit√† taglio a 90¬∞,67 mm
Schnittpositionen,7,positions de coupe,7,posizioni di taglio,7
Schnitt¬≠breite,2.6 mm,largeur de coupe,2.6 mm,larghezza del taglio,2.6 mm
Spannung,18 V,tension,18 V,tensione,18 V
Span¬≠nung,220-240 V,ten¬≠si¬≠on,220-240 V,ten¬≠sio¬≠ne,220-240 V
Span¬≠winkel,-5 ¬∞,angle,-5 ¬∞,angolo di spoglia superiore,-5 ¬∞
Stamm¬≠blatt¬≠st√§rke,1.7 mm,√©paisseur du corps de lame,1.7 mm,spessore lama,1.7 mm
Staubklasse,L,classe de poussi√®re,L,classe di polveri,L
Steckdosen,6 St√ºck,prises,6 pi√®ce,prese,6 pezzo
Steckdosen Typ,T13,prises type,T13,tipo presa,T13
Steckdosen-Ausrichtung,quer,direction de la prise,de travers,allineamento presa,di traverso
Stecker Typ,T12,fiche type,T12,tipo spina,T12
Steck¬≠dosen Typ,4xT13,prises type,4xT13,tipo presa,4xT13
Stromst√§rke,10 A,amp√®rage,10 A,amperaggio,10 A
Strom¬≠st√§rke,10 A,amp√®rage,10 A,amperaggio,10 A
S√§geblatt-√ò,190 mm,√ò de la lame de scie,190 mm,√ò della lama,190 mm
S√§geblatt-√ò Bohrung,30 mm,√ò de forage de la lame de scie,30 mm,"√ò lama da sega, foro",30 mm
S√§gekettenteilung,3/8 P Zoll,pas de la cha√Æne de scie,3/8 P pouces,passo della catena deiie sega,3/8 P pollice
S√§gekettentyp,PS3 Pro,type de cha√Æne de scie,PS3 Pro,tipo di catena da sega,PS3 Pro
Tiefe,50 mm,profondeur,50 mm,profondit√†,50 mm
Treibglieddicke/Nutbreite,1.1 mm,√©paisseur du maillon/largeur de la rainure,1.1 mm,spessore maglia catena/larghezza incastro,1.1 mm
Treibglieder,55 St√ºck,maillons de pouss√©e,55 pi√®ce,maglie catena,55 pezzo
Trom¬≠mel¬≠farbe,blau / grau,couleur du tambour,bleu / gris,colore del tamburo,blu / grigio
UV-Schutz,ja,protection contre les UV,oui,protezione dai raggi UV,s√¨
Verpackung,T-STAK,emballage,T-STAK,imballaggio,T-STAK
Zahn¬≠form,WZ,denture,WZ,forma del dente,WZ
halogen¬≠frei,nein,exempt d'halog√®ne,non,,

------------------------------------------------- ./outputs/master_bmecat_dabag.json.backup2 --------------------------------------------------

{
  "metadata": {
    "created_at": "2025-10-16T11:38:43.726045",
    "last_updated": "2025-10-17T11:22:22.290522",
    "total_products": 6
  },
  "products": {
    "DCH273NT-XJ": {
      "SUPPLIER_PID": "DCH273NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCH273NT-XJ&markId=DCH273NT-XJ&artId=100292296&trefferUUID=939A9D21-61A6-4C4D-9CD919E95FCA05A7",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Leerlaufdrehzahl": "0-1100 min-1",
          "Drehstopp": "ja",
          "Schlagzahl": "0-4600 min-1",
          "Einzelschlagenergie": "2.1 J (EPTA)",
          "Bohrleistung Beton": "24 mm",
          "LED-Leuchte": "mit",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "2.50 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "nombre de tours √† vide": "0-1100 min-1",
          "arr√™t de frappe": "oui",
          "fr√©quence de frappe": "0-4600 min-1",
          "energie de frappe": "2.1 J (EPTA)",
          "cap. b√©ton": "24 mm",
          "lampe DEL": "avec",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "2.50 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "numero di giri a vuoto": "0-1100 min-1",
          "arresto di rotazione": "s√¨",
          "frequenza colpo al minuto": "0-4600 min-1",
          "energia percussione singola": "2.1 J (EPTA)",
          "cap. calcestruzzo": "24 mm",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "2.50 kg"
        }
      },
      "scraped_at": "2025-10-17T11:22:22.290511",
      "updated_at": "2025-10-17T11:22:22.290519"
    },
    "DCG405NT-XJ": {
      "SUPPLIER_PID": "DCG405NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCG405NT-XJ&markId=DCG405NT-XJ&artId=100321765&trefferUUID=D315C16A-D4EA-4D1B-8FF9BF9F8F6D43E4",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Scheiben-√ò": "125 mm",
          "Leerlaufdrehzahl": "9000 min-1",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "1.80 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "√ò disques": "125 mm",
          "nombre de tours √† vide": "9000 min-1",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "1.80 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "√ò dischi": "125 mm",
          "numero di giri a vuoto": "9000 min-1",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "1.80 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:03.420664"
    },
    "DCK329P2T-QW": {
      "SUPPLIER_PID": "DCK329P2T-QW",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCK329P2T-QW&markId=DCK329P2T-QW&artId=100683390&trefferUUID=15A26530-F3A5-4A78-884602D12ABD18D2",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "Li-Ion",
          "Akku-Kapazit√§t": "5 Ah",
          "Akku": "2 St√ºck",
          "Ladeger√§t": "DCB1104",
          "Verpackung": "T STAK-Box VI / T STAK-Box II"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "Li-Ion",
          "capacit√© de l‚Äôaccu": "5 Ah",
          "accu": "2 pi√®ce",
          "chargeur": "DCB1104",
          "emballage": "T STAK-Box VI / T STAK-Box II"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "Li-Ion",
          "capacit√† batteria": "5 Ah",
          "batteria": "2 pezzo",
          "caricabatterie": "DCB1104",
          "imballaggio": "T STAK-Box VI / T STAK-Box II"
        }
      },
      "scraped_at": "2025-10-16T11:48:51.723644",
      "updated_at": "2025-10-16T11:48:51.723650"
    },
    "DCS573NT-XJ": {
      "SUPPLIER_PID": "DCS573NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCS573NT-XJ&markId=DCS573NT-XJ&artId=100571860&trefferUUID=46DDD712-4A88-4D51-88021A3495BD79D7",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "S√§geblatt-√ò": "190 mm",
          "S√§geblatt-√ò Bohrung": "30 mm",
          "Schnittleistung 45¬∞": "51 mm",
          "Schnittleistung 90¬∞": "67 mm",
          "Drehzahl": "5500 min-1",
          "LED-Leuchte": "mit",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "3.60 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "√ò de la lame de scie": "190 mm",
          "√ò de forage de la lame de scie": "30 mm",
          "profondeur de coupe 45¬∞": "51 mm",
          "profondeur de coupe √† 90¬∞": "67 mm",
          "vitesse": "5500 min-1",
          "lampe DEL": "avec",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "3.60 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "√ò della lama": "190 mm",
          "√ò lama da sega, foro": "30 mm",
          "profondit√† taglio 45¬∞": "51 mm",
          "profondit√† taglio a 90¬∞": "67 mm",
          "numero di giri": "5500 min-1",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "3.60 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:20.815164"
    },
    "DT1953-QZ": {
      "SUPPLIER_PID": "DT1953-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1953-QZ&markId=DT1953-QZ&artId=100255912&trefferUUID=A626F45F-87B8-44F1-93F95608C46F4DF7",
      "languages": {
        "de": {
          "Aussen-√ò": "216 mm",
          "Boh¬≠rungs-√ò": "30 mm",
          "Schnitt¬≠breite": "2.6 mm",
          "Stamm¬≠blatt¬≠st√§rke": "1.7 mm",
          "Span¬≠winkel": "-5 ¬∞",
          "Zahn¬≠form": "WZ"
        },
        "fr": {
          "√ò ex¬≠t√©¬≠rieur": "216 mm",
          "√ò per√ßage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "√©paisseur du corps de lame": "1.7 mm",
          "angle": "-5 ¬∞",
          "denture": "WZ"
        },
        "it": {
          "√ò esterno": "216 mm",
          "√ò foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 ¬∞",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:29.230573"
    },
    "DT1952-QZ": {
      "SUPPLIER_PID": "DT1952-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1952-QZ&markId=DT1952-QZ&artId=100255912&trefferUUID=06749BE5-CA8A-4501-81644005B0BBFB13",
      "languages": {
        "de": {
          "Aussen-√ò": "216 mm",
          "Boh¬≠rungs-√ò": "30 mm",
          "Schnitt¬≠breite": "2.6 mm",
          "Stamm¬≠blatt¬≠st√§rke": "1.7 mm",
          "Span¬≠winkel": "-5 ¬∞",
          "Zahn¬≠form": "WZ"
        },
        "fr": {
          "√ò ex¬≠t√©¬≠rieur": "216 mm",
          "√ò per√ßage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "√©paisseur du corps de lame": "1.7 mm",
          "angle": "-5 ¬∞",
          "denture": "WZ"
        },
        "it": {
          "√ò esterno": "216 mm",
          "√ò foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 ¬∞",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:36.676448"
    }
  }
}
------------------------------------------------- ./outputs/master_bmecat_dabag.json.backup1 --------------------------------------------------

{
  "metadata": {
    "created_at": "2025-10-16T11:38:43.726045",
    "last_updated": "2025-10-17T17:15:20.761046",
    "total_products": 14
  },
  "products": {
    "DCH273NT-XJ": {
      "SUPPLIER_PID": "DCH273NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCH273NT-XJ&markId=DCH273NT-XJ&artId=100292296&trefferUUID=939A9D21-61A6-4C4D-9CD919E95FCA05A7",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Leerlaufdrehzahl": "0-1100 min-1",
          "Drehstopp": "ja",
          "Schlagzahl": "0-4600 min-1",
          "Einzelschlagenergie": "2.1 J (EPTA)",
          "Bohrleistung Beton": "24 mm",
          "LED-Leuchte": "mit",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "2.50 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "nombre de tours √† vide": "0-1100 min-1",
          "arr√™t de frappe": "oui",
          "fr√©quence de frappe": "0-4600 min-1",
          "energie de frappe": "2.1 J (EPTA)",
          "cap. b√©ton": "24 mm",
          "lampe DEL": "avec",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "2.50 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "numero di giri a vuoto": "0-1100 min-1",
          "arresto di rotazione": "s√¨",
          "frequenza colpo al minuto": "0-4600 min-1",
          "energia percussione singola": "2.1 J (EPTA)",
          "cap. calcestruzzo": "24 mm",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "2.50 kg"
        }
      },
      "scraped_at": "2025-10-17T11:22:22.290511",
      "updated_at": "2025-10-17T11:22:22.290519"
    },
    "DCG405NT-XJ": {
      "SUPPLIER_PID": "DCG405NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCG405NT-XJ&markId=DCG405NT-XJ&artId=100321765&trefferUUID=D315C16A-D4EA-4D1B-8FF9BF9F8F6D43E4",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Scheiben-√ò": "125 mm",
          "Leerlaufdrehzahl": "9000 min-1",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "1.80 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "√ò disques": "125 mm",
          "nombre de tours √† vide": "9000 min-1",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "1.80 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "√ò dischi": "125 mm",
          "numero di giri a vuoto": "9000 min-1",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "1.80 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:03.420664"
    },
    "DCK329P2T-QW": {
      "SUPPLIER_PID": "DCK329P2T-QW",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCK329P2T-QW&markId=DCK329P2T-QW&artId=100683390&trefferUUID=15A26530-F3A5-4A78-884602D12ABD18D2",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "Li-Ion",
          "Akku-Kapazit√§t": "5 Ah",
          "Akku": "2 St√ºck",
          "Ladeger√§t": "DCB1104",
          "Verpackung": "T STAK-Box VI / T STAK-Box II"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "Li-Ion",
          "capacit√© de l‚Äôaccu": "5 Ah",
          "accu": "2 pi√®ce",
          "chargeur": "DCB1104",
          "emballage": "T STAK-Box VI / T STAK-Box II"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "Li-Ion",
          "capacit√† batteria": "5 Ah",
          "batteria": "2 pezzo",
          "caricabatterie": "DCB1104",
          "imballaggio": "T STAK-Box VI / T STAK-Box II"
        }
      },
      "scraped_at": "2025-10-16T11:48:51.723644",
      "updated_at": "2025-10-16T11:48:51.723650"
    },
    "DCS573NT-XJ": {
      "SUPPLIER_PID": "DCS573NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCS573NT-XJ&markId=DCS573NT-XJ&artId=100571860&trefferUUID=46DDD712-4A88-4D51-88021A3495BD79D7",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "S√§geblatt-√ò": "190 mm",
          "S√§geblatt-√ò Bohrung": "30 mm",
          "Schnittleistung 45¬∞": "51 mm",
          "Schnittleistung 90¬∞": "67 mm",
          "Drehzahl": "5500 min-1",
          "LED-Leuchte": "mit",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "3.60 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "√ò de la lame de scie": "190 mm",
          "√ò de forage de la lame de scie": "30 mm",
          "profondeur de coupe 45¬∞": "51 mm",
          "profondeur de coupe √† 90¬∞": "67 mm",
          "vitesse": "5500 min-1",
          "lampe DEL": "avec",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "3.60 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "√ò della lama": "190 mm",
          "√ò lama da sega, foro": "30 mm",
          "profondit√† taglio 45¬∞": "51 mm",
          "profondit√† taglio a 90¬∞": "67 mm",
          "numero di giri": "5500 min-1",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "3.60 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:20.815164"
    },
    "DT1953-QZ": {
      "SUPPLIER_PID": "DT1953-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1953-QZ&markId=DT1953-QZ&artId=100255912&trefferUUID=A626F45F-87B8-44F1-93F95608C46F4DF7",
      "languages": {
        "de": {
          "Aussen-√ò": "216 mm",
          "Boh¬≠rungs-√ò": "30 mm",
          "Schnitt¬≠breite": "2.6 mm",
          "Stamm¬≠blatt¬≠st√§rke": "1.7 mm",
          "Span¬≠winkel": "-5 ¬∞",
          "Zahn¬≠form": "WZ"
        },
        "fr": {
          "√ò ex¬≠t√©¬≠rieur": "216 mm",
          "√ò per√ßage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "√©paisseur du corps de lame": "1.7 mm",
          "angle": "-5 ¬∞",
          "denture": "WZ"
        },
        "it": {
          "√ò esterno": "216 mm",
          "√ò foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 ¬∞",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:29.230573"
    },
    "DT1952-QZ": {
      "SUPPLIER_PID": "DT1952-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1952-QZ&markId=DT1952-QZ&artId=100255912&trefferUUID=06749BE5-CA8A-4501-81644005B0BBFB13",
      "languages": {
        "de": {
          "Aussen-√ò": "216 mm",
          "Boh¬≠rungs-√ò": "30 mm",
          "Schnitt¬≠breite": "2.6 mm",
          "Stamm¬≠blatt¬≠st√§rke": "1.7 mm",
          "Span¬≠winkel": "-5 ¬∞",
          "Zahn¬≠form": "WZ"
        },
        "fr": {
          "√ò ex¬≠t√©¬≠rieur": "216 mm",
          "√ò per√ßage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "√©paisseur du corps de lame": "1.7 mm",
          "angle": "-5 ¬∞",
          "denture": "WZ"
        },
        "it": {
          "√ò esterno": "216 mm",
          "√ò foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 ¬∞",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:36.676448"
    },
    "00008840364": {
      "SUPPLIER_PID": "00008840364",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=00008840364&markId=00008840364&artId=100322385&trefferUUID=93AF684A-3DAC-4F71-A0A704949AAE0BA4",
      "languages": {
        "de": {
          "Schei¬≠ben¬≠material": "PC (Polycarbonat)",
          "Schei¬≠ben¬≠beschich¬≠tung": "kratzfest, beschlagfrei",
          "UV-Schutz": "ja",
          "Blendschutz": "nein"
        },
        "fr": {
          "mat√©riau des verres": "polycarbonate (PC)",
          "rev√™tement des verre": "r√©sistant aux rayures, antibu√©e",
          "protection contre les UV": "oui",
          "protection contre l'√©blouissement": "non"
        },
        "it": {
          "materiale del disco": "polycarbonato (PC)",
          "rivestimento del disco": "antigraffi, antiappannante",
          "protezione dai raggi UV": "s√¨"
        }
      },
      "scraped_at": "2025-10-17T17:13:53.533489"
    },
    "00008840365": {
      "SUPPLIER_PID": "00008840365",
      "product_url": null,
      "languages": {},
      "scraped_at": "2025-10-17T17:14:04.942165"
    },
    "00008840366": {
      "SUPPLIER_PID": "00008840366",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=00008840366&markId=00008840366&artId=100322385&trefferUUID=B6A7E29B-F8E1-4583-A9A54FC06BDA5D6E",
      "languages": {
        "de": {
          "Schei¬≠ben¬≠material": "PC (Polycarbonat)",
          "Schei¬≠ben¬≠beschich¬≠tung": "kratzfest, beschlagfrei",
          "UV-Schutz": "ja",
          "Blendschutz": "nein"
        },
        "fr": {
          "mat√©riau des verres": "polycarbonate (PC)",
          "rev√™tement des verre": "r√©sistant aux rayures, antibu√©e",
          "protection contre les UV": "oui",
          "protection contre l'√©blouissement": "non"
        },
        "it": {
          "materiale del disco": "polycarbonato (PC)",
          "rivestimento del disco": "antigraffi, antiappannante",
          "protezione dai raggi UV": "s√¨"
        }
      },
      "scraped_at": "2025-10-17T17:14:17.501249"
    },
    "42410110932": {
      "SUPPLIER_PID": "42410110932",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=42410110932&markId=42410110932&artId=100027592&trefferUUID=2387C24E-D914-48BE-B13EFD12AE2EA290",
      "languages": {
        "de": {
          "Motortechnik": "2-MIX",
          "Saugleistung mit Rundd√ºse": "770 m¬≥ / h",
          "Hubraum": "27.2 cm¬≥",
          "Gewicht": "5.70 kg"
        },
        "fr": {
          "technique du moteur": "2-MIX",
          "capacit√© d‚Äôaspiration avec buse ronde": "770 m¬≥ / h",
          "cylindr√©e": "27.2 cm¬≥",
          "poids": "5.70 kg"
        },
        "it": {
          "tecnica del motore": "2-MIX",
          "capacit√† d‚Äôaspirazione con ugello rotondo": "770 m¬≥ / e",
          "cilindrata": "27.2 cm¬≥",
          "peso": "5.70 kg"
        }
      },
      "scraped_at": "2025-10-17T17:14:30.170351"
    },
    "MA032000021": {
      "SUPPLIER_PID": "MA032000021",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=MA032000021&markId=MA032000021&artId=100785489&trefferUUID=B76AEEF2-63F1-47F0-A9C97D53062638BB",
      "languages": {
        "de": {
          "Spannung": "36 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Schienenl√§nge": "40 cm",
          "S√§gekettentyp": "PS3 Pro",
          "S√§gekettenteilung": "3/8 P Zoll",
          "Treibglieddicke/Nutbreite": "1.1 mm",
          "Treibglieder": "55 St√ºck",
          "Schalldruckpegel (10 m) (LpA)": "89 dB(A)",
          "Schallleistungspegel (LwA)": "100 dB(A)",
          "Akku-System": "AP-System",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Gewicht": "3.10 kg"
        },
        "fr": {
          "tension": "36 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "longueur du rail": "40 cm",
          "type de cha√Æne de scie": "PS3 Pro",
          "pas de la cha√Æne de scie": "3/8 P pouces",
          "√©paisseur du maillon/largeur de la rainure": "1.1 mm",
          "maillons de pouss√©e": "55 pi√®ce",
          "niveau de pression acoustique (10 m) (LpA)": "89 dB(A)",
          "niveau de puissance acoustique (LwA)": "100 dB(A)",
          "syst√®me √† accu": "syst√®me AP",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "poids": "3.10 kg"
        },
        "it": {
          "tensione": "36 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "lunghezza rotaia": "40 cm",
          "tipo di catena da sega": "PS3 Pro",
          "passo della catena deiie sega": "3/8 P pollice",
          "spessore maglia catena/larghezza incastro": "1.1 mm",
          "maglie catena": "55 pezzo",
          "livello di pressione acustica (10 m) (LpA)": "89 dB(A)",
          "livello di potenza acustica (LwA)": "100 dB(A)",
          "sistema a batterie": "sistema AP",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "peso": "3.10 kg"
        }
      },
      "scraped_at": "2025-10-17T17:14:42.130518"
    },
    "SA040117310": {
      "SUPPLIER_PID": "SA040117310",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=SA040117310&markId=SA040117310&artId=101141773&trefferUUID=565F5FFC-55CE-44CB-8F5AEA5594A522E3",
      "languages": {
        "de": {
          "Spannung": "36 V",
          "Akku-Typ": "--",
          "Akku-Kapazit√§t": "-- Ah",
          "Akku": "-- St√ºck",
          "Ladeger√§t": "--",
          "Luftmenge": "3284 l / min.",
          "Saugkraft / Vakuum": "200 mbar",
          "Beh√§ltervolumen": "12 l",
          "Nenn-√ò DN (Schlauch, Rohr, D√ºse)": "32 mm",
          "Saugschlauchl√§nge": "2.4 m",
          "Staubklasse": "L",
          "Akku-System": "AP-System",
          "Gewicht (ohne Akkupack)": "7.6 kg",
          "Orig. Nr.": "SA04-011-7311"
        },
        "fr": {
          "tension": "36 V",
          "type d‚Äôaccu": "--",
          "capacit√© de l‚Äôaccu": "-- Ah",
          "accu": "-- pi√®ce",
          "chargeur": "--",
          "d√©bit air": "3284 l / min.",
          "puissance d‚Äôaspiration / sous-vide": "200 mbar",
          "volume du r√©cipient": "12 l",
          "√ò nominal DN (tuyau, tube, buse)": "32 mm",
          "longueur du tuyau d‚Äôaspiration": "2.4 m",
          "classe de poussi√®re": "L",
          "syst√®me √† accu": "syst√®me AP",
          "poids (sans accu)": "7.6 kg",
          "no. orig.": "SA04-011-7311"
        },
        "it": {
          "tensione": "36 V",
          "tipo di batteria": "--",
          "capacit√† batteria": "-- Ah",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "volume d‚Äôaria": "3284 l / min.",
          "forza d‚Äôaspirazione / sotto vuoto": "200 mbar",
          "volume del recipiente": "12 l",
          "√ò nominale DN (tubo flessibile, tubo, ugello)": "32 mm",
          "lunghezza tubo aspirazione": "2.4 m",
          "classe di polveri": "L",
          "sistema a batterie": "sistema AP",
          "peso (senza pacco batterie)": "7.6 kg",
          "no. orig.": "SA04-011-7311"
        }
      },
      "scraped_at": "2025-10-17T17:14:55.723146"
    },
    "WA412000002": {
      "SUPPLIER_PID": "WA412000002",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=WA412000002&markId=WA412000002&artId=101075327&trefferUUID=C01F1033-B3B8-40E0-85279B2936019036",
      "languages": {
        "de": {
          "Spannung": "36 V",
          "Akku-Typ": "Li-Ion",
          "Akku-Kapazit√§t": "7.2 Ah",
          "Schalldruckpegel": "82 dB(A)",
          "Antrieb": "Vario",
          "Schnittbreite": "460 mm",
          "Schnitth√∂he": "20-100 mm",
          "Schnittpositionen": "7",
          "Fangvolumen": "52 l",
          "Akku": "1 St√ºck",
          "Ladeger√§t": "AL 301",
          "Akku-System": "AP-System",
          "Gewicht": "27.00 kg"
        },
        "fr": {
          "tension": "36 V",
          "type d‚Äôaccu": "Li-Ion",
          "capacit√© de l‚Äôaccu": "7.2 Ah",
          "niveau de pression acoustique": "82 dB(A)",
          "entra√Ænement": "vario",
          "largeur de coupe": "460 mm",
          "hauteur de coupe": "20-100 mm",
          "positions de coupe": "7",
          "volume de r√©ception": "52 l",
          "accu": "1 pi√®ce",
          "chargeur": "AL 301",
          "syst√®me √† accu": "syst√®me AP",
          "poids": "27.00 kg"
        },
        "it": {
          "tensione": "36 V",
          "tipo di batteria": "Li-Ion",
          "capacit√† batteria": "7.2 Ah",
          "livello di pressione acustica": "82 dB(A)",
          "azionamento": "vario",
          "larghezza del taglio": "460 mm",
          "altezza taglio": "20-100 mm",
          "posizioni di taglio": "7",
          "volume di raccolta": "52 l",
          "batteria": "1 pezzo",
          "caricabatterie": "AL 301",
          "sistema a batterie": "sistema AP",
          "peso": "27.00 kg"
        }
      },
      "scraped_at": "2025-10-17T17:15:08.784664"
    },
    "SA090117010": {
      "SUPPLIER_PID": "SA090117010",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=SA090117010&markId=SA090117010&artId=101157877&trefferUUID=4A4E2C53-B8BD-42CB-A686F36BF38DB1E3",
      "languages": {
        "de": {
          "Spannung": "10.8 V",
          "Akku-Typ": "Li-Ion",
          "Akku-Kapazit√§t": "2.1 Ah",
          "Akku": "1 St√ºck",
          "Inhalt": "5 l",
          "Arbeitsdruck max.": "1.5-2.5 bar",
          "Ladeger√§t": "AL 1",
          "Akku-System": "AS-System",
          "Gewicht (ohne Akkupack)": "2.1 kg",
          "Orig. Nr.": "SA09-011-7010"
        },
        "fr": {
          "tension": "10.8 V",
          "type d‚Äôaccu": "Li-Ion",
          "capacit√© de l‚Äôaccu": "2.1 Ah",
          "accu": "1 pi√®ce",
          "contenu": "5 l",
          "pression de travail max.": "1.5-2.5 bar",
          "chargeur": "AL 1",
          "syst√®me √† accu": "syst√®me AS",
          "poids (sans accu)": "2.1 kg",
          "no. orig.": "SA09-011-7010"
        },
        "it": {
          "tensione": "10.8 V",
          "tipo di batteria": "Li-Ion",
          "capacit√† batteria": "2.1 Ah",
          "batteria": "1 pezzo",
          "contenuto": "5 l",
          "pressione di lavoro mas.": "1.5-2.5 bar",
          "caricabatterie": "AL 1",
          "sistema a batterie": "sistema AS",
          "peso (senza pacco batterie)": "2.1 kg",
          "no. orig.": "SA09-011-7010"
        }
      },
      "scraped_at": "2025-10-17T17:15:20.761034"
    }
  }
}
------------------------------------------------- ./BMEcat_transformer/ui/user_prompt.py --------------------------------------------------

"""User prompt handler for BMEcat_transformer.

Handles user interaction when an existing product is found in master JSON.
Shows existing data and prompts for update decision.
"""

from __future__ import annotations

from typing import Dict, Any, Literal


class UserPrompt:
    """Handle user prompts and display existing data."""

    @staticmethod
    def show_existing_data(supplier_pid: str, existing_data: Dict[str, Any]) -> None:
        """Display existing data from master JSON.

        Args:
            supplier_pid: The product ID.
            existing_data: Data currently in master JSON.
        """
        print("\n" + "=" * 80)
        print(f"üîç Product {supplier_pid} already exists in master JSON")
        print("-" * 80)

        # Show timestamps
        scraped_at = existing_data.get("scraped_at", "N/A")
        updated_at = existing_data.get("updated_at", scraped_at)
        print(f"Last scraped: {scraped_at}")
        if updated_at != scraped_at:
            print(f"Last updated: {updated_at}")

        # Show language coverage
        existing_langs = existing_data.get("languages", {})
        print("\nLanguage Coverage:")
        for lang in ["de", "fr", "it"]:
            spec_count = len(existing_langs.get(lang, {}))
            print(f"  {lang.upper()}: {spec_count} specs")

        # Show URL
        existing_url = existing_data.get("product_url", "N/A")
        print(f"\nURL: {existing_url}")

        print("-" * 80)

    @staticmethod
    def prompt_update_decision(supplier_pid: str) -> Literal["update", "skip"]:
        """Prompt user to decide whether to update or skip.

        Args:
            supplier_pid: The product ID.

        Returns:
            'update' if user wants to update, 'skip' otherwise.
        """
        while True:
            response = input(f"Update {supplier_pid}? [y/n]: ").strip().lower()
            if response in ["y", "yes"]:
                return "update"
            elif response in ["n", "no"]:
                return "skip"
            else:
                print("‚ö†Ô∏è  Invalid input. Please enter 'y' or 'n'.")

------------------------------------------------- ./BMEcat_transformer/ui/visualize_tables.py --------------------------------------------------

#!/usr/bin/env python3
"""
Visualize BMEcat comparison tables in terminal or export to Excel/CSV.

Usage:
    python3 visualize_comparison_tables.py --product DCG405NT-XJ --lang de
    python3 visualize_comparison_tables.py --master  # Show all products
    python3 visualize_comparison_tables.py --product DT1953-QZ --lang fr --export excel
"""

import json
import argparse
from pathlib import Path
from typing import Dict, Any, List
from tabulate import tabulate
import sys

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("‚ö†Ô∏è  pandas not available. Excel export disabled.")


def load_comparison_table(product_id: str, lang: str, base_dir: str = "outputs/comparison_tables") -> Dict[str, Any]:
    """Load a single comparison table JSON file."""
    filepath = Path(base_dir) / f"comparison_{product_id}_{lang}.json"
    
    if not filepath.exists():
        print(f"‚ùå File not found: {filepath}")
        sys.exit(1)
    
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_master_catalog(base_dir: str = "outputs/comparison_tables") -> Dict[str, Any]:
    """Load the master comparison catalog."""
    filepath = Path(base_dir) / "master_comparison_catalog.json"
    
    if not filepath.exists():
        print(f"‚ùå Master catalog not found: {filepath}")
        sys.exit(1)
    
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def visualize_table(data: Dict[str, Any], show_units: bool = False) -> None:
    """Pretty-print a comparison table to terminal."""
    supplier_id = data.get("supplier_id", "Unknown")
    lang = data.get("lang", "xx")
    url = data.get("product_url")
    
    print("\n" + "=" * 120)
    print(f"Product: {supplier_id} | Language: {lang.upper()}")
    if url:
        print(f"URL: {url}")
    print("=" * 120)
    
    rows = data.get("rows", [])
    units = data.get("units", [])
    
    if not rows:
        print("‚ö†Ô∏è  No data rows found.")
        return
    
    # Build table data
    table_data = []
    headers = [
        "Feature",
        "Original Name", "Original Value",
        "DABAG Name", "DABAG Value",
        "Web Name", "Web Value",
        "AI Name", "AI Value"
    ]
    
    for i, row in enumerate(rows):
        # Get the primary feature name (first non-empty)
        fname = (row.get("original_fname") or 
                row.get("dabag_fname") or 
                row.get("web_fname") or 
                row.get("ai_fname", ""))

        row_unit = units[i] if i < len(units) else {}

        def format_value_with_unit(value, unit, has_fname):
            # If no feature name exists, return empty string
            if not has_fname:
                return ""
            # If no value, show -- with unit if available
            if not value or value == "":
                unit_str = str(unit).strip() if unit is not None else ""
                if unit_str:
                    return f"-- {unit_str}"
                return "--"
            # Normal case: value exists
            val_str = str(value).strip()
            unit_str = str(unit).strip() if unit is not None else ""
            if unit_str:
                return f"{val_str} {unit_str}"
            return val_str

        table_row = [
            fname,
            row.get("original_fname", ""),
            format_value_with_unit(
                row.get("original_fvalue"),
                row_unit.get("original_funit"),
                has_fname=bool(row.get("original_fname"))
            ),
            row.get("dabag_fname", ""),
            format_value_with_unit(
                row.get("dabag_fvalue"),
                row_unit.get("dabag_funit"),
                has_fname=bool(row.get("dabag_fname"))
            ),
            row.get("web_fname", ""),
            format_value_with_unit(
                row.get("web_fvalue"),
                None,
                has_fname=bool(row.get("web_fname"))
            ),
            row.get("ai_fname", ""),
            format_value_with_unit(
                row.get("ai_fvalue"),
                None,
                has_fname=bool(row.get("ai_fname"))
            )
        ]
        table_data.append(table_row)
    
    # Print main table
    print(tabulate(table_data, headers=headers, tablefmt="grid", maxcolwidths=[20, 15, 20, 15, 20, 15, 20, 15, 20]))
    
    
    
    # Print statistics
    print("\n" + "-" * 120)
    print("STATISTICS:")
    print("-" * 120)
    
    orig_count = sum(1 for r in rows if r.get("original_fname"))
    dabag_count = sum(1 for r in rows if r.get("dabag_fname"))
    web_count = sum(1 for r in rows if r.get("web_fname"))
    ai_count = sum(1 for r in rows if r.get("ai_fname"))
    
    print(f"Original features: {orig_count}")
    print(f"DABAG features: {dabag_count}")
    print(f"Web scraped features: {web_count}")
    print(f"AI mapped features: {ai_count}")
    print(f"Total unique features: {len(rows)}")


def export_to_excel(data: Dict[str, Any], output_path: str = None) -> None:
    """Export comparison table to Excel."""
    if not PANDAS_AVAILABLE:
        print("‚ùå pandas not installed. Run: pip install pandas openpyxl")
        return
    
    supplier_id = data.get("supplier_id", "unknown")
    lang = data.get("lang", "xx")
    
    if not output_path:
        output_path = f"comparison_{supplier_id}_{lang}.xlsx"
    
    rows = data.get("rows", [])
    units = data.get("units", [])

    enhanced_rows = []
    for i, row in enumerate(rows):
        row_unit = units[i] if i < len(units) else {}
        enhanced_row = dict(row)
        # Determine primary feature name for placeholder behavior
        fname = (row.get("original_fname") or 
                 row.get("dabag_fname") or 
                 row.get("web_fname") or 
                 row.get("ai_fname", ""))
        placeholder = "" if (not fname or str(fname).strip() == "") else "--"
        if row.get("original_fvalue"):
            unit = row_unit.get("original_funit", "")
            combined = f"{row['original_fvalue']} {unit}".strip()
            enhanced_row["original_fvalue"] = combined if combined else placeholder
        else:
            enhanced_row["original_fvalue"] = placeholder
        if row.get("dabag_fvalue"):
            unit = row_unit.get("dabag_funit", "")
            combined = f"{row['dabag_fvalue']} {unit}".strip()
            enhanced_row["dabag_fvalue"] = combined if combined else placeholder
        else:
            enhanced_row["dabag_fvalue"] = placeholder
        enhanced_rows.append(enhanced_row)

    # Create main dataframe
    df_main = pd.DataFrame(enhanced_rows)

    # Reorder columns for readability
    col_order = [
        "original_fname", "original_fvalue",
        "dabag_fname", "dabag_fvalue",
        "web_fname", "web_fvalue",
        "ai_fname", "ai_fvalue"
    ]
    df_main = df_main[[c for c in col_order if c in df_main.columns]]
    
    # Write to Excel with multiple sheets
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        df_main.to_excel(writer, sheet_name='Comparison', index=False)
        
        # Add metadata sheet
        metadata = pd.DataFrame({
            'Property': ['Supplier ID', 'Language', 'Product URL'],
            'Value': [supplier_id, lang, data.get("product_url", "N/A")]
        })
        metadata.to_excel(writer, sheet_name='Metadata', index=False)
    
    print(f"‚úÖ Exported to Excel: {output_path}")


def export_to_csv(data: Dict[str, Any], output_path: str = None) -> None:
    """Export comparison table to CSV."""
    supplier_id = data.get("supplier_id", "unknown")
    lang = data.get("lang", "xx")
    
    if not output_path:
        output_path = f"comparison_{supplier_id}_{lang}.csv"
    
    rows = data.get("rows", [])
    units = data.get("units", [])

    enhanced_rows = []
    for i, row in enumerate(rows):
        row_unit = units[i] if i < len(units) else {}
        enhanced_row = dict(row)
        # Determine primary feature name for placeholder behavior
        fname = (row.get("original_fname") or 
                 row.get("dabag_fname") or 
                 row.get("web_fname") or 
                 row.get("ai_fname", ""))
        placeholder = "" if (not fname or str(fname).strip() == "") else "--"
        if row.get("original_fvalue"):
            unit = row_unit.get("original_funit", "")
            combined = f"{row['original_fvalue']} {unit}".strip()
            enhanced_row["original_fvalue"] = combined if combined else placeholder
        else:
            enhanced_row["original_fvalue"] = placeholder
        if row.get("dabag_fvalue"):
            unit = row_unit.get("dabag_funit", "")
            combined = f"{row['dabag_fvalue']} {unit}".strip()
            enhanced_row["dabag_fvalue"] = combined if combined else placeholder
        else:
            enhanced_row["dabag_fvalue"] = placeholder
        enhanced_rows.append(enhanced_row)

    if PANDAS_AVAILABLE:
        df = pd.DataFrame(enhanced_rows)
        df.to_csv(output_path, index=False, encoding='utf-8')
    else:
        import csv
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            if enhanced_rows:
                writer = csv.DictWriter(f, fieldnames=enhanced_rows[0].keys())
                writer.writeheader()
                writer.writerows(enhanced_rows)
    
    print(f"‚úÖ Exported to CSV: {output_path}")


def show_master_overview(catalog: Dict[str, Any]) -> None:
    """Display overview of all products in master catalog."""
    products = catalog.get("products", {})
    
    print("\n" + "=" * 80)
    print(f"MASTER COMPARISON CATALOG - {len(products)} Products")
    print("=" * 80)
    
    overview_data = []
    for supplier_id, product_data in products.items():
        languages = product_data.get("languages", {})
        
        de_features = len(languages.get("de", {}).get("rows", []))
        fr_features = len(languages.get("fr", {}).get("rows", []))
        it_features = len(languages.get("it", {}).get("rows", []))
        
        overview_data.append([
            supplier_id,
            de_features,
            fr_features,
            it_features,
            max(de_features, fr_features, it_features)
        ])
    
    print(tabulate(
        overview_data,
        headers=["Supplier ID", "DE Features", "FR Features", "IT Features", "Max"],
        tablefmt="grid"
    ))
    
    print("\nüí° To view a specific product:")
    print("   python3 visualize_comparison_tables.py --product <SUPPLIER_ID> --lang <de|fr|it>")


def main():
    parser = argparse.ArgumentParser(
        description="Visualize BMEcat comparison tables"
    )
    
    parser.add_argument(
        "--product",
        help="Supplier ID / product code (e.g., DCG405NT-XJ)"
    )
    
    parser.add_argument(
        "--lang",
        choices=["de", "fr", "it"],
        help="Language code"
    )
    
    parser.add_argument(
        "--master",
        action="store_true",
        help="Show master catalog overview"
    )
    
    parser.add_argument(
        "--export",
        choices=["excel", "csv"],
        help="Export to file format"
    )
    
    parser.add_argument(
        "--no-units",
        action="store_true",
        help="Hide units table"
    )
    
    parser.add_argument(
        "--output",
        help="Custom output filename for export"
    )
    
    parser.add_argument(
        "--dir",
        default="outputs/comparison_tables",
        help="Base directory for comparison tables"
    )
    
    args = parser.parse_args()
    
    # Show master overview
    if args.master:
        catalog = load_master_catalog(args.dir)
        show_master_overview(catalog)
        return
    
    # Validate product and lang
    if not args.product or not args.lang:
        print("‚ùå Error: --product and --lang are required (or use --master)")
        parser.print_help()
        sys.exit(1)
    
    # Load and visualize
    data = load_comparison_table(args.product, args.lang, args.dir)
    
    # Export if requested
    if args.export == "excel":
        export_to_excel(data, args.output)
    elif args.export == "csv":
        export_to_csv(data, args.output)
    else:
        # Default: terminal visualization
        visualize_table(data, show_units=not args.no_units)


if __name__ == "__main__":
    main()
------------------------------------------------- ./BMEcat_transformer/count_products.py --------------------------------------------------

from lxml import etree as LXML_ET

original_xml = "/Users/dannycrescimone/Documents/data_scraping/DEWALT_BMEcat_Original.xml"
dabag_xml = "/Users/dannycrescimone/Documents/data_scraping/DEWALT_Version_DABAG.xml"

print("="*70)
print("ORIGINAL XML:")
with open(original_xml, 'rb') as f:
    root = LXML_ET.fromstring(f.read(), LXML_ET.XMLParser(recover=True))
products = root.xpath('.//*[local-name()="PRODUCT"]')
print(f"Total PRODUCT nodes: {len(products)}")
for i, prod in enumerate(products[:10], 1):  # Show first 10
    pids = prod.xpath('.//*[local-name()="SUPPLIER_PID"]')
    pid = pids[0].text if pids and pids[0].text else "NO PID"
    print(f"  {i}. {pid}")

print("\n" + "="*70)
print("DABAG XML:")
with open(dabag_xml, 'rb') as f:
    root = LXML_ET.fromstring(f.read(), LXML_ET.XMLParser(recover=True))
products = root.xpath('.//*[local-name()="PRODUCT"]')
print(f"Total PRODUCT nodes: {len(products)}")
for i, prod in enumerate(products[:10], 1):
    pids = prod.xpath('.//*[local-name()="SUPPLIER_PID"]')
    pid = pids[0].text if pids and pids[0].text else "NO PID"
    print(f"  {i}. {pid}")

print("="*70)

------------------------------------------------- ./BMEcat_transformer/config.py --------------------------------------------------

from __future__ import annotations

"""
Configuration settings for BMEcat_transformer.
Follows the style of existing configs in `doc_processor/` and `easy_rich/`.
"""

import os
from dotenv import load_dotenv

# Load environment variables from root .env
load_dotenv()

# Scraping method selection: "firecrawl" or "playwright"
SCRAPING_METHOD: str = os.getenv("SCRAPING_METHOD", "firecrawl").strip().lower()

# DABAG settings
DABAG_BASE_URL: str = os.getenv("DABAG_BASE_URL", "https://www.dabag.ch")

# Language mapping for DABAG (de=1, fr=2, it=3)
LANGUAGES: dict[str, int] = {
    "de": 1,
    "fr": 2,
    "it": 3,
}

# Output directory for saved JSON
OUTPUT_DIR: str = os.getenv("BME_OUTPUT_DIR", "outputs/")

# Comparison tables output directory and master filename
# Default COMPARISON_TABLES_DIR nests under OUTPUT_DIR
COMPARISON_TABLES_DIR: str = os.getenv(
    "COMPARISON_TABLES_DIR",
    os.path.join(OUTPUT_DIR, "comparison_tables/")
)
# Scraped text output directory for intermediate UDX XML text extraction
SCRAPED_TEXT_DIR: str = os.getenv(
    "SCRAPED_TEXT_DIR",
    os.path.join(OUTPUT_DIR, "scraped_text/")
)
MASTER_COMPARISON_FILENAME: str = os.getenv(
    "MASTER_COMPARISON_FILENAME",
    "master_comparison_catalog.json"
)

# Master JSON settings
MASTER_JSON_FILENAME: str = os.getenv("MASTER_JSON_FILENAME", "master_bmecat_dabag.json")
MASTER_JSON_BACKUP_COUNT: int = int(os.getenv("MASTER_JSON_BACKUP_COUNT", "2"))

# Grok AI Configuration for XML Specs Extraction
GROK_API_KEY: str | None = os.getenv("GROK_API_KEY")
GROK_MODEL: str = os.getenv("GROK_MODEL", "grok-4-fast-reasoning")
GROK_BASE_URL: str = "https://api.x.ai/v1"
GROK_CONFIDENCE_THRESHOLD: float = 0.70

# AI Generated Features Management
AI_FEATURES_FILENAME: str = "ai_generated_features.json"
AI_FEATURES_PATH: str = os.path.join(OUTPUT_DIR, AI_FEATURES_FILENAME)

# UDX XML Fields Mapping
UDX_FIELD_MAPPING: dict[str, str] = {
    "produktstaerken": "UDX.EDXF.LANGTEXT",
    "lieferumfang": "UDX.EDXF.LIEFERUMFANG",
    "technische_daten": "UDX.EDXF.TECHNISCHE_DATEN",
    "garantie": "UDX.EDXF.GARANTIEBEDINGUNGEN",
    "anwendungsbeispiele": "UDX.EDXF.ANWENDUNGSBEISPIELE"
}

# Optional keys depending on method
FIRECRAWL_API_KEY: str | None = os.getenv("FIRECRAWL_API_KEY")

# Validation
if SCRAPING_METHOD not in {"firecrawl", "playwright"}:
    raise ValueError(
        "SCRAPING_METHOD must be either 'firecrawl' or 'playwright'"
    )

if SCRAPING_METHOD == "firecrawl" and not FIRECRAWL_API_KEY:
    raise ValueError("FIRECRAWL_API_KEY not found in environment variables for firecrawl mode")

# Grok validation
if GROK_API_KEY:
    print(f"‚úÖ Grok API configured: {GROK_MODEL}")

# Normalize directories to have trailing slash
if not OUTPUT_DIR.endswith("/"):
    OUTPUT_DIR = OUTPUT_DIR + "/"
if not COMPARISON_TABLES_DIR.endswith("/"):
    COMPARISON_TABLES_DIR = COMPARISON_TABLES_DIR + "/"
if not SCRAPED_TEXT_DIR.endswith("/"):
    SCRAPED_TEXT_DIR = SCRAPED_TEXT_DIR + "/"

------------------------------------------------- ./BMEcat_transformer/core/master_json_manager.py --------------------------------------------------

"""Master JSON manager for BMEcat_transformer.

Manages a persistent master JSON file that tracks all scraped products.
Provides functionality to check existence, append new entries, update existing ones,
and maintain backup versions.
"""

from __future__ import annotations

import json
import os
import shutil
from datetime import datetime
from typing import Dict, Any, Optional, Tuple


class MasterJSONManager:
    """Manage the master JSON file for scraped product data."""

    def __init__(self, master_filename: str, output_dir: str, backup_count: int = 2) -> None:
        """Initialize the master JSON manager.

        Args:
            master_filename: Name of the master JSON file.
            output_dir: Directory where master JSON is stored.
            backup_count: Number of backup versions to keep.
        """
        self.master_filename = master_filename
        self.output_dir = output_dir
        self.backup_count = backup_count
        self.master_path = os.path.join(output_dir, master_filename)
        self.data: Dict[str, Any] = {"metadata": {}, "products": {}}
        os.makedirs(output_dir, exist_ok=True)

    def load(self) -> None:
        """Load the master JSON file. Creates new if doesn't exist."""
        if os.path.exists(self.master_path):
            try:
                with open(self.master_path, "r", encoding="utf-8") as f:
                    self.data = json.load(f)
                print(f"‚úì Loaded master JSON from: {self.master_path}")
            except (json.JSONDecodeError, IOError) as e:
                print(f"‚ö†Ô∏è  Warning: Failed to load master JSON ({e}). Starting fresh.")
                self._initialize_fresh()
        else:
            print(f"‚úì Master JSON not found. Starting fresh: {self.master_path}")
            self._initialize_fresh()

    def _initialize_fresh(self) -> None:
        """Initialize a fresh master JSON structure."""
        self.data = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "total_products": 0,
            },
            "products": {},
        }

    def check_id_exists(self, supplier_pid: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
        """Check if a SUPPLIER_PID exists in master JSON.

        Args:
            supplier_pid: The product ID to check.

        Returns:
            Tuple of (exists: bool, existing_data: dict or None)
        """
        exists = supplier_pid in self.data.get("products", {})
        existing_data = self.data["products"].get(supplier_pid) if exists else None
        return exists, existing_data

    def append_product(self, supplier_pid: str, product_data: Dict[str, Any]) -> None:
        """Append a new product to the master JSON.

        Args:
            supplier_pid: The product ID.
            product_data: The scraped product data.
        """
        enriched_data = product_data.copy()
        enriched_data["scraped_at"] = datetime.now().isoformat()
        self.data["products"][supplier_pid] = enriched_data
        self.data["metadata"]["total_products"] = len(self.data["products"])
        self.data["metadata"]["last_updated"] = datetime.now().isoformat()
        print(f"‚úì Appended {supplier_pid} to master JSON")

    def update_product(self, supplier_pid: str, product_data: Dict[str, Any]) -> None:
        """Update an existing product in the master JSON.

        Args:
            supplier_pid: The product ID.
            product_data: The new scraped product data.
        """
        enriched_data = product_data.copy()
        enriched_data["scraped_at"] = datetime.now().isoformat()
        enriched_data["updated_at"] = datetime.now().isoformat()
        self.data["products"][supplier_pid] = enriched_data
        self.data["metadata"]["last_updated"] = datetime.now().isoformat()
        print(f"‚úì Updated {supplier_pid} in master JSON")

    def save(self) -> None:
        """Save the master JSON with backup rotation."""
        self._rotate_backups()
        try:
            with open(self.master_path, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            print(f"‚úì Saved master JSON to: {self.master_path}")
        except IOError as e:
            print(f"‚ùå Error saving master JSON: {e}")

    def _rotate_backups(self) -> None:
        """Rotate backup files, keeping only the specified number of backups."""
        if not os.path.exists(self.master_path):
            return

        # Shift existing backups
        for i in range(self.backup_count - 1, 0, -1):
            old_backup = f"{self.master_path}.backup{i}"
            new_backup = f"{self.master_path}.backup{i+1}"
            if os.path.exists(old_backup):
                if i == self.backup_count - 1:
                    # Remove oldest backup if at limit
                    if os.path.exists(new_backup):
                        os.remove(new_backup)
                shutil.move(old_backup, new_backup)

        # Create backup of current master
        backup_path = f"{self.master_path}.backup1"
        shutil.copy2(self.master_path, backup_path)

    def get_statistics(self) -> Dict[str, Any]:
        """Get statistics about the master JSON.

        Returns:
            Dictionary with statistics.
        """
        return {
            "total_products": len(self.data.get("products", {})),
            "created_at": self.data.get("metadata", {}).get("created_at", "N/A"),
            "last_updated": self.data.get("metadata", {}).get("last_updated", "N/A"),
        }

------------------------------------------------- ./BMEcat_transformer/core/xml_readers/dabag_xml_reader.py --------------------------------------------------

from __future__ import annotations

"""DABAG BMEcat XML reader with multi-language feature extraction.

Extracts per-product, per-language features from a DABAG-style BMEcat XML.
Supported languages: deu, fra, ita mapped to de, fr, it.

Returns data grouped by `SUPPLIER_PID` and language.
"""

from typing import Dict, List, Any, DefaultDict
from collections import defaultdict
import xml.etree.ElementTree as ET
from utils.logger import setup_logger


LANG_MAP = {
    "deu": "de",
    "fra": "fr",
    "ita": "it",
    # also accept short codes if encountered
    "de": "de",
    "fr": "fr",
    "it": "it",
}


class DABAGXMLReader:
    """Read DABAG XML and extract per-product, per-language features."""

    def __init__(self, xml_path: str) -> None:
        self.xml_path = xml_path
        self.logger = setup_logger(__name__)

    def extract_features(self) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        """Extract features per product and language keyed by `SUPPLIER_PID`.

        Parsing strategy (in order):
        1) lxml with recovery (if available)
        2) Standard xml.etree parsing

        Returns:
            Mapping: {supplier_id: {lang: [{"fname": str, "fvalue": str, "funit": str|None}, ...]}}
        """
        self.logger.info(f"Starting DABAG feature extraction from: {self.xml_path}")
        
        try:
            from lxml import etree as LXML_ET  # type: ignore
            self.logger.debug("Attempting extraction with lxml")
            result = self._extract_via_lxml()
            self.logger.info(f"DABAG lxml extraction: found {len(result)} products")
            return result
        except Exception as e:
            self.logger.warning(f"DABAG lxml extraction failed: {e}")
            self.logger.debug("Falling back to xml.etree.ElementTree")
            result = self._extract_via_xml_parsing()
            self.logger.info(f"DABAG ElementTree extraction: found {len(result)} products")
            return result

    def _extract_via_xml_parsing(self) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        results: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}

        try:
            self.logger.debug("Reading DABAG XML file")
            with open(self.xml_path, "rb") as f:
                raw = f.read()
            self.logger.debug(f"DABAG XML file size: {len(raw)} bytes")
            if raw.startswith(b"\xef\xbb\xbf"):
                raw = raw[3:]
            text = raw.decode("utf-8", errors="replace").lstrip()
            root = ET.fromstring(text)
        except Exception as e:
            self.logger.error(f"DABAG XML parsing failed: {e}")
            return results

        if root.tag.startswith("{"):
            ns = root.tag.split("}")[0] + "}"
        else:
            ns = ""
        self.logger.debug(f"DABAG root tag: {root.tag}, namespace: {ns if ns else 'none'}")

        product_nodes_list = root.findall(f".//{ns}PRODUCT") + root.findall(".//PRODUCT")
        self.logger.debug(f"DABAG found {len(product_nodes_list)} PRODUCT nodes")
        for prod in product_nodes_list:
            pid = self._get_first_text(prod, [f"{ns}MANUFACTURER_PID", "MANUFACTURER_PID"]) or ""
            if not pid:
                continue

            lang_map: DefaultDict[str, List[Dict[str, Any]]] = defaultdict(list)

            for feat in prod.findall(f".//{ns}FEATURE") + prod.findall(".//FEATURE"):
                funit = self._get_first_text(feat, [f"{ns}FUNIT", "FUNIT"]) or None

                # FNAME and FVALUE may appear multiple times with lang attribute
                for fname_node in feat.findall(f".//{ns}FNAME") + feat.findall(".//FNAME"):
                    lang_attr = fname_node.attrib.get("lang") or fname_node.attrib.get("xml:lang")
                    lang = LANG_MAP.get((lang_attr or "").strip().lower())
                    if not lang:
                        # skip unsupported or missing language entries gracefully
                        continue
                    fname = (fname_node.text or "").strip()
                    # find corresponding FVALUE with same lang, if any
                    fvalue = ""
                    for fvalue_node in feat.findall(f".//{ns}FVALUE") + feat.findall(".//FVALUE"):
                        v_lang_attr = fvalue_node.attrib.get("lang") or fvalue_node.attrib.get("xml:lang")
                        v_lang = LANG_MAP.get((v_lang_attr or "").strip().lower())
                        if v_lang == lang:
                            fvalue = (fvalue_node.text or "").strip()
                            break

                    if fname or fvalue or funit:
                        lang_map[lang].append({
                            "fname": fname,
                            "fvalue": fvalue,
                            "funit": funit.strip() if isinstance(funit, str) else funit,
                        })

            if lang_map:
                results[pid] = dict(lang_map)

        return results

    def _extract_via_lxml(self) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        from lxml import etree as LXML_ET  # type: ignore

        results: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}

        with open(self.xml_path, "rb") as f:
            raw = f.read()
        parser = LXML_ET.XMLParser(recover=True, encoding="utf-8")
        root = LXML_ET.fromstring(raw, parser)

        product_nodes = root.xpath('.//PRODUCT | .//*[local-name()="PRODUCT"]')
        self.logger.debug(f"DABAG lxml found {len(product_nodes)} PRODUCT nodes")
        for prod in product_nodes:
            pid_nodes = prod.xpath('.//MANUFACTURER_PID | .//*[local-name()="MANUFACTURER_PID"]')
            pid = pid_nodes[0].text.strip() if pid_nodes and pid_nodes[0].text else ""
            if not pid:
                continue

            lang_map: DefaultDict[str, List[Dict[str, Any]]] = defaultdict(list)

            feat_nodes = prod.xpath('.//FEATURE | .//*[local-name()="FEATURE"]')
            for feat in feat_nodes:
                funit_nodes = feat.xpath('.//FUNIT | .//*[local-name()="FUNIT"]')
                funit = (funit_nodes[0].text or "").strip() if funit_nodes and funit_nodes[0].text else None

                fname_nodes = feat.xpath('.//FNAME | .//*[local-name()="FNAME"]')
                for fname_node in fname_nodes:
                    lang_attr = fname_node.attrib.get("lang") or fname_node.attrib.get("xml:lang")
                    lang = LANG_MAP.get((lang_attr or "").strip().lower())
                    if not lang:
                        continue
                    fname = (fname_node.text or "").strip()

                    fvalue = ""
                    fvalue_nodes = feat.xpath('.//FVALUE | .//*[local-name()="FVALUE"]')
                    for fvalue_node in fvalue_nodes:
                        v_lang_attr = fvalue_node.attrib.get("lang") or fvalue_node.attrib.get("xml:lang")
                        v_lang = LANG_MAP.get((v_lang_attr or "").strip().lower())
                        if v_lang == lang:
                            fvalue = (fvalue_node.text or "").strip()
                            break

                    if fname or fvalue or funit:
                        lang_map[lang].append({
                            "fname": fname,
                            "fvalue": fvalue,
                            "funit": funit,
                        })

            if lang_map:
                results[pid] = dict(lang_map)

        return results

    def _get_first_text(self, node: Any, tags: List[str]) -> str | None:
        for tag in tags:
            elem = node.find(f".//{tag}")
            if elem is not None and elem.text:
                return elem.text
        return None

------------------------------------------------- ./BMEcat_transformer/core/xml_readers/original_xml_reader.py --------------------------------------------------

from __future__ import annotations

"""Original BMEcat XML reader.

Extracts per-product features from an Original BMEcat XML, using `FDESCR` as
feature names and normalizing numeric values in German format.

Returns data grouped by `SUPPLIER_PID`.
"""

from typing import Dict, List, Any
import re
from utils.logger import setup_logger


class OriginalXMLReader:
    """Read Original BMEcat XML and extract per-product features.

    Uses regex-based extraction to handle malformed XML that lxml/ElementTree can't parse.
    Product identification: checks SUPPLIER_AID first, falls back to SUPPLIER_PID.
    
    Attributes:
        xml_path: Path to the BMEcat XML file.
    """

    def __init__(self, xml_path: str) -> None:
        """Initialize with the XML file path.

        Args:
            xml_path: Path to the BMEcat XML file.
        """
        self.xml_path = xml_path
        self.logger = setup_logger(__name__)

    def extract_features(self) -> Dict[str, List[Dict[str, Any]]]:
        """Extract features per product keyed by SUPPLIER_AID or SUPPLIER_PID.

        Uses regex parsing to handle malformed XML.
        Priority: SUPPLIER_AID checked first, SUPPLIER_PID as fallback.
        
        IMPORTANT: ALL products with valid PIDs are loaded, even if they have
        no structured FEATURE data. Products without features get empty lists.

        Returns:
            Mapping: {supplier_id: [{"fname": str, "fvalue": str, "funit": str|None}, ...]}
        """
        self.logger.info(f"Starting feature extraction from: {self.xml_path}")
        
        results: Dict[str, List[Dict[str, Any]]] = {}
        
        try:
            with open(self.xml_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            
            self.logger.debug(f"File size: {len(content)} characters")
            
            # Find all PRODUCT blocks using regex
            product_pattern = r'<PRODUCT[^>]*>(.*?)</PRODUCT>'
            products = re.findall(product_pattern, content, re.DOTALL)
            
            self.logger.info(f"Found {len(products)} PRODUCT blocks")
            
            for product_xml in products:
                # Extract SUPPLIER_AID first (primary), fallback to SUPPLIER_PID
                aid_match = re.search(r'<SUPPLIER_AID>\s*([^<]+?)\s*</SUPPLIER_AID>', product_xml)
                pid_match = re.search(r'<SUPPLIER_PID>\s*([^<]+?)\s*</SUPPLIER_PID>', product_xml)

                # Priority: AID first, then PID
                if aid_match:
                    supplier_id = aid_match.group(1).strip()
                elif pid_match:
                    supplier_id = pid_match.group(1).strip()
                else:
                    self.logger.debug("Skipping product without SUPPLIER_AID or SUPPLIER_PID")
                    continue
                
                self.logger.debug(f"Processing product: {supplier_id}")
                
                # Extract all FEATURE blocks
                feature_pattern = r'<FEATURE>(.*?)</FEATURE>'
                features_xml = re.findall(feature_pattern, product_xml, re.DOTALL)
                
                features: List[Dict[str, Any]] = []
                
                for feature_xml in features_xml:
                    # Extract FDESCR (feature name)
                    fname_match = re.search(r'<FDESCR>\s*([^<]+?)\s*</FDESCR>', feature_xml)
                    fname = fname_match.group(1).strip() if fname_match else ""
                    
                    # Extract FVALUE (feature value)
                    fvalue_match = re.search(r'<FVALUE>\s*([^<]+?)\s*</FVALUE>', feature_xml)
                    fvalue = fvalue_match.group(1).strip() if fvalue_match else ""
                    
                    # Extract FUNIT (feature unit)
                    funit_match = re.search(r'<FUNIT>\s*([^<]+?)\s*</FUNIT>', feature_xml)
                    funit = funit_match.group(1).strip() if funit_match else None
                    
                    # Normalize German number format
                    if fvalue:
                        fvalue = self._normalize_number(fvalue)
                    
                    # Only add if we have at least a name or value
                    if fname or fvalue or funit:
                        features.append({
                            "fname": fname,
                            "fvalue": fvalue,
                            "funit": funit,
                        })
                
                # CRITICAL FIX: Add ALL products with valid PIDs, even if no features
                # This ensures the count matches PRODUCT blocks and DABAG XML
                results[supplier_id] = features
                
                if features:
                    self.logger.debug(f"Product {supplier_id}: extracted {len(features)} features")
                else:
                    self.logger.debug(f"Product {supplier_id}: no structured FEATURE data (will be empty)")
            
            self.logger.info(f"Successfully extracted features for {len(results)} products")
            
        except Exception as e:
            self.logger.error(f"Feature extraction failed: {e}")
        
        return results

    def _normalize_number(self, value: str) -> str:
        """Normalize German-formatted numbers to dot-decimal.

        Examples:
            "2,5" -> "2.5"
            "1.234,56" -> "1234.56"

        If the value is not numeric-like, returns original string.
        """
        s = value.strip()
        # Potential patterns like 1.234,56 or 2,5 or 1234
        # Heuristic: if there's a comma and (digits or dot+digits) before it, treat comma as decimal
        if re.search(r"\d,\d", s):
            s_no_thousands = s.replace(".", "")
            s_norm = s_no_thousands.replace(",", ".")
            return s_norm
        # If only thousands separators (e.g., 1.234), keep as is but also offer a version without dots if it parses
        if re.fullmatch(r"\d{1,3}(\.\d{3})+", s):
            return s.replace(".", "")
        return s
------------------------------------------------- ./BMEcat_transformer/core/xml_readers/original_supplier_id_extractor.py --------------------------------------------------

"""XML Reader for BMEcat_transformer.

Provides `OriginalSupplierIDExtractor` that extracts unique `SUPPLIER_PID`s from a BMEcat XML file.
Uses multiple fallback strategies to handle malformed XML.
"""

from __future__ import annotations

from typing import List, Set
import sys
import re
from pathlib import Path
import xml.etree.ElementTree as ET


# Ensure we can import the BMEcatParser from the sibling module `doc_processor`
DOC_PROCESSOR_PATH = Path(__file__).parent.parent.parent / "doc_processor"
sys.path.append(str(DOC_PROCESSOR_PATH))
try:
    from src.bmecat_parser import BMEcatParser  # type: ignore
except Exception as e:  # pragma: no cover - import guard
    BMEcatParser = None  # Fallback if import fails; we'll handle at runtime


class OriginalSupplierIDExtractor:
    """Read BMEcat XML and extract product IDs.

    Attributes:
        xml_path: Path to the BMEcat XML file.
    """

    def __init__(self, xml_path: str) -> None:
        """Initialize with the XML file path.

        Args:
            xml_path: Path to the BMEcat XML file.
        """
        self.xml_path = xml_path

    def extract_SUPPLIER_PIDs(self) -> List[str]:
        """Extract a list of unique SUPPLIER_PID/SUPPLIER_AID strings from the XML file.

        Uses multiple strategies, prioritizing regex for malformed XML:
        1. Regex extraction for SUPPLIER_AID (primary)
        2. Regex extraction for SUPPLIER_PID (fallback)
        3. Try with lxml (if available)
        4. Try standard XML parsing

        Priority: SUPPLIER_AID is checked first, then SUPPLIER_PID.
        Both are treated as equivalent product identifiers.

        Returns:
            List of unique product IDs (order preserved by first appearance).
        """
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        # Strategy 1: Regex-based extraction (most reliable for malformed XML)
        try:
            print("üîç Using regex-based extraction (most reliable)...")
            SUPPLIER_PIDs = self._extract_via_regex()
            if SUPPLIER_PIDs:
                print(f"‚úÖ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via regex")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except Exception as e:
            print(f"‚ö†Ô∏è  Regex extraction failed: {e}")

        # Strategy 2: Try lxml with recovery mode (more lenient)
        try:
            print("üîç Attempting lxml parsing with recovery mode...")
            from lxml import etree as LXML_ET
            SUPPLIER_PIDs = self._extract_via_lxml()
            if SUPPLIER_PIDs:
                print(f"‚úÖ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via lxml recovery")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except ImportError:
            print("‚ö†Ô∏è  lxml not available, skipping...")
        except Exception as e:
            print(f"‚ö†Ô∏è  lxml parsing failed: {e}")

        # Strategy 3: Try standard XML parsing
        try:
            print("üîç Attempting standard XML parsing...")
            SUPPLIER_PIDs = self._extract_via_xml_parsing()
            if SUPPLIER_PIDs:
                print(f"‚úÖ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via XML parsing")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except Exception as e:
            print(f"‚ö†Ô∏è  XML parsing failed: {e}")

        if not SUPPLIER_PIDs:
            print("‚ö†Ô∏è  Warning: No SUPPLIER_PID elements found in the provided XML.")

        return SUPPLIER_PIDs

    def _extract_via_xml_parsing(self) -> List[str]:
        """Extract SUPPLIER_PIDs using standard XML parsing."""
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'rb') as f:
            raw = f.read()

        # Remove BOM and strip leading whitespace
        if raw.startswith(b'\xef\xbb\xbf'):
            raw = raw[3:]
        text = raw.decode('utf-8', errors='replace').lstrip()
        
        root = ET.fromstring(text)

        # Determine namespace
        if root.tag.startswith('{'):
            ns = root.tag.split('}')[0] + '}'
        else:
            ns = ''

        # Search for SUPPLIER_PID elements
        for elem in root.findall(f'.//{ns}SUPPLIER_PID'):
            if elem is not None and elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        # Also try without namespace
        for elem in root.findall('.//SUPPLIER_PID'):
            if elem is not None and elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        return SUPPLIER_PIDs

    def _extract_via_lxml(self) -> List[str]:
        """Extract SUPPLIER_PIDs using lxml with recovery mode."""
        from lxml import etree as LXML_ET
        
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'rb') as f:
            raw = f.read()

        # Create parser with recovery mode (tolerates malformed XML)
        parser = LXML_ET.XMLParser(recover=True, encoding='utf-8')
        root = LXML_ET.fromstring(raw, parser)

        # Find all SUPPLIER_PID elements
        for elem in root.xpath('.//SUPPLIER_PID | .//*[local-name()="SUPPLIER_PID"]'):
            if elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        return SUPPLIER_PIDs

    def _extract_via_regex(self) -> List[str]:
        """Extract SUPPLIER_PIDs using regex pattern matching.
        
        This method is resilient to XML structure errors and will work
        even with malformed XML, as long as the tags exist.
        """
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()

        # Patterns to match SUPPLIER_AID (primary) and SUPPLIER_PID (fallback)
        # Works with or without namespace, handles whitespace
        pattern_aid = r'<(?:\w+:)?SUPPLIER_AID[^>]*>\s*([^<]+?)\s*</(?:\w+:)?SUPPLIER_AID>'
        pattern_pid = r'<(?:\w+:)?SUPPLIER_PID[^>]*>\s*([^<]+?)\s*</(?:\w+:)?SUPPLIER_PID>'

        # Try SUPPLIER_AID first (primary)
        matches_aid = re.findall(pattern_aid, content, re.DOTALL | re.IGNORECASE)

        for match in matches_aid:
            val = match.strip()
            # Filter out empty or whitespace-only values
            if val and not val.isspace() and val not in seen:
                seen.add(val)
                SUPPLIER_PIDs.append(val)

        # Fallback to SUPPLIER_PID if no AIDs found or to catch additional PIDs
        matches_pid = re.findall(pattern_pid, content, re.DOTALL | re.IGNORECASE)

        for match in matches_pid:
            val = match.strip()
            # Filter out empty or whitespace-only values
            if val and not val.isspace() and val not in seen:
                seen.add(val)
                SUPPLIER_PIDs.append(val)

        # Optional: Print extraction summary (for debugging)
        # Uncomment if needed for troubleshooting
        # print(f"Extracted {len(SUPPLIER_PIDs)} unique IDs (AID+PID combined)")
        return SUPPLIER_PIDs
------------------------------------------------- ./BMEcat_transformer/core/xml_readers/__init__.py --------------------------------------------------

from __future__ import annotations

"""XML readers package for BMEcat_transformer.

Exports specialized XML readers for different BMEcat sources.
"""

from .original_supplier_id_extractor import OriginalSupplierIDExtractor
from .original_xml_reader import OriginalXMLReader
from .dabag_xml_reader import DABAGXMLReader

__all__ = [
    "OriginalSupplierIDExtractor",
    "OriginalXMLReader",
    "DABAGXMLReader",
]

------------------------------------------------- ./BMEcat_transformer/core/comparison_table_builder.py --------------------------------------------------

from __future__ import annotations

"""Comparison table builder orchestrator.

Coordinates data from Original XML, DABAG XML, and web scraped master JSON.
Optionally triggers scraping for missing supplier IDs.
"""

from typing import Dict, List, Any, Tuple, Set
import sys
from pathlib import Path

# Local imports
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

from core.xml_readers import OriginalXMLReader, DABAGXMLReader  # type: ignore
from core.master_json_manager import MasterJSONManager  # type: ignore
import config  # type: ignore

# Scraper import
SCRAPERS_DIR = PROJECT_ROOT / "scrapers"
if str(SCRAPERS_DIR.parent) not in sys.path:
    sys.path.append(str(SCRAPERS_DIR.parent))
from scrapers.dabag_scraper import DABAGScraper  # type: ignore
from utils.logger import setup_logger
from processors.xml_specs_extractor import XMLSpecsExtractor  # type: ignore
from processors.ai_feature_matcher import AIFeatureMatcher  # type: ignore


class ComparisonTableBuilder:
    """Build merged comparison data across sources for each supplier/product."""

    def __init__(
        self,
        original_xml_path: str,
        dabag_xml_path: str,
        master_json_path: str | None = None,
    ) -> None:
        self.logger = setup_logger(__name__)
        self.original_xml_path = original_xml_path
        self.dabag_xml_path = dabag_xml_path
        # Master JSON manager uses OUTPUT_DIR and filename from config
        self.master_manager = MasterJSONManager(
            master_filename=config.MASTER_JSON_FILENAME,
            output_dir=config.OUTPUT_DIR,
            backup_count=config.MASTER_JSON_BACKUP_COUNT,
        )
        # Load or initialize master JSON
        self.master_manager.load()

        self.original_reader = OriginalXMLReader(original_xml_path)
        self.dabag_reader = DABAGXMLReader(dabag_xml_path)
        self.scraper = DABAGScraper()

    def _read_scraped_text(self, supplier_pid: str) -> Dict[str, str]:
        """Read scraped text file for a product if it exists.
        
        Args:
            supplier_pid: Product identifier.
            
        Returns:
            Dict with field names and text, empty dict if file not found.
        """
        from pathlib import Path
        
        text_file = Path(config.SCRAPED_TEXT_DIR) / f"{supplier_pid}.txt"
        
        if not text_file.exists():
            self.logger.debug(f"No scraped text file for {supplier_pid}")
            return {}
        
        try:
            with open(text_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse sections back into dict
            result = {}
            current_section = None
            current_text = []
            
            for line in content.split('\n'):
                if line.startswith('=== ') and line.endswith(' ==='):
                    # Save previous section
                    if current_section:
                        result[current_section.lower().replace(' ', '_')] = '\n'.join(current_text).strip()
                    # Start new section
                    current_section = line.strip('= ').strip()
                    current_text = []
                elif current_section and line.strip():
                    current_text.append(line)
            
            # Save last section
            if current_section:
                result[current_section.lower().replace(' ', '_')] = '\n'.join(current_text).strip()
            
            self.logger.debug(f"Read {len(result)} sections from {text_file}")
            return result
            
        except Exception as e:
            self.logger.warning(f"Failed to read scraped text for {supplier_pid}: {e}")
            return {}

    def build_comparison_tables(self, auto_scrape: bool = False) -> Dict[str, Dict[str, Any]]:
        """Build merged comparison data per supplier ID and language.

        Returns:
            Mapping per supplier: {supplier_id: {
                "product_url": str | None,
                "languages": {
                    lang: {
                        "rows": [
                            {
                                "original_fname": str, "original_fvalue": str, "original_funit": str | None,
                                "dabag_fname": str, "dabag_fvalue": str, "dabag_funit": str | None,
                                "web_fname": str, "web_fvalue": str,
                                "ai_fname": str, "ai_fvalue": str,
                            }, ...
                        ]
                    }
                }
            }}
        """
        self.logger.info("="*60)
        self.logger.info("Starting comparison table generation")
        self.logger.info(f"Auto-scrape enabled: {auto_scrape}")

        # Load data from sources
        original_features = self.original_reader.extract_features()  # {pid: [ {fname,fvalue,funit} ]}
        self.logger.info(f"Original XML: loaded {len(original_features)} products")
        dabag_features = self.dabag_reader.extract_features()        # {pid: {lang: [ {fname,fvalue,funit} ]} }
        self.logger.info(f"DABAG XML: loaded {len(dabag_features)} products")

        # NEW: Extract XML specs from UDX fields and match via AI (optional)
        ai_specs_by_pid: Dict[str, List[Dict[str, Any]]] = {}
        if getattr(config, "GROK_API_KEY", None):
            try:
                self.logger.info("Extracting UDX XML specifications and matching with AI...")
                extractor = XMLSpecsExtractor(self.original_xml_path)
                if extractor.load_xml():
                    udx_map = getattr(config, "UDX_FIELD_MAPPING", {})
                    all_udx = extractor.extract_all_products(udx_map)  # {pid: {field: text}}

                    prompt_path = PROJECT_ROOT / "prompts" / "xml_specs_mapping.yaml"
                    csv_path = Path(config.OUTPUT_DIR) / "unique_features.csv"
                    ai_matcher = AIFeatureMatcher(
                        api_key=config.GROK_API_KEY,
                        model=getattr(config, "GROK_MODEL", "grok-4-fast-reasoning"),
                        base_url=getattr(config, "GROK_BASE_URL", "https://api.x.ai/v1"),
                        confidence_threshold=float(getattr(config, "GROK_CONFIDENCE_THRESHOLD", 0.70)),
                        prompt_path=str(prompt_path),
                        csv_path=str(csv_path),
                        ai_features_path=getattr(config, "AI_FEATURES_PATH", str(Path(config.OUTPUT_DIR) / "ai_generated_features.json")),
                    )

                    if not csv_path.exists():
                        self.logger.warning(f"unique_features.csv not found at {csv_path} ‚Äî AI matching may be limited")

                    if ai_matcher.load_references() and ai_matcher.load_prompt():
                        for pid in all_udx.keys():
                            # Try to read from saved text file first
                            raw_text = self._read_scraped_text(pid)
                            
                            # Fallback to memory if file doesn't exist
                            if not raw_text:
                                raw_text = all_udx.get(pid, {})
                            
                            if not raw_text:
                                continue
                            
                            feats = ai_matcher.match_features(raw_text, pid)
                            if feats:
                                ai_specs_by_pid[pid] = feats
                        self.logger.info(f"AI-matched XML specs for {len(ai_specs_by_pid)} products")
                    else:
                        self.logger.warning("AI matcher references or prompt failed to load; skipping AI matching")
                else:
                    self.logger.warning("Could not load Original XML for UDX extraction; skipping AI matching")
            except Exception as e:
                self.logger.error(f"XML specs extraction/matching failed: {e}")
        else:
            self.logger.info("Grok API key not configured ‚Äî skipping XML specs extraction")

        # Prepare web data from master JSON
        web_products = self.master_manager.data.get("products", {})
        self.logger.info(f"Master JSON: loaded {len(web_products)} products")
        web_ids: Set[str] = set(web_products.keys())

        # Determine missing IDs to potentially scrape
        original_ids: List[str] = list(original_features.keys())
        missing_ids = self._check_missing_ids(original_ids, list(web_ids))
        self.logger.info(f"Missing IDs to scrape: {len(missing_ids)}")

        if auto_scrape and missing_ids:
            for pid in missing_ids:
                scraped = self.scraper.process_product(pid)
                if scraped:
                    exists, _ = self.master_manager.check_id_exists(pid)
                    if exists:
                        self.master_manager.update_product(pid, scraped)
                    else:
                        self.master_manager.append_product(pid, scraped)
            # Persist updates
            self.master_manager.save()
            web_products = self.master_manager.data.get("products", {})

        # Merge per supplier and per language
        merged: Dict[str, Dict[str, Any]] = {}
        for pid, orig_list in original_features.items():
            pid_dabag = dabag_features.get(pid, {})  # {lang: [..]}
            web_entry = web_products.get(pid, {})     # {product_url, languages: {lang: {label: value}}}
            product_url = web_entry.get("product_url")
            web_langs: Dict[str, Dict[str, str]] = web_entry.get("languages", {})

            # Build per-language rows for de, fr, it
            langs = ["de", "fr", "it"]
            lang_data: Dict[str, Any] = {}
            for lang in langs:
                dabag_list = pid_dabag.get(lang, [])
                web_specs = web_langs.get(lang, {})  # dict label->value
                ai_list = ai_specs_by_pid.get(pid, [])
                rows = self._align_features(orig_list, dabag_list, web_specs, ai_list)
                lang_data[lang] = {"rows": rows}

            merged[pid] = {
                "product_url": product_url,
                "languages": lang_data,
            }

        self.logger.info(f"Comparison tables built for {len(merged)} products")
        self.logger.info("="*60)
        return merged

    def _check_missing_ids(self, original_ids: List[str], web_ids: List[str]) -> List[str]:
        """Return IDs present in original XML but missing in web master JSON."""
        web_set = set(web_ids)
        return [pid for pid in original_ids if pid not in web_set]

    def _align_features(
        self,
        original_list: List[Dict[str, Any]],
        dabag_list: List[Dict[str, Any]],
        web_specs: Dict[str, str],
        ai_list: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Align features across sources by simple exact-name matching.

        Strategy:
        - Use union of all feature names across sources as the index set.
        - For web specs (dict label->value), use labels as names.
        - Construct rows with 8 columns (plus funit for original/dabag), AI left blank.
        """
        # Collect all feature names
        names: Set[str] = set()
        names.update([f.get("fname", "") for f in original_list if f.get("fname")])
        names.update([f.get("fname", "") for f in dabag_list if f.get("fname")])
        names.update([k for k in web_specs.keys() if k])
        names.update([f.get("fname", "") for f in ai_list if f.get("fname")])

        # Build quick lookups
        orig_map = {f.get("fname", ""): f for f in original_list if f.get("fname")}
        dabag_map = {f.get("fname", ""): f for f in dabag_list if f.get("fname")}
        ai_map = {f.get("fname", ""): f for f in ai_list if f.get("fname")}

        rows: List[Dict[str, Any]] = []
        for name in sorted(names):
            o = orig_map.get(name, {})
            d = dabag_map.get(name, {})
            w_val = web_specs.get(name, "")
            a = ai_map.get(name, {})
            rows.append({
                "original_fname": name if o else "",
                "original_fvalue": o.get("fvalue", ""),
                "original_funit": o.get("funit"),
                "dabag_fname": name if d else "",
                "dabag_fvalue": d.get("fvalue", ""),
                "dabag_funit": d.get("funit"),
                "web_fname": name if w_val else "",
                "web_fvalue": w_val,
                "ai_fname": name if a else "",
                "ai_fvalue": a.get("fvalue", "") if a else "",
            })
        return rows

------------------------------------------------- ./BMEcat_transformer/core/input_handler.py --------------------------------------------------

"""Input handler for BMEcat_transformer.

Provides `InputHandler` that auto-detects and loads SUPPLIER_PIDs from
supported input files (XML or JSON).
"""

from __future__ import annotations

from pathlib import Path
from typing import List
import json

from core.xml_readers.original_supplier_id_extractor import OriginalSupplierIDExtractor


class InputHandler:
    """Handle loading SUPPLIER_PIDs from XML or JSON input files.

    This is a stateless utility class that provides a single entry point to
    load product identifiers for downstream processing.
    """

    @staticmethod
    def load_supplier_ids(file_path: str) -> List[str]:
        """Auto-detect file type and return list of SUPPLIER_PIDs.

        Args:
            file_path: Path to input file (.xml or .json).

        Returns:
            List of unique SUPPLIER_PID strings.

        Raises:
            ValueError: If the file does not exist, has an unsupported type,
                or contains invalid JSON/invalid content.
        """
        path = Path(file_path)
        if not path.exists():
            raise ValueError(f"Input file does not exist: {file_path}")

        suffix = path.suffix.lower()
        if suffix == ".xml":
            print("üîç Detected XML file, extracting SUPPLIER_PIDs...")
            reader = OriginalSupplierIDExtractor(str(path))
            ids = reader.extract_SUPPLIER_PIDs()
            # Ensure uniqueness and cleanliness
            seen = set()
            cleaned: List[str] = []
            for val in ids:
                v = val.strip()
                if v and v not in seen:
                    seen.add(v)
                    cleaned.append(v)
            print(f"‚úÖ Successfully loaded {len(cleaned)} SUPPLIER_PID(s) from XML")
            return cleaned
        elif suffix == ".json":
            print("üîç Detected JSON file, loading SUPPLIER_PIDs...")
            ids = InputHandler._load_from_json(str(path))
            print(f"‚úÖ Successfully loaded {len(ids)} SUPPLIER_PID(s) from JSON")
            return ids
        else:
            raise ValueError(
                f"Unsupported input format '{suffix}'. Only .xml and .json are supported."
            )

    @staticmethod
    def _load_from_json(file_path: str) -> List[str]:
        """Load SUPPLIER_PIDs from a JSON array file.

        The JSON file must contain a simple array of strings, e.g.:
        ["ID1", "ID2", "ID3"]

        Args:
            file_path: Path to the JSON file.

        Returns:
            List of unique SUPPLIER_PID strings.

        Raises:
            ValueError: If JSON is invalid or content fails validation.
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  Invalid JSON: {e}")
            raise ValueError(f"Invalid JSON in file: {file_path}") from e

        # Validate structure: must be a list
        if not isinstance(data, list):
            print("‚ö†Ô∏è  JSON must be an array of strings, e.g. [\"ID1\", \"ID2\"]")
            raise ValueError("JSON must be an array of strings")

        # Validate elements are strings
        if not all(isinstance(x, str) for x in data):
            print("‚ö†Ô∏è  All elements in the JSON array must be strings")
            raise ValueError("All elements in the JSON array must be strings")

        # Clean, strip, and filter out empty strings; enforce uniqueness preserving order
        seen = set()
        cleaned: List[str] = []
        for x in data:
            v = x.strip()
            if v and v not in seen:
                seen.add(v)
                cleaned.append(v)

        if not cleaned:
            print("‚ö†Ô∏è  JSON array is empty or contains only empty/whitespace strings")
            raise ValueError("JSON must contain at least one non-empty string")

        return cleaned

------------------------------------------------- ./BMEcat_transformer/test_debug.py --------------------------------------------------

from __future__ import annotations
import logging
import sys

# Set DEBUG level for ALL loggers
logging.basicConfig(
    level=logging.DEBUG,
    format='[%(levelname)s] %(name)s: %(message)s',
    stream=sys.stdout
)

from core.xml_readers import OriginalXMLReader

xml_path = "/Users/dannycrescimone/Documents/data_scraping/DEWALT_BMEcat_Original.xml"

reader = OriginalXMLReader(xml_path)
reader.logger.setLevel(logging.DEBUG)

result = reader.extract_features()

print("\n" + "="*70)
print(f"RESULT: {len(result)} products found")
if result:
    first_id = list(result.keys())[0]
    print(f"First product ID: {first_id}")
    print(f"First product has {len(result[first_id])} features")
    print(f"First feature: {result[first_id][0]}")
else:
    print("NO PRODUCTS FOUND")
    
    # Manual check with lxml
    print("\nMANUAL LXML CHECK:")
    from lxml import etree as LXML_ET
    
    with open(xml_path, 'rb') as f:
        raw = f.read()
    
    parser = LXML_ET.XMLParser(recover=True, encoding='utf-8')
    root = LXML_ET.fromstring(raw, parser)
    
    print(f"Root tag: {root.tag}")
    print(f"Root has {len(root)} direct children")
    
    # Try different xpath queries
    products_1 = root.xpath('.//PRODUCT')
    products_2 = root.xpath('.//*[local-name()="PRODUCT"]')
    products_3 = root.xpath('//PRODUCT')
    
    print(f"\nXPath './/PRODUCT': {len(products_1)} nodes")
    print(f"XPath './/*[local-name()=\"PRODUCT\"]': {len(products_2)} nodes")
    print(f"XPath '//PRODUCT': {len(products_3)} nodes")
    
    if products_2:
        prod = products_2[0]
        print(f"\nFirst PRODUCT tag: {prod.tag}")
        print(f"First PRODUCT attributes: {prod.attrib}")
        
        # Check for SUPPLIER_PID
        pids_1 = prod.xpath('.//SUPPLIER_PID')
        pids_2 = prod.xpath('.//*[local-name()="SUPPLIER_PID"]')
        
        print(f"SUPPLIER_PID with './/': {len(pids_1)}")
        print(f"SUPPLIER_PID with local-name: {len(pids_2)}")
        
        if pids_1:
            print(f"SUPPLIER_PID value: {pids_1[0].text}")
        if pids_2:
            print(f"SUPPLIER_PID value (local-name): {pids_2[0].text}")

print("="*70)

------------------------------------------------- ./BMEcat_transformer/test_imports.py --------------------------------------------------

#!/usr/bin/env python3
"""Test all imports after restructuring."""

import sys
from pathlib import Path

# Add current directory to path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

def test_imports():
    """Test all module imports."""
    errors = []
    
    print("Testing imports...\n")
    
    # Test config
    try:
        import config
        print("‚úì config imported successfully")
    except Exception as e:
        errors.append(f"‚úó config: {e}")
        print(f"‚úó config: {e}")
    
    # Test core modules
    try:
        from core.input_handler import InputHandler
        print("‚úì core.input_handler imported successfully")
    except Exception as e:
        errors.append(f"‚úó core.input_handler: {e}")
        print(f"‚úó core.input_handler: {e}")
    
    try:
        from core.xml_readers.original_supplier_id_extractor import OriginalSupplierIDExtractor
        print("‚úì core.xml_readers.original_supplier_id_extractor imported successfully")
    except Exception as e:
        errors.append(f"‚úó core.xml_readers.original_supplier_id_extractor: {e}")
        print(f"‚úó core.xml_readers.original_supplier_id_extractor: {e}")
    
    try:
        from core.master_json_manager import MasterJSONManager
        print("‚úì core.master_json_manager imported successfully")
    except Exception as e:
        errors.append(f"‚úó core.master_json_manager: {e}")
        print(f"‚úó core.master_json_manager: {e}")
    
    # Test scraper modules
    try:
        from scrapers.table_extractor import TableExtractor
        print("‚úì scrapers.table_extractor imported successfully")
    except Exception as e:
        errors.append(f"‚úó scrapers.table_extractor: {e}")
        print(f"‚úó scrapers.table_extractor: {e}")
    
    try:
        from scrapers.dabag_scraper import DABAGScraper
        print("‚úì scrapers.dabag_scraper imported successfully")
    except Exception as e:
        errors.append(f"‚úó scrapers.dabag_scraper: {e}")
        print(f"‚úó scrapers.dabag_scraper: {e}")
    
    # Test processor modules
    try:
        from processors.feature_extractor import FeatureExtractor
        print("‚úì processors.feature_extractor imported successfully")
    except Exception as e:
        errors.append(f"‚úó processors.feature_extractor: {e}")
        print(f"‚úó processors.feature_extractor: {e}")
    
    # Test output modules
    try:
        from output.output_formatter import OutputFormatter
        print("‚úì output.output_formatter imported successfully")
    except Exception as e:
        errors.append(f"‚úó output.output_formatter: {e}")
        print(f"‚úó output.output_formatter: {e}")
    
    # Test UI modules
    try:
        from ui.user_prompt import UserPrompt
        print("‚úì ui.user_prompt imported successfully")
    except Exception as e:
        errors.append(f"‚úó ui.user_prompt: {e}")
        print(f"‚úó ui.user_prompt: {e}")
    
    # Summary
    print("\n" + "="*60)
    if errors:
        print(f"‚ùå FAILED: {len(errors)} import error(s)")
        for error in errors:
            print(f"  {error}")
        return False
    else:
        print("‚úÖ SUCCESS: All imports working correctly!")
        return True

if __name__ == "__main__":
    success = test_imports()
    sys.exit(0 if success else 1)

------------------------------------------------- ./BMEcat_transformer/output/output_formatter.py --------------------------------------------------

"""Output formatter for BMEcat_transformer.

- Pretty-prints per-product, per-language specification tables to terminal.
- Displays stats for each product (specs per language)
- Saves complete results to JSON in configured output directory.
- Displays a processing summary.
"""

from __future__ import annotations

from typing import Dict, Any, List
import os
import time
import json
import sys
from pathlib import Path
from tabulate import tabulate  # type: ignore


# Import config from project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
import config  # type: ignore


class OutputFormatter:
    """Format and output scraping results."""

    def __init__(self, output_dir: str | None = None) -> None:
        """Initialize with an output directory.

        Args:
            output_dir: Directory to save JSON outputs. Defaults to config.OUTPUT_DIR
        """
        self.output_dir = output_dir or config.OUTPUT_DIR
        os.makedirs(self.output_dir, exist_ok=True)

    def print_results(self, results: Dict[str, Dict[str, Any]]) -> None:
        """Print tables for each product with per-language statistics.

        Args:
            results: Mapping of SUPPLIER_PID -> {"product_url": str|None, "languages": {de|fr|it: specs dict}}
        """
        for SUPPLIER_PID, data in results.items():
            print("\n" + "=" * 80)
            print(f"Product: {SUPPLIER_PID}")
            url = data.get("product_url")
            if url:
                print(f"URL: {url}")
            print("-" * 80)

            # Build row keys from union of all labels across languages
            langs = data.get("languages", {})
            labels = set()
            for lang_dict in langs.values():
                labels.update(lang_dict.keys())
            ordered_labels = sorted(labels)

            table_rows = []
            for label in ordered_labels:
                row = [label]
                row.append(langs.get("de", {}).get(label, ""))
                row.append(langs.get("fr", {}).get(label, ""))
                row.append(langs.get("it", {}).get(label, ""))
                table_rows.append(row)

            print(tabulate(table_rows, headers=["Spec Label", "DE", "FR", "IT"], tablefmt="grid"))
            
            # Add per-product statistics
            de_specs = len([v for v in langs.get("de", {}).values() if v and v.strip()])
            fr_specs = len([v for v in langs.get("fr", {}).values() if v and v.strip()])
            it_specs = len([v for v in langs.get("it", {}).values() if v and v.strip()])
            
            print(f"\nüìä Stats: DE: {de_specs} specs | FR: {fr_specs} specs | IT: {it_specs} specs")

    def save_to_json(self, results: Dict[str, Dict[str, Any]], filename: str | None = None) -> str:
        """Save results to a timestamped JSON file in the output directory.

        Args:
            results: The complete results dict.
            filename: Optional base filename without extension.

        Returns:
            The full path of the saved JSON file.
        """
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"bmecat_dabag_results_{timestamp}.json"

        if not filename.endswith(".json"):
            filename += ".json"

        filepath = os.path.join(self.output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"Saved JSON: {filepath}")
        return filepath

    def display_summary(self, results: Dict[str, Dict[str, Any]]) -> None:
        """Display a summary of the scraping session.

        Args:
            results: The complete results dict.
        """
        total_products = len(results)
        
        # Count total unique specs per language across all products
        all_de_specs = set()
        all_fr_specs = set()
        all_it_specs = set()
        
        for data in results.values():
            langs = data.get("languages", {})
            all_de_specs.update(langs.get("de", {}).keys())
            all_fr_specs.update(langs.get("fr", {}).keys())
            all_it_specs.update(langs.get("it", {}).keys())

        print("\n" + "=" * 80)
        print("Summary")
        print("-" * 80)
        print(f"Total products processed: {total_products}")
        print(f"Total unique spec fields - DE: {len(all_de_specs)}, FR: {len(all_fr_specs)}, IT: {len(all_it_specs)}")
        print("-" * 80)

    def format_comparison_table(self, data: Dict[str, Any], supplier_id: str, lang: str) -> Dict[str, Any]:
        """Format merged comparison data for a single product/language into 8-column rows.

        Args:
            data: Merged data for one supplier from orchestrator (`comparison_table_builder`).
            supplier_id: The SUPPLIER_PID for identification.
            lang: Language code (de|fr|it).

        Returns:
            Structured dict containing columns, rows (8 columns only), and units handled separately.
        """
        lang_block = data.get("languages", {}).get(lang, {})
        rows_in = lang_block.get("rows", [])

        columns = [
            "original_fname", "original_fvalue",
            "dabag_fname", "dabag_fvalue",
            "web_fname", "web_fvalue",
            "ai_fname", "ai_fvalue",
        ]

        rows: List[Dict[str, Any]] = []
        units: List[Dict[str, Any]] = []
        for r in rows_in:
            rows.append({
                "original_fname": r.get("original_fname", ""),
                "original_fvalue": r.get("original_fvalue", ""),
                "dabag_fname": r.get("dabag_fname", ""),
                "dabag_fvalue": r.get("dabag_fvalue", ""),
                "web_fname": r.get("web_fname", ""),
                "web_fvalue": r.get("web_fvalue", ""),
                "ai_fname": r.get("ai_fname", ""),
                "ai_fvalue": r.get("ai_fvalue", ""),
            })
            units.append({
                "name": r.get("original_fname") or r.get("dabag_fname") or r.get("web_fname") or r.get("ai_fname", ""),
                "original_funit": r.get("original_funit"),
                "dabag_funit": r.get("dabag_funit"),
            })

        return {
            "supplier_id": supplier_id,
            "lang": lang,
            "product_url": data.get("product_url"),
            "columns": columns,
            "rows": rows,
            "units": units,
        }

    def save_comparison_table(self, table: Dict[str, Any], output_path: str | None = None) -> str:
        """Save a single comparison table to JSON.

        Args:
            table: The formatted comparison table dict.
            output_path: Optional directory to save into. Defaults to OUTPUT_DIR/comparison_tables/.

        Returns:
            Full path of the saved file.
        """
        base_dir = output_path or config.COMPARISON_TABLES_DIR
        os.makedirs(base_dir, exist_ok=True)

        supplier_id = table.get("supplier_id", "unknown")
        lang = table.get("lang", "xx")
        filename = f"comparison_{supplier_id}_{lang}.json"
        filepath = os.path.join(base_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(table, f, ensure_ascii=False, indent=2)
        print(f"Saved comparison table: {filepath}")
        return filepath

    def save_master_comparison_catalog(self, tables: List[Dict[str, Any]], output_path: str | None = None) -> str:
        """Save a master catalog consolidating all comparison tables.

        Deduplicates by supplier_id and organizes by language.

        Args:
            tables: List of formatted comparison tables.
            output_path: Optional directory to save into. Defaults to OUTPUT_DIR/comparison_tables/.

        Returns:
            Full path of the saved master catalog JSON file.
        """
        base_dir = output_path or config.COMPARISON_TABLES_DIR
        os.makedirs(base_dir, exist_ok=True)

        master: Dict[str, Any] = {"products": {}}
        for t in tables:
            sid = t.get("supplier_id", "unknown")
            lang = t.get("lang", "xx")
            entry = master["products"].setdefault(sid, {"languages": {}})
            entry["languages"][lang] = t

        filename = config.MASTER_COMPARISON_FILENAME
        filepath = os.path.join(base_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(master, f, ensure_ascii=False, indent=2)
        print(f"Saved master comparison catalog: {filepath}")
        return filepath
------------------------------------------------- ./BMEcat_transformer/utils/__init__.py --------------------------------------------------

from __future__ import annotations

"""Utilities package for BMEcat_transformer."""

from .logger import setup_logger

__all__ = ["setup_logger"]

------------------------------------------------- ./BMEcat_transformer/utils/logger.py --------------------------------------------------

from __future__ import annotations

"""Logging utility for BMEcat_transformer.

Provides a simple, consistent logging interface across all modules.
"""

import logging
import sys
from pathlib import Path
from typing import Optional


def setup_logger(
    name: str,
    level: int = logging.INFO,
    log_file: Optional[str] = None
) -> logging.Logger:
    """Setup and return a logger instance.
    
    Args:
        name: Logger name (typically __name__ from calling module)
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Optional file path to write logs to
        
    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Avoid adding handlers multiple times
    if logger.handlers:
        return logger
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    
    # Format: [INFO] module_name: message
    formatter = logging.Formatter(
        '[%(levelname)s] %(name)s: %(message)s'
    )
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Optional file handler
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

------------------------------------------------- ./BMEcat_transformer/test_original_xml.py --------------------------------------------------

from __future__ import annotations
import logging
from core.xml_readers import OriginalXMLReader
from utils.logger import setup_logger

# Enable DEBUG level
logger = setup_logger(__name__, level=logging.DEBUG)

xml_path = "/Users/dannycrescimone/Documents/data_scraping/DEWALT_BMEcat_Original.xml"
reader = OriginalXMLReader(xml_path)

# This will show all debug messages
result = reader.extract_features()

print(f"\n{'='*60}")
print(f"FINAL RESULT: {len(result)} products found")
if result:
    print(f"First product ID: {list(result.keys())[0]}")
    print(f"First product features: {len(result[list(result.keys())[0]])} features")
print(f"{'='*60}")

------------------------------------------------- ./BMEcat_transformer/processors/ai_feature_matcher.py --------------------------------------------------

"""AI-powered feature matcher for BMEcat_transformer.

Uses Grok API to intelligently match unstructured XML text to 
structured feature names from unique_features.csv and ai_generated_features.json.
"""

from __future__ import annotations

import json
import csv
import os
from typing import Dict, List, Any
from datetime import datetime
import requests

from utils.logger import setup_logger


class AIFeatureMatcher:
    """Match XML specs to structured features using Grok AI."""

    def __init__(
        self, 
        api_key: str,
        model: str,
        base_url: str,
        confidence_threshold: float,
        prompt_path: str,
        csv_path: str,
        ai_features_path: str
    ) -> None:
        """Initialize with API configuration and reference data paths.

        Args:
            api_key: Grok API key.
            model: Grok model name.
            base_url: Grok API base URL.
            confidence_threshold: Minimum confidence for AI-generated fields (0.0-1.0).
            prompt_path: Path to YAML prompt file.
            csv_path: Path to unique_features.csv (primary reference).
            ai_features_path: Path to ai_generated_features.json (fallback).
        """
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.confidence_threshold = confidence_threshold
        self.prompt_path = prompt_path
        self.csv_path = csv_path
        self.ai_features_path = ai_features_path
        self.logger = setup_logger(__name__)
        
        self.allowed_features: List[Dict[str, str]] = []
        self.ai_features: Dict[str, Any] = {"metadata": {}, "features": []}
        self.prompt_template: str = ""

    def load_references(self) -> bool:
        """Load CSV and AI features references.

        Returns:
            True if successful, False otherwise.
        """
        try:
            # Load primary CSV
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                self.allowed_features = [row for row in reader]
            self.logger.info(f"Loaded {len(self.allowed_features)} features from CSV")

            # Load AI features (create if doesn't exist)
            if os.path.exists(self.ai_features_path):
                with open(self.ai_features_path, 'r', encoding='utf-8') as f:
                    self.ai_features = json.load(f)
                self.logger.info(f"Loaded {len(self.ai_features.get('features', []))} AI features")
            else:
                self._initialize_ai_features_file()

            return True
        except Exception as e:
            self.logger.error(f"Failed to load references: {e}")
            return False

    def load_prompt(self) -> bool:
        """Load prompt template from YAML file.

        Returns:
            True if successful, False otherwise.
        """
        try:
            import yaml
            with open(self.prompt_path, 'r', encoding='utf-8') as f:
                prompt_config = yaml.safe_load(f)
                self.prompt_template = prompt_config.get('prompt', '')
            self.logger.info("Loaded prompt template")
            return True
        except Exception as e:
            self.logger.error(f"Failed to load prompt: {e}")
            return False

    def match_features(self, raw_text: Dict[str, str], product_id: str) -> List[Dict[str, Any]]:
        """Match raw XML text to structured features using AI.

        Args:
            raw_text: Dict with field names and unstructured text.
            product_id: SUPPLIER_PID for logging.

        Returns:
            List of matched features with schema:
            [{
                "fname": str,
                "fvalue": str,
                "funit": str | None,
                "source": str,
                "ai_generated": bool,
                "confidence": float
            }]
        """
        if not raw_text or not any(raw_text.values()):
            return []

        # Combine all text fields
        combined_text = "\n\n".join([f"=== {k.upper()} ===\n{v}" for k, v in raw_text.items() if v])

        # Build prompt
        prompt = self._build_prompt(combined_text)

        # Call Grok API
        try:
            response = self._call_grok_api(prompt)
            features = self._parse_grok_response(response, product_id)
            features = self._add_warning_if_needed(features)  # NEW: Add validation
            self.logger.info(f"Matched {len(features)} features for {product_id}")
            return features
        except Exception as e:
            self.logger.error(f"AI matching failed for {product_id}: {e}")
            return []

    def _build_prompt(self, text: str) -> str:
        """Build complete prompt with text and references.

        Args:
            text: Combined raw text.

        Returns:
            Complete prompt string.
        """
        # Format CSV features as reference
        csv_features = "\n".join([
            f"- {row['fname_de']}: {row['fvalue_de']}"
            for row in self.allowed_features[:50]  # Limit to avoid token overflow
        ])

        # Format AI features as secondary reference
        ai_features_list = "\n".join([
            f"- {feat['fname_de']}: {feat['fvalue_de']}"
            for feat in self.ai_features.get('features', [])[:20]
        ])

        prompt = self.prompt_template.replace("{RAW_TEXT}", text)
        prompt = prompt.replace("{ALLOWED_FEATURES_CSV}", csv_features)
        prompt = prompt.replace("{AI_FEATURES_FALLBACK}", ai_features_list)
        prompt = prompt.replace("{CONFIDENCE_THRESHOLD}", str(self.confidence_threshold))

        return prompt

    def _call_grok_api(self, prompt: str) -> Dict[str, Any]:
        """Call Grok API with prompt.

        Args:
            prompt: Complete prompt string.

        Returns:
            API response as dict.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,  # Low temp for consistency
            "response_format": {"type": "json_object"}
        }

        response = requests.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        return response.json()

    def _parse_grok_response(self, response: Dict[str, Any], product_id: str) -> List[Dict[str, Any]]:
        """Parse Grok API response into feature list.

        Args:
            response: Raw API response.
            product_id: SUPPLIER_PID for tracking.

        Returns:
            List of structured features.
        """
        try:
            content = response['choices'][0]['message']['content']
            data = json.loads(content)
            features = data.get('features', [])

            # Process and validate features
            result = []
            for feat in features:
                # Check if AI-generated and meets threshold
                if feat.get('ai_generated', False):
                    if feat.get('confidence', 0) < self.confidence_threshold:
                        continue  # Skip low confidence AI fields
                    # Add warning marker
                    feat['fname'] = f"‚ö†Ô∏è {feat['fname']}"
                    # Update AI features JSON
                    self._update_ai_features_json(feat, product_id)

                result.append({
                    "fname": feat.get('fname', ''),
                    "fvalue": feat.get('fvalue', ''),
                    "funit": feat.get('funit'),
                    "source": feat.get('source', ''),
                    "ai_generated": feat.get('ai_generated', False),
                    "confidence": feat.get('confidence', 1.0)
                })

            return result
        except Exception as e:
            self.logger.error(f"Failed to parse Grok response: {e}")
            return []

    def _update_ai_features_json(self, feature: Dict[str, Any], product_id: str) -> None:
        """Update ai_generated_features.json with new field.

        Args:
            feature: Feature dict from AI.
            product_id: SUPPLIER_PID where first seen.
        """
        # Remove warning marker for storage
        fname_clean = feature['fname'].replace('‚ö†Ô∏è ', '')

        # Check if already exists
        existing = next(
            (f for f in self.ai_features['features'] if f['fname_de'] == fname_clean),
            None
        )

        if existing:
            existing['occurrences'] += 1
        else:
            self.ai_features['features'].append({
                "fname_de": fname_clean,
                "fvalue_de": feature.get('fvalue', ''),
                "fname_fr": "",
                "fvalue_fr": "",
                "fname_it": "",
                "fvalue_it": "",
                "confidence": feature.get('confidence', 0.0),
                "first_seen_pid": product_id,
                "occurrences": 1
            })

        # Save immediately
        self._save_ai_features_json()

    def _save_ai_features_json(self) -> None:
        """Save ai_generated_features.json to disk."""
        try:
            self.ai_features['metadata']['last_updated'] = datetime.now().isoformat()
            self.ai_features['metadata']['total_features'] = len(self.ai_features['features'])
            
            with open(self.ai_features_path, 'w', encoding='utf-8') as f:
                json.dump(self.ai_features, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"Failed to save AI features: {e}")

    def _initialize_ai_features_file(self) -> None:
        """Create initial ai_generated_features.json file."""
        self.ai_features = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "confidence_threshold": self.confidence_threshold,
                "total_features": 0
            },
            "features": []
        }
        self._save_ai_features_json()
        self.logger.info("Initialized ai_generated_features.json")

    def _is_feature_in_csv(self, feature_name: str) -> bool:
        """Check if feature name exists in unique_features.csv.
        
        Uses fuzzy matching to account for minor variations.
        
        Args:
            feature_name: Feature name to check (German).
            
        Returns:
            True if found in CSV, False otherwise.
        """
        if not self.allowed_features:
            return False
        
        # Normalize for comparison
        feature_normalized = feature_name.lower().strip()
        
        # Check exact match first
        for row in self.allowed_features:
            csv_name = row.get('fname_de', '').lower().strip()
            if csv_name == feature_normalized:
                return True
        
        # Check substring match (e.g., "Garantie" in "Garantieumfang")
        for row in self.allowed_features:
            csv_name = row.get('fname_de', '').lower().strip()
            if csv_name and (csv_name in feature_normalized or feature_normalized in csv_name):
                return True
        
        return False

    def _add_warning_if_needed(self, features: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add ‚ö†Ô∏è prefix to features NOT in unique_features.csv.
        
        Args:
            features: List of feature dicts from AI.
            
        Returns:
            Same list with warning prefixes added where needed.
        """
        for feature in features:
            fname = feature.get('fname', '')
            if fname and not self._is_feature_in_csv(fname):
                # Add warning prefix if not already present
                if not fname.startswith('‚ö†Ô∏è '):
                    feature['fname'] = f"‚ö†Ô∏è {fname}"
                    self.logger.warning(f"Feature NOT in CSV taxonomy: {fname}")
        
        return features

------------------------------------------------- ./BMEcat_transformer/processors/feature_extractor.py --------------------------------------------------

"""Feature extractor for BMEcat_transformer.

Extracts unique feature names (fname) and example values (fvalue) from 
master_bmecat_dabag.json across all languages (de, fr, it).
Creates a 6-column matrix for feature name mapping reference.
"""

from __future__ import annotations

import json
import csv
import os
from typing import Dict, List, Tuple, Any
from pathlib import Path


class FeatureExtractor:
    """Extract and export unique feature names from master JSON."""

    def __init__(self, master_json_path: str, output_dir: str) -> None:
        """Initialize with paths.

        Args:
            master_json_path: Path to master_bmecat_dabag.json
            output_dir: Directory to save output CSV
        """
        self.master_json_path = master_json_path
        self.output_dir = output_dir
        self.data: Dict[str, Any] = {}
        
        # Store unique features as: {(fname_de, fname_fr, fname_it): (fvalue_de, fvalue_fr, fvalue_it)}
        self.unique_features: Dict[Tuple[str, str, str], Tuple[str, str, str]] = {}

    def load_master_json(self) -> bool:
        """Load the master JSON file.

        Returns:
            True if successful, False otherwise.
        """
        try:
            with open(self.master_json_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"‚úÖ Loaded master JSON: {len(self.data.get('products', {}))} products")
            return True
        except FileNotFoundError:
            print(f"‚ùå Error: Master JSON not found at {self.master_json_path}")
            return False
        except json.JSONDecodeError as e:
            print(f"‚ùå Error: Invalid JSON: {e}")
            return False

    def extract_features(self) -> None:
        """Extract unique features from all products.
        
        Matches features by position index across languages.
        """
        products = self.data.get('products', {})
        
        for pid, product_data in products.items():
            languages = product_data.get('languages', {})
            
            de_features = languages.get('de', {})
            fr_features = languages.get('fr', {})
            it_features = languages.get('it', {})
            
            # Convert to lists to maintain order
            de_items = list(de_features.items())
            fr_items = list(fr_features.items())
            it_items = list(it_features.items())
            
            # Match by position index
            max_len = max(len(de_items), len(fr_items), len(it_items))
            
            for i in range(max_len):
                fname_de = de_items[i][0] if i < len(de_items) else ""
                fvalue_de = de_items[i][1] if i < len(de_items) else ""
                
                fname_fr = fr_items[i][0] if i < len(fr_items) else ""
                fvalue_fr = fr_items[i][1] if i < len(fr_items) else ""
                
                fname_it = it_items[i][0] if i < len(it_items) else ""
                fvalue_it = it_items[i][1] if i < len(it_items) else ""
                
                # Create unique key and store
                feature_key = (fname_de, fname_fr, fname_it)
                
                # Skip if all empty
                if not any([fname_de, fname_fr, fname_it]):
                    continue
                
                # Store first occurrence as example
                if feature_key not in self.unique_features:
                    self.unique_features[feature_key] = (fvalue_de, fvalue_fr, fvalue_it)
        
        print(f"‚úÖ Extracted {len(self.unique_features)} unique feature mappings")

    def export_to_csv(self, filename: str = "unique_features.csv") -> str:
        """Export unique features to CSV file.

        Args:
            filename: Output CSV filename.

        Returns:
            Full path to the saved CSV file.
        """
        os.makedirs(self.output_dir, exist_ok=True)
        output_path = os.path.join(self.output_dir, filename)
        
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # Write header
            writer.writerow([
                'fname_de', 'fvalue_de',
                'fname_fr', 'fvalue_fr',
                'fname_it', 'fvalue_it'
            ])
            
            # Write data rows
            for feature_key, feature_values in sorted(self.unique_features.items()):
                row = [
                    feature_key[0], feature_values[0],  # de
                    feature_key[1], feature_values[1],  # fr
                    feature_key[2], feature_values[2]   # it
                ]
                writer.writerow(row)
        
        print(f"‚úÖ Saved CSV: {output_path}")
        return output_path

    def run(self) -> bool:
        """Run the complete feature extraction pipeline.

        Returns:
            True if successful, False otherwise.
        """
        print("="*80)
        print("Feature Extractor - Unique Feature Name Database")
        print("="*80)
        
        if not self.load_master_json():
            return False
        
        self.extract_features()
        self.export_to_csv()
        
        print("="*80)
        print("‚úÖ Feature extraction complete!")
        print("="*80)
        
        return True

------------------------------------------------- ./BMEcat_transformer/processors/xml_specs_extractor.py --------------------------------------------------

from __future__ import annotations

"""XML Technical Specs Extractor for BMEcat_transformer.

Extracts unstructured technical specifications from UDX.EDXF.* XML fields.
Handles HTML entities and preserves formatting for AI processing.
"""

import os
import re
from typing import Dict, Optional
from utils.logger import setup_logger


class XMLSpecsExtractor:
    """Extract technical specifications from UDX.EDXF XML fields."""

    def __init__(self, xml_path: str) -> None:
        """Initialize with XML file path.

        Args:
            xml_path: Path to the Original BMEcat XML file.
        """
        self.xml_path = xml_path
        self.logger = setup_logger(__name__)
        self.root: Optional[any] = None

    def load_xml(self) -> bool:
        """Load and parse the XML file using multiple fallback strategies.

        Parsing priority:
        1. lxml with recovery mode (most reliable)
        2. Regex-based extraction (ultimate fallback)

        Returns:
            True if successful, False otherwise.
        """
        # Strategy 1: Try lxml with recovery mode
        try:
            from lxml import etree as LXML_ET
            self.logger.debug("Attempting lxml parsing with recovery mode")
            with open(self.xml_path, 'rb') as f:
                raw = f.read()
            parser = LXML_ET.XMLParser(recover=True, encoding='utf-8')
            self.root = LXML_ET.fromstring(raw, parser)
            self.logger.info("Successfully loaded XML with lxml recovery mode")
            return True
        except ImportError:
            self.logger.debug("lxml not available, using regex-based extraction")
            self.root = None  # Will use regex
            return True
        except Exception as e:
            self.logger.warning(f"lxml parsing failed: {e}, falling back to regex")
            self.root = None
            return True

    def _clean_html_entities(self, text: str) -> str:
        """Clean HTML entities from text.

        Args:
            text: Raw text with HTML entities.

        Returns:
            Cleaned text with proper formatting.
        """
        # Replace common HTML entities
        text = text.replace("&lt;br&gt;", "\n")
        text = text.replace("&lt;", "<")
        text = text.replace("&gt;", ">")
        text = text.replace("&amp;", "&")
        text = text.replace("&quot;", '"')
        
        # Remove remaining HTML-like tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # Clean excessive whitespace
        text = re.sub(r'\n\s*\n', '\n', text)
        text = text.strip()
        
        return text

    def _extract_udx_block_regex(self, product_xml: str, field_mapping: Dict[str, str]) -> Dict[str, str]:
        """Extract all UDX.EDXF fields from a product's USER_DEFINED_EXTENSIONS block.

        Uses regex to extract the entire UDX section and then parse each field.
        Handles both empty and populated fields.

        Args:
            product_xml: XML content of a single PRODUCT block.
            field_mapping: Dict mapping friendly names to XML tag names.

        Returns:
            Dict with field names as keys and extracted text as values.
        """
        result: Dict[str, str] = {}

        # Extract the USER_DEFINED_EXTENSIONS block
        udx_pattern = r'<USER_DEFINED_EXTENSIONS>(.*?)</USER_DEFINED_EXTENSIONS>'
        udx_match = re.search(udx_pattern, product_xml, re.DOTALL | re.IGNORECASE)

        if not udx_match:
            self.logger.debug("No USER_DEFINED_EXTENSIONS block found")
            return result

        udx_content = udx_match.group(1)

        # Extract each UDX field from the field_mapping
        for field_name, xml_tag in field_mapping.items():
            # Pattern handles both self-closing tags (<TAG/>) and tags with content
            pattern = f'<{re.escape(xml_tag)}(?:\\s[^>]*)?>\\s*(.*?)\\s*</{re.escape(xml_tag)}>'
            match = re.search(pattern, udx_content, re.DOTALL | re.IGNORECASE)

            if match:
                raw_text = match.group(1).strip()
                if raw_text:  # Only add non-empty content
                    clean_text = self._clean_html_entities(raw_text)
                    result[field_name] = clean_text
                    self.logger.debug(f"Extracted {field_name}: {len(clean_text)} chars")
                else:
                    self.logger.debug(f"{field_name} is empty")
            else:
                # Check if it's a self-closing tag
                self_closing_pattern = f'<{re.escape(xml_tag)}\\s*/>'
                if re.search(self_closing_pattern, udx_content, re.IGNORECASE):
                    self.logger.debug(f"{field_name} is self-closing (empty)")
                else:
                    self.logger.debug(f"{field_name} not found in UDX block")

        return result

    def extract_udx_fields(self, supplier_pid: str, field_mapping: Dict[str, str]) -> Dict[str, str]:
        """Extract UDX.EDXF fields for a specific product.

        Args:
            supplier_pid: Product ID to extract fields for.
            field_mapping: Dict mapping friendly names to XML tag names.

        Returns:
            Dict with field names as keys and cleaned text as values.
        """
        try:
            with open(self.xml_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()

            # Find the PRODUCT block for this supplier_pid
            product_pattern = f'<PRODUCT[^>]*>.*?<SUPPLIER_PID>\\s*{re.escape(supplier_pid)}\\s*</SUPPLIER_PID>.*?</PRODUCT>'
            product_match = re.search(product_pattern, content, re.DOTALL | re.IGNORECASE)

            if not product_match:
                self.logger.debug(f"Product {supplier_pid} not found")
                return {}

            product_xml = product_match.group(0)
            return self._extract_udx_block_regex(product_xml, field_mapping)

        except Exception as e:
            self.logger.error(f"Extraction failed for {supplier_pid}: {e}")
            return {}

    def extract_all_products(self, field_mapping: Dict[str, str]) -> Dict[str, Dict[str, str]]:
        """Extract UDX fields for all products in XML.

        CRITICAL: This method now loads ALL products with valid PIDs,
        even if their UDX fields are empty. Products without any UDX data
        will have empty dictionaries, but they will still be in the results.

        Args:
            field_mapping: Dict mapping friendly names to XML tag names.

        Returns:
            Dict with SUPPLIER_PID as keys and field data as values.
        """
        results = {}

        try:
            with open(self.xml_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()

            # Find all PRODUCT blocks
            product_pattern = r'<PRODUCT[^>]*>(.*?)</PRODUCT>'
            products = re.findall(product_pattern, content, re.DOTALL)

            self.logger.debug(f"Found {len(products)} PRODUCT blocks")

            for product_xml in products:
                # Extract SUPPLIER_PID
                pid_match = re.search(r'<SUPPLIER_PID>\s*([^<]+?)\s*</SUPPLIER_PID>', product_xml)
                
                if not pid_match:
                    self.logger.debug("Skipping product without SUPPLIER_PID")
                    continue

                supplier_pid = pid_match.group(1).strip()
                
                # Extract UDX fields for this product
                product_data = self._extract_udx_block_regex(product_xml, field_mapping)
                
                # NEW: Save to text file
                if product_data:
                    self.save_to_text_file(supplier_pid, product_data)
                
                # This ensures consistency with product counts across all XML readers
                results[supplier_pid] = product_data
                
                if product_data:
                    self.logger.debug(f"Product {supplier_pid}: extracted {len(product_data)} UDX fields")
                else:
                    self.logger.debug(f"Product {supplier_pid}: no UDX data (empty fields)")

        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")

        self.logger.info(f"Extracted UDX fields for {len(results)} products")
        return results

    def save_to_text_file(self, supplier_pid: str, udx_fields: Dict[str, str]) -> str:
        """Save extracted UDX fields to a text file for AI processing.
        
        Creates human-readable text file with section headers.
        Overwrites existing file on each run.
        
        Args:
            supplier_pid: Product identifier for filename.
            udx_fields: Dict with field names and extracted text.
            
        Returns:
            Full path to saved text file.
        """
        import sys
        from pathlib import Path
        
        # Import config
        PROJECT_ROOT = Path(__file__).resolve().parent.parent
        if str(PROJECT_ROOT) not in sys.path:
            sys.path.append(str(PROJECT_ROOT))
        import config
        
        # Create directory if not exists
        output_dir = config.SCRAPED_TEXT_DIR
        os.makedirs(output_dir, exist_ok=True)
        
        # Build file path
        filename = f"{supplier_pid}.txt"
        filepath = os.path.join(output_dir, filename)
        
        # Build content with section headers
        content_parts = [
            f"Product: {supplier_pid}",
            f"Extracted from: Original XML (DEWALT BMEcat)\n"
        ]
        
        for field_name, text in udx_fields.items():
            if text:  # Only add non-empty sections
                section_header = field_name.upper().replace("_", " ")
                content_parts.append(f"=== {section_header} ===")
                content_parts.append(text)
                content_parts.append("")  # Empty line between sections
        
        # Write to file (overwrite mode)
        full_content = "\n".join(content_parts)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        self.logger.info(f"Saved scraped text: {filepath}")
        return filepath
------------------------------------------------- ./BMEcat_transformer/processors/feature_mapper.py --------------------------------------------------

from __future__ import annotations

"""Feature Mapper (AI placeholder).

Provides an interface for future AI-based mapping between Original XML features
and other sources (DABAG, Web, etc.). Currently returns placeholders.

Expected input and output structures are intentionally simple:
- Input:  List[{"fname": str, "fvalue": str, "funit": str|None}]
- Output: List[{"fname": str, "fvalue": str, "funit": str|None}]

Future enhancements may include:
- Semantic similarity matching and canonicalization of feature names
- Unit conversion and normalization logic
- Confidence scores and rationale traces
- Model/backend selection via configuration
"""

from typing import List, Dict, Any


class FeatureMapper:
    """Placeholder for AI-based feature mapping pipeline."""

    def map_features(self, original_features: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Map and enrich features using AI (placeholder).

        Args:
            original_features: List of features from Original XML.

        Returns:
            List of mapped features with the same schema. Currently returns
            an empty list as a placeholder to be filled by future AI logic.
        """
        # TODO: Implement AI-based mapping logic (LLM embeddings, rules, etc.)
        return []

------------------------------------------------- ./BMEcat_transformer/scripts/main.py --------------------------------------------------

"""Main entry point for BMEcat_transformer.

Pipeline:
- Parse XML path from args/user input
- Extract SUPPLIER_PIDs
- For each SUPPLIER_PID, search and scrape DABAG in DE/FR/IT
- Print tables, save JSON, display summary
"""

from __future__ import annotations

import sys
import time
from pathlib import Path
from typing import Dict, Any

# Add parent directory to path for imports
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

import config
from scrapers.dabag_scraper import DABAGScraper
from output.output_formatter import OutputFormatter
from core.input_handler import InputHandler
from core.master_json_manager import MasterJSONManager
from ui.user_prompt import UserPrompt


def get_input_path() -> str:
    """Get input file path from command-line args or prompt user.

    Returns:
        Validated path string to input file.
    """
    path: str | None = None
    if len(sys.argv) > 1:
        path = sys.argv[1]
    else:
        path = input("Enter path to BMEcat XML file: ").strip()

    if not path:
        print("‚ö†Ô∏è No input file path provided.")
        sys.exit(1)

    p = Path(path)
    if not p.exists() or not p.is_file():
        print(f"‚ö†Ô∏è Input file path does not exist or is not a file: {path}")
        sys.exit(1)
    return str(p)


def main() -> None:
    start = time.time()
    print("=" * 80)
    print("BMEcat_transformer - DABAG Spec Scraper")
    print(f"Scraping method: {config.SCRAPING_METHOD}")
    print("=" * 80)

    input_path = get_input_path()

    # Extract product IDs
    SUPPLIER_PIDs = InputHandler.load_supplier_ids(input_path)
    print(f"Found {len(SUPPLIER_PIDs)} SUPPLIER_PID(s) to process.")
    if not SUPPLIER_PIDs:
        print("Nothing to do. Exiting.")
        return

    # Initialize master JSON manager
    master_manager = MasterJSONManager(
        master_filename=config.MASTER_JSON_FILENAME,
        output_dir=config.OUTPUT_DIR,
        backup_count=config.MASTER_JSON_BACKUP_COUNT
    )
    master_manager.load()

    # Show master JSON statistics
    stats = master_manager.get_statistics()
    print(f"\nüìä Master JSON Status:")
    print(f"  Total products: {stats['total_products']}")
    print(f"  Last updated: {stats['last_updated']}")
    print()

    scraper = DABAGScraper()
    results: Dict[str, Dict[str, Any]] = {}
    
    # Counters for summary
    new_count = 0
    updated_count = 0
    skipped_count = 0

    for idx, pid in enumerate(SUPPLIER_PIDs, 1):
        print("-" * 80)
        print(f"[{idx}/{len(SUPPLIER_PIDs)}] Processing SUPPLIER_PID: {pid}")

        # Check if ID exists in master JSON
        exists, existing_data = master_manager.check_id_exists(pid)

        if exists:
            # Show existing data WITHOUT scraping
            UserPrompt.show_existing_data(pid, existing_data)

            # Ask user for decision BEFORE scraping
            decision = UserPrompt.prompt_update_decision(pid)

            if decision == "update":
                # Only scrape if user wants to update
                try:
                    new_data = scraper.process_product(pid)
                    master_manager.update_product(pid, new_data)
                    results[pid] = new_data
                    updated_count += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è  Warning: Error processing {pid}: {e}")
                    results[pid] = existing_data  # Keep existing on error
            else:
                print(f"‚è≠Ô∏è  Skipping {pid} (keeping existing data)")
                results[pid] = existing_data
                skipped_count += 1

        else:
            # New ID - scrape and append
            try:
                data = scraper.process_product(pid)
                master_manager.append_product(pid, data)
                results[pid] = data
                new_count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è  Warning: Error processing {pid}: {e}")
                results[pid] = {"SUPPLIER_PID": pid, "product_url": None, "languages": {}}

    # Save master JSON
    master_manager.save()

    # Display results and save timestamped export
    formatter = OutputFormatter(output_dir=config.OUTPUT_DIR)
    formatter.print_results(results)
    formatter.save_to_json(results)  # Keeps timestamped export functionality
    formatter.display_summary(results)

    # Display master JSON update summary
    print("\n" + "=" * 80)
    print("Master JSON Update Summary")
    print("-" * 80)
    print(f"New products added: {new_count}")
    print(f"Products updated: {updated_count}")
    print(f"Products skipped: {skipped_count}")
    print(f"Total in master: {master_manager.get_statistics()['total_products']}")
    print("-" * 80)

    elapsed = time.time() - start
    print(f"Elapsed time: {elapsed:.2f}s")


if __name__ == "__main__":
    main()

------------------------------------------------- ./BMEcat_transformer/scripts/extract_features_StandAlone.py --------------------------------------------------

"""Standalone script to extract unique feature names from master JSON.
OVERVIEW:
This script analyzes the master product database (master_bmecat_dabag.json) 
and extracts all unique feature/specification field names across three languages 
(German, French, Italian). It creates a reference CSV that maps how the same 
feature is named in each language.

PURPOSE:
When scraping product specifications from DABAG, the same technical feature 
may have different names in different languages. For example:
  - German: "Leistung" 
  - French: "Puissance"
  - Italian: "Potenza"
  
This tool helps identify these cross-language mappings by analyzing position 
indices in the specification tables, assuming features appear in the same order 
across all language versions.

WORKFLOW:
1. Loads master_bmecat_dabag.json containing all scraped products
2. Iterates through all products and their multi-language specifications
3. Matches features by their position index (1st feature in DE = 1st in FR = 1st in IT)
4. Stores unique feature name combinations with example values
5. Exports to CSV for manual review and mapping

INPUT:
- File: master_bmecat_dabag.json (from outputs/ directory)
- Structure: JSON with products containing nested language-specific specs
- Example product structure:
  {
    "products": {
      "PROD123": {
        "languages": {
          "de": {"Leistung": "1200W", "Gewicht": "2.5kg"},
          "fr": {"Puissance": "1200W", "Poids": "2.5kg"},
          "it": {"Potenza": "1200W", "Peso": "2.5kg"}
        }
      }
    }
  }

OUTPUT:
- File: unique_features.csv (saved to outputs/ directory)
- Format: 6 columns showing feature names and example values:
  | fname_de | fvalue_de | fname_fr | fvalue_fr | fname_it | fvalue_it |
  |----------|-----------|----------|-----------|----------|-----------|
  | Leistung | 1200W     | Puissance| 1200W     | Potenza  | 1200W     |
  | Gewicht  | 2.5kg     | Poids    | 2.5kg     | Peso     | 2.5kg     |

USAGE:
    python3 extract_features.py

DEPENDENCIES:
- config.py: Contains OUTPUT_DIR and MASTER_JSON_FILENAME settings
- src/feature_extractor.py: Core extraction logic

NOTES:
- Features are matched by position, not by value matching
- Only the first occurrence of each unique feature combination is stored
- Empty/missing features in any language are preserved as empty strings
- The script requires master_bmecat_dabag.json to exist (run main scraper first)

"""

from __future__ import annotations

import sys
from pathlib import Path

# Add parent directory to path
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))

import config
from processors.feature_extractor import FeatureExtractor


def main() -> None:
    """Run feature extraction."""
    # Construct master JSON path
    master_json_path = Path(config.OUTPUT_DIR) / config.MASTER_JSON_FILENAME
    
    if not master_json_path.exists():
        print(f"‚ùå Error: Master JSON not found at {master_json_path}")
        print(f"Please run the main scraper first to generate {config.MASTER_JSON_FILENAME}")
        sys.exit(1)
    
    # Run extractor
    extractor = FeatureExtractor(
        master_json_path=str(master_json_path),
        output_dir=config.OUTPUT_DIR
    )
    
    success = extractor.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

------------------------------------------------- ./BMEcat_transformer/scripts/create_comparison_tables.py --------------------------------------------------

from __future__ import annotations

"""Create comparison tables from Original XML, DABAG XML, and web master JSON.

Usage:
    python3 scripts/create_comparison_tables.py \
      --original path/to/DEWALT_BMEcat_Original.xml \
      --dabag path/to/DEWALT_Version_DABAG.xml \
      [--auto-scrape]
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List

# Add project root for imports
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Local imports
import config  # type: ignore
from core.comparison_table_builder import ComparisonTableBuilder  # type: ignore
from output.output_formatter import OutputFormatter  # type: ignore
from utils.logger import setup_logger


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate comparison tables from Original and DABAG BMEcat XMLs",
    )
    parser.add_argument(
        "--original",
        required=True,
        help="Path to Original BMEcat XML file (e.g., DEWALT_BMEcat_Original.xml)",
    )
    parser.add_argument(
        "--dabag",
        required=True,
        help="Path to DABAG BMEcat XML file (e.g., DEWALT_Version_DABAG.xml)",
    )
    parser.add_argument(
        "--auto-scrape",
        action="store_true",
        help="Automatically scrape missing supplier IDs from web",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    
    # Initialize logging
    logger = setup_logger(__name__)

    original_xml_path = Path(args.original)
    dabag_xml_path = Path(args.dabag)

    if not original_xml_path.exists() or not original_xml_path.is_file():
        print(f"‚ùå Original XML not found or not a file: {original_xml_path}")
        sys.exit(1)
    if not dabag_xml_path.exists() or not dabag_xml_path.is_file():
        print(f"‚ùå DABAG XML not found or not a file: {dabag_xml_path}")
        sys.exit(1)

    print("=" * 80)
    print("BMEcat_transformer - Comparison Table Generation")
    print("=" * 80)

    builder = ComparisonTableBuilder(
        original_xml_path=str(original_xml_path),
        dabag_xml_path=str(dabag_xml_path),
    )

    merged = builder.build_comparison_tables(auto_scrape=bool(args.auto_scrape))

    formatter = OutputFormatter(output_dir=config.OUTPUT_DIR)

    # Format and save per-product per-language tables
    saved_files: List[str] = []
    formatted_tables: List[Dict[str, Any]] = []

    for supplier_id, data in merged.items():
        for lang in ["de", "fr", "it"]:
            table = formatter.format_comparison_table(data, supplier_id, lang)
            formatted_tables.append(table)
            path = formatter.save_comparison_table(table)
            saved_files.append(path)

    # Save master comparison catalog
    master_path = formatter.save_master_comparison_catalog(formatted_tables)

    # Display summary
    print("\n" + "=" * 80)
    print("Comparison Table Generation Summary")
    print("-" * 80)
    print(f"Products processed: {len(merged)}")
    print(f"Tables created: {len(saved_files)}")
    print(f"Master catalog: {master_path}")
    print("-" * 80)


if __name__ == "__main__":
    main()

------------------------------------------------- ./BMEcat_transformer/prompts/xml_specs_mapping.yaml --------------------------------------------------

prompt: |
  You are a precision technical specification extraction expert for power tools and industrial equipment.
  
  ## TASK
  Extract structured feature-value pairs from unstructured German technical text. Match features to an allowed taxonomy when possible.
  
  ## INPUT DATA
  ### Raw Technical Text:
  {RAW_TEXT}
  
  ### Allowed Feature Names (PRIMARY - use these first):
  {ALLOWED_FEATURES_CSV}
  
  ### AI-Generated Features (FALLBACK - only if no CSV match):
  {AI_FEATURES_FALLBACK}
  
  ## EXTRACTION RULES
  1. **Feature Name Matching** (CRITICAL - FOLLOW STRICTLY):
     - ALWAYS try to match to the PRIMARY allowed features list FIRST
     - The PRIMARY list is your source of truth - prefer it over creating new features
     - Use semantic similarity, not exact string matching
     - Consider abbreviations, units, and variations (e.g., "√ò" = "Durchmesser", "V" = "Volt")
     - ONLY use FALLBACK list if confidence in PRIMARY match is < 40%
     - NEVER create new feature names if a reasonable PRIMARY match exists (>40% confidence)
     - Examples of good matches:
       * "Garantie" in text ‚Üí match to "Garantie" in CSV (exact)
       * "Akkukapazit√§t" in text ‚Üí match to "Akku-Kapazit√§t" in CSV (similar)
       * "Gewicht mit Akku" in text ‚Üí match to "Gewicht" in CSV (substring)
  
  2. **Value Extraction**:
     - Extract ONLY values explicitly stated in the text
     - NEVER hallucinate or infer values
     - Preserve units (mm, V, kg, W, min‚Åª¬π, etc.)
     - Normalize formats: "2,5 kg" ‚Üí "2.5 kg", "1.200 W" ‚Üí "1200 W"
  
  3. **AI-Generated Fields** (USE SPARINGLY - confidence ‚â• {CONFIDENCE_THRESHOLD}):
     - ONLY create new feature names if NO match in PRIMARY (>40% confidence) OR FALLBACK
     - Before creating new feature, double-check PRIMARY list one more time
     - Use same naming style as existing features (German, technical, concise)
     - Mark with ai_generated: true
     - Include confidence score (must be ‚â• {CONFIDENCE_THRESHOLD})
     - Example: If text says "Garantie" and CSV has "Garantie", DO NOT create new feature
  
  4. **Source Tracking**:
     - Track which section text came from (LANGTEXT, TECHNISCHE_DATEN, etc.)
  
  ## EXAMPLES
  
  Example 1: Direct CSV Match
  Input: "Scheibendurchmesser (mm): 125"
  Output JSON:
  {
    "fname": "Scheiben-√ò",
    "fvalue": "125 mm",
    "funit": null,
    "source": "TECHNISCHE_DATEN",
    "ai_generated": false,
    "confidence": 0.95
  }
  
  Example 2: Unit Normalization
  Input: "Akku-Spannung: 18 Volt"
  Output JSON:
  {
    "fname": "Spannung",
    "fvalue": "18 V",
    "funit": null,
    "source": "TECHNISCHE_DATEN",
    "ai_generated": false,
    "confidence": 1.0
  }
  
  Example 3: Complex Text Parsing
  Input: "Leerlaufdrehzahl: 0 ‚Äì 1.100 min-1"
  Output JSON:
  {
    "fname": "Leerlaufdrehzahl",
    "fvalue": "0-1100 min‚Åª¬π",
    "funit": null,
    "source": "TECHNISCHE_DATEN",
    "ai_generated": false,
    "confidence": 0.98
  }
  
  Example 4: AI-Generated Field (no CSV match)
  Input: "Innovative dreistufige LED-Leuchte mit sehr hoher Leuchtkraft von bis zu 77 Lumen"
  Output JSON:
  {
    "fname": "LED-Leuchtkraft",
    "fvalue": "77 Lumen",
    "funit": null,
    "source": "LANGTEXT",
    "ai_generated": true,
    "confidence": 0.87
  }
  
  Example 5: Multi-Item List
  Input: "Lieferumfang: 1x T STAK-Box II, 1x Zusatzhandgriff, 1x Bohrtiefenanschlag"
  Output JSON:
  {
    "fname": "Verpackung",
    "fvalue": "T-STAK-Box II",
    "funit": null,
    "source": "LIEFERUMFANG",
    "ai_generated": false,
    "confidence": 0.92
  }
  
  ## OUTPUT FORMAT
  Respond with valid JSON only in this structure:
  {
    "features": [
      {
        "fname": "string (from CSV or AI-generated)",
        "fvalue": "string (extracted from text)",
        "funit": "string or null",
        "source": "string (XML field name)",
        "ai_generated": "boolean",
        "confidence": "float 0.0-1.0"
      }
    ]
  }
  
  ## CRITICAL REQUIREMENTS
  - Output valid JSON only, no markdown or explanations
  - Do NOT include features with empty/null values
  - Confidence must be ‚â•{CONFIDENCE_THRESHOLD} for ai_generated fields
  - Preserve original German feature names
  - Track source field for every feature
------------------------------------------------- ./BMEcat_transformer/scrapers/dabag_scraper.py --------------------------------------------------

"""DABAG scraper for BMEcat_transformer.

Selects scraping backend based on config (firecrawl or playwright) to locate
product pages, then fetches product page HTML and extracts specification tables
for multiple languages.
"""

from __future__ import annotations

from typing import Dict, Optional
import sys
from pathlib import Path
from urllib.parse import urljoin
import requests

# Import config from project root
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))
import config  # type: ignore

# Import appropriate scraper based on config
PARENT_PROJECT = PROJECT_ROOT.parent
sys.path.append(str(PARENT_PROJECT))

if config.SCRAPING_METHOD == "firecrawl":
    from easy_rich.src.web_scraper import WebScraper  # type: ignore
elif config.SCRAPING_METHOD == "playwright":
    from manual_scrape.src.web_scraper import WebScraper  # type: ignore
else:  # Defensive
    from easy_rich.src.web_scraper import WebScraper  # type: ignore

# Local imports
from scrapers.table_extractor import TableExtractor


class DABAGScraper:
    """Search and scrape DABAG product pages in multiple languages."""

    def __init__(self) -> None:
        """Initialize scraper backend and table extractor."""
        try:
            self.scraper = WebScraper()
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Failed to initialize scraper backend: {e}")
            self.scraper = None
        self.table_extractor = TableExtractor()

    def search_product(self, SUPPLIER_PID: str) -> Optional[str]:
        """Search DABAG for a product and return the product detail URL.

        Args:
            SUPPLIER_PID: The product identifier to search.

        Returns:
            Full URL to the product detail page, or None if not found.
        """
        try:
            search_url = f"{config.DABAG_BASE_URL}/?q={SUPPLIER_PID}&srv=search"
            if not self.scraper:
                print("‚ö†Ô∏è Warning: Scraper not initialized; cannot perform search.")
                return None

            result = self.scraper.scrape_page(search_url)
            if not result:
                print(f"‚ö†Ô∏è Warning: Search failed for SUPPLIER_PID={SUPPLIER_PID}")
                return None

            markdown = result.get("markdown_content", "")
            # Reuse link extractor from the backend if available
            try:
                links = self.scraper.extract_links_from_markdown(markdown, search_url)  # type: ignore[attr-defined]
            except Exception:
                links = []

            # Find product detail link with expected pattern
            target: Optional[str] = None
            for _title, href in links:
                if "srv=search&pg=det&q=" in href:
                    target = href
                    break

            if not target:
                print(f"‚ö†Ô∏è Warning: Product detail link not found for SUPPLIER_PID={SUPPLIER_PID}")
                return None

            # Ensure full URL
            full_url = target if target.startswith("http") else urljoin(config.DABAG_BASE_URL, target)
            return full_url

        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Error during product search for {SUPPLIER_PID}: {e}")
            return None

    def scrape_product_languages(self, base_url: str, SUPPLIER_PID: str) -> Dict[str, Dict[str, str]]:
        """Scrape product specs for all configured languages.

        Args:
            base_url: The base product detail page URL (likely in default language).
            SUPPLIER_PID: The product identifier for logging only.

        Returns:
            Mapping of language code (de/fr/it) to specs dict.
        """
        results: Dict[str, Dict[str, str]] = {}
        headers = {"User-Agent": "Mozilla/5.0 (compatible; BMEcatTransformer/1.0)"}

        for lang_code, lang_id in config.LANGUAGES.items():
            try:
                url = f"{base_url}&&&lngId={lang_id}"
                resp = requests.get(url, headers=headers, timeout=30)
                if resp.status_code >= 400:
                    print(f"‚ö†Ô∏è Warning: {lang_code.upper()} page HTTP {resp.status_code} for {SUPPLIER_PID}")
                    results[lang_code] = {}
                    continue

                html = resp.text
                specs = self.table_extractor.extract_specs_table(html)
                results[lang_code] = specs
            except Exception as e:
                print(f"‚ö†Ô∏è Warning: Failed to scrape {lang_code.upper()} for {SUPPLIER_PID}: {e}")
                results[lang_code] = {}

        return results

    def process_product(self, SUPPLIER_PID: str) -> Dict[str, object]:
        """Search and scrape a single product across languages.

        Returns a dict with structure:
        {
            "SUPPLIER_PID": str,
            "product_url": str | None,
            "languages": {"de": {...}, "fr": {...}, "it": {...}}
        }
        """
        product_url = self.search_product(SUPPLIER_PID)
        lang_data: Dict[str, Dict[str, str]] = {}
        if product_url:
            lang_data = self.scrape_product_languages(product_url, SUPPLIER_PID)
        else:
            print(f"‚ö†Ô∏è Warning: Skipping language scrape; no product URL for {SUPPLIER_PID}")

        return {
            "SUPPLIER_PID": SUPPLIER_PID,
            "product_url": product_url,
            "languages": lang_data,
        }


------------------------------------------------- ./BMEcat_transformer/scrapers/table_extractor.py --------------------------------------------------

"""Table extractor for BMEcat_transformer.

Parses DABAG product pages' specification table into a dictionary mapping
label -> value. Designed to be resilient to minor HTML variations.
"""

from __future__ import annotations

from typing import Dict
from bs4 import BeautifulSoup  # type: ignore


class TableExtractor:
    """Extract specification tables from DABAG HTML content.

    Methods:
        extract_specs_table(html_content): Return dict of spec label -> value.
    """

    def __init__(self) -> None:
        """Initialize the extractor (no state required)."""
        pass

    def extract_specs_table(self, html_content: str) -> Dict[str, str]:
        """Extract specifications table from given HTML.

        The table is expected to have class
        "w-100 table table-striped m-0". Each row should contain two
        <td> cells: label and value.

        Args:
            html_content: Full HTML content of the product page.

        Returns:
            Dictionary mapping label -> value. Empty if table not found
            or on parse errors.
        """
        specs: Dict[str, str] = {}
        try:
            soup = BeautifulSoup(html_content or "", "html.parser")
            table = soup.find("table", class_="w-100 table table-striped m-0")
            if table is None:
                # Attempt a broader match if exact class chain changes order
                # or is partially applied by the site.
                candidate_tables = soup.find_all("table")
                for t in candidate_tables:
                    classes = set((t.get("class") or []))
                    needed = {"w-100", "table", "table-striped", "m-0"}
                    if needed.issubset(classes):
                        table = t
                        break

            if table is None:
                print("‚ö†Ô∏è Warning: Specification table not found in HTML.")
                return specs

            for tr in table.find_all("tr"):
                tds = tr.find_all("td")
                if len(tds) < 2:
                    continue
                label = (tds[0].get_text(strip=True) or "").strip()
                value = (tds[1].get_text(strip=True) or "").strip()
                if label:
                    specs[label] = value

        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Failed to parse specification table: {e}")
            return {}

        return specs


------------------------------------------------- ./BMEcat_transformer/test_debug2.py --------------------------------------------------

from lxml import etree as LXML_ET

xml_path = "/Users/dannycrescimone/Documents/data_scraping/DEWALT_BMEcat_Original.xml"

with open(xml_path, 'rb') as f:
    raw = f.read()

parser = LXML_ET.XMLParser(recover=True, encoding='utf-8')
root = LXML_ET.fromstring(raw, parser)

product_nodes = root.xpath('.//*[local-name()="PRODUCT"]')
print(f"Found {len(product_nodes)} PRODUCT nodes")

for i, prod in enumerate(product_nodes):
    print(f"\n--- Product {i+1} ---")
    
    # Test the EXACT xpath from your code
    pid_nodes = prod.xpath('.//SUPPLIER_PID | .//*[local-name()="SUPPLIER_PID"]')
    print(f"pid_nodes length: {len(pid_nodes)}")
    print(f"pid_nodes: {pid_nodes}")
    
    if pid_nodes:
        print(f"pid_nodes[0]: {pid_nodes[0]}")
        print(f"pid_nodes[0].text: {pid_nodes[0].text}")
        if pid_nodes[0].text:
            supplier_id = pid_nodes[0].text.strip()
            print(f"supplier_id after strip: '{supplier_id}'")
            print(f"bool(supplier_id): {bool(supplier_id)}")
        else:
            print("pid_nodes[0].text is None or empty!")
