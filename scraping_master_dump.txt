Folder structure:
.
‚îú‚îÄ‚îÄ dabag_ch_20251010_183516
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ www_dabag_ch__data.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ www_dabag_ch_.md
‚îú‚îÄ‚îÄ doc_processor
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ firecrawl_parser.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ json_generator.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ llm_processor.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ pdf_handler.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ user_prompt.py
‚îú‚îÄ‚îÄ doc_scraper
‚îú‚îÄ‚îÄ easy_rich
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ serp_api_client.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ web_scraper.py
‚îú‚îÄ‚îÄ imdb_com_20250926_170527
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ www_imdb_com_title_tt0120363_fullcredits_data.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ www_imdb_com_title_tt0120363_fullcredits.md
‚îú‚îÄ‚îÄ imdb_com_toy_story_2_20250926_170241
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ toy_story_2_results_data.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ toy_story_2_results.md
‚îú‚îÄ‚îÄ linkedin_com_20250927_112938
‚îú‚îÄ‚îÄ manual_scrape
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ install_browsers.sh
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ README.md
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ src
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ browser_utils.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ browsers
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ browser_chromium.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ browser_factory.py
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ browser_firefox.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ content_extractor.py
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ serp_api_client.py
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ web_scraper.py
‚îú‚îÄ‚îÄ outputs
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ Stihl_Version DABAG_summary_20251010_183123.json
‚îú‚îÄ‚îÄ project_dump.sh
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ scraping_master_dump.txt
‚îî‚îÄ‚îÄ store_steampowered_com_spider_man_20250926_171735
    ‚îú‚îÄ‚îÄ spider_man_results_data.json
    ‚îî‚îÄ‚îÄ spider_man_results.md

16 directories, 41 files


--- File Contents ---


------------------------------------------------- ./doc_processor/config.py --------------------------------------------------

"""
Configuration settings for Document Processor.
Follows same pattern as easy_rich and manual_scrape configs.
"""

import os
from dotenv import load_dotenv

# Load environment variables from root .env
load_dotenv()

# API Configuration
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")  # Used for URL-based PDF parsing
GROK_API_KEY = os.getenv("GROK_API_KEY")  # xAI API key
GROK_MODEL = os.getenv("GROK_MODEL", "grok-beta")  # Default model

# Output Settings
OUTPUT_DIR = "outputs/"
FILENAME_PATTERN = "{original_name}_summary_{timestamp}.json"

# JSON Template - AI fills these fields
DOCUMENT_TEMPLATE = {
    "executive_summary": "",
    "document_type": "",
    "key_topics": [],
    "technical_details": {
        "technologies_mentioned": [],
        "requirements": [],
        "constraints": []
    },
    "entities": {
        "people": [],
        "organizations": [],
        "products": [],
        "dates": []
    },
    "action_items": [],
    "decisions_made": [],
    "open_questions": [],
    "complexity_assessment": "",
    "estimated_read_time_minutes": 0,
    "critical_sections": []
}

# User Questions - asked in terminal after AI processing
USER_QUESTIONS = [
    "What is the primary purpose of this document?",
    "Who is the intended audience? (e.g., developers, executives)",
    "Project or client name (if applicable):",
    "Any critical deadlines or milestones mentioned?",
    "Specific areas you want highlighted or tracked:",
    "Additional context or notes:"
]

# Validation
if not FIRECRAWL_API_KEY:
    raise ValueError("FIRECRAWL_API_KEY not found in environment variables")

if not GROK_API_KEY:
    raise ValueError("GROK_API_KEY not found in environment variables")


------------------------------------------------- ./doc_processor/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Document Processor - PDF & XML Analysis Tool
Parses PDFs/XMLs, analyzes with AI, collects user context, outputs structured JSON.
"""

import sys
import time
from pathlib import Path

# Import configuration
import config

# Import modules
from src.pdf_handler import PDFHandler
from src.firecrawl_parser import FirecrawlParser
from src.llm_processor import LLMProcessor
from src.user_prompt import UserPrompt
from src.json_generator import JSONGenerator


def get_file_path() -> str:
    """Get file path (PDF or XML) from command line or user input."""
    if len(sys.argv) > 1:
        return sys.argv[1]
    else:
        print("üìÑ Document Processor (PDF & XML)")
        print("="*60)
        file_path = input("Enter path to PDF or XML file: ").strip()
        return file_path


def main():
    """Main orchestration function."""
    start_time = time.time()
    
    try:
        # Get file path
        file_path = get_file_path()
        
        # PHASE 1: Validate file
        print("\nüîç Validating file...")
        pdf_handler = PDFHandler()
        
        if not pdf_handler.validate_file(file_path):
            print("‚ùå Validation failed. Exiting.")
            return
        
        file_info = pdf_handler.get_file_info(file_path)
        file_type = file_info.get('file_type', 'pdf')
        print(f"‚úÖ Valid {file_type.upper()}: {file_info['filename']} ({file_info['size_mb']} MB)")
        
        # PHASE 2: Parse document (auto-detects URL vs local, PDF vs XML)
        print(f"\nüìÑ Extracting content from {file_type.upper()}...")
        parser = FirecrawlParser(config.FIRECRAWL_API_KEY)
        
        parse_result = parser.parse_document(file_path, file_type)
        if not parse_result:
            print(f"‚ùå Failed to extract content from {file_type.upper()}. Exiting.")
            return
        
        markdown_text = parser.get_parsed_content(parse_result)
        file_info["word_count"] = parse_result.get("word_count", 0)
        print(f"‚úÖ Extracted {file_info['word_count']} words")
        
        # PHASE 3: Process with Grok AI
        print("\nü§ñ Analyzing with LLM (xAI Grok by default)...")
        grok = LLMProcessor(config.GROK_API_KEY, config.GROK_MODEL, base_url="https://api.x.ai/v1")
        
        ai_analysis = grok.process_document(markdown_text, config.DOCUMENT_TEMPLATE)
        if not ai_analysis:
            print("‚ùå AI processing failed. Exiting.")
            return
        
        # PHASE 4: Collect user context
        print("\nüìù Collecting additional context...")
        user_prompt = UserPrompt(config.USER_QUESTIONS)
        
        raw_answers = user_prompt.ask_questions()
        user_context = user_prompt.format_for_json(raw_answers)
        
        # PHASE 5: Generate output
        print("\nüíæ Generating JSON output...")
        
        # Calculate processing time
        processing_time = round(time.time() - start_time, 2)
        
        processing_meta = {
            "grok_model": config.GROK_MODEL,
            "parsing_method": parse_result.get("method", "unknown"),
            "file_type": file_type,
            "total_processing_time_seconds": processing_time
        }
        
        generator = JSONGenerator(config.OUTPUT_DIR)
        
        final_output = generator.create_output(
            ai_data=ai_analysis,
            user_answers=user_context,
            file_metadata=file_info,
            processing_metadata=processing_meta
        )
        
        output_path = generator.save_to_file(final_output, file_info["filename"])
        
        if output_path:
            # Display summary
            generator.display_summary(final_output)
            print(f"‚úÖ Processing complete! Saved to: {output_path}")
            print(f"‚è±Ô∏è  Total time: {processing_time}s")
        else:
            print("‚ùå Failed to save output file.")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Process interrupted by user. Exiting.")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


------------------------------------------------- ./doc_processor/src/pdf_handler.py --------------------------------------------------

"""PDF and XML file validation and handling."""

import os
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict


class PDFHandler:
    """Validates and prepares PDF and XML files for processing."""
    
    def __init__(self):
        """Initialize file handler for PDF and XML files."""
        pass
    
    # Supported file extensions
    SUPPORTED_EXTENSIONS = {'.pdf', '.xml'}
    
    def validate_file(self, file_path: str) -> bool:
        """
        Validate that input is either a valid local PDF/XML file or a PDF/XML URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
        """
        try:
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Basic sanity: must have netloc and path ending with supported extension
                if not parsed.netloc:
                    print(f"‚ùå Invalid URL: {file_path}")
                    return False
                path_lower = parsed.path.lower()
                if not any(path_lower.endswith(ext) for ext in self.SUPPORTED_EXTENSIONS):
                    print(f"‚ùå URL does not point to a supported file (PDF or XML): {file_path}")
                    return False
                return True

            # Local file case
            path = Path(file_path)

            # Check exists
            if not path.exists():
                print(f"‚ùå File not found: {file_path}")
                return False

            # Check is file (not directory)
            if not path.is_file():
                print(f"‚ùå Not a file: {file_path}")
                return False

            # Check supported extension
            if path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
                print(f"‚ùå Not a supported file type (PDF or XML): {file_path}")
                return False

            # Check readable
            if not os.access(file_path, os.R_OK):
                print(f"‚ùå File not readable: {file_path}")
                return False

            return True

        except Exception as e:
            print(f"‚ùå Error validating file: {e}")
            return False

    def get_file_type(self, file_path: str) -> str:
        """
        Determine file type (pdf or xml).
        
        Args:
            file_path: Path to file or URL
            
        Returns:
            'pdf' or 'xml' or 'unknown'
        """
        try:
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                path_lower = parsed.path.lower()
            else:
                path_lower = str(file_path).lower()
            
            if path_lower.endswith('.pdf'):
                return 'pdf'
            elif path_lower.endswith('.xml'):
                return 'xml'
            else:
                return 'unknown'
        except Exception:
            return 'unknown'
    
    def get_file_info(self, file_path: str) -> Dict[str, object]:
        """
        Extract file metadata for local PDF/XML or URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
            Dict with file info
        """
        try:
            file_type = self.get_file_type(file_path)
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Derive filename from URL path
                name = Path(parsed.path).name or f"document.{file_type}"
                return {
                    "filename": name,
                    "file_path": file_path,
                    "size_mb": 0,
                    "file_type": file_type
                }
            
            # Local file case
            path = Path(file_path)
            size_mb = path.stat().st_size / (1024 * 1024)
            
            return {
                "filename": path.name,
                "file_path": str(path.absolute()),
                "size_mb": round(size_mb, 2),
                "file_type": file_type
            }
        except Exception as e:
            print(f"‚ö†Ô∏è  Error getting file info: {e}")
            return {
                "filename": "unknown",
                "file_path": file_path,
                "size_mb": 0,
                "file_type": "unknown"
            }

------------------------------------------------- ./doc_processor/src/firecrawl_parser.py --------------------------------------------------

"""Firecrawl PDF/XML parsing integration."""

from typing import Optional, Dict
from firecrawl import Firecrawl
import fitz  # PyMuPDF
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom

class FirecrawlParser:
    """Handles PDF and XML to text conversion using Firecrawl and local parsers."""

    def __init__(self, api_key: str):
        """
        Initialize Firecrawl client.

        Args:
            api_key: Firecrawl API key
        """
        if not api_key:
            raise ValueError("Firecrawl API key is required")

        self.firecrawl = Firecrawl(api_key=api_key)

    def _is_url(self, path: str) -> bool:
        """
        Check if input is a URL or local file path.

        Args:
            path: Input path or URL

        Returns:
            True if URL, False if local path
        """
        return path.startswith("http://") or path.startswith("https://")

    def _parse_local_pdf(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local PDF file using PyMuPDF.

        Args:
            file_path: Path to local PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("üìÑ Parsing local PDF with PyMuPDF...")

            # Open PDF
            doc = fitz.open(file_path)

            # Extract text from all pages
            markdown_content = ""
            for page_num, page in enumerate(doc, start=1):
                text = page.get_text()
                markdown_content += f"\n\n## Page {page_num}\n\n{text}"

            page_count = len(doc)
            doc.close()

            if not markdown_content or len(markdown_content) < 50:
                print("‚ö†Ô∏è  Warning: Extracted content is very short")
                return None

            # Count words
            word_count = len(markdown_content.split())

            print(f"‚úÖ Extracted {word_count:,} words from {page_count} pages")

            return {
                "markdown": markdown_content.strip(),
                "word_count": word_count,
                "page_count": page_count,
                "method": "pymupdf",
            }

        except Exception as e:
            print(f"‚ùå Error parsing local PDF: {e}")
            return None

    def _parse_local_xml(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local XML file and convert to readable text format.
        
        Args:
            file_path: Path to local XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            # Read XML file
            tree = ET.parse(file_path)
            root = tree.getroot()

            # Convert to pretty-printed string
            xml_string = ET.tostring(root, encoding='unicode')
            pretty_xml = minidom.parseString(xml_string).toprettyxml(indent="  ")

            # Extract text content for analysis
            text_content = self._extract_xml_text(root)

            # Combine structure and content
            markdown_text = f"# XML Document Structure\n\n```xml\n{pretty_xml}\n```\n\n# Extracted Text Content\n\n{text_content}"

            word_count = len(text_content.split())

            return {
                "markdown": markdown_text,
                "word_count": word_count,
                "method": "xml_parser"
            }
        
        except Exception as e:
            print(f"‚ùå Error parsing local XML: {e}")
            return None

    def _parse_url_pdf(self, pdf_url: str) -> Optional[Dict[str, object]]:
        """
        Parse PDF from URL using Firecrawl.

        Args:
            pdf_url: URL to PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("üìÑ Parsing PDF from URL with Firecrawl...")

            # Use Firecrawl with PDF parser explicitly
            result = self.firecrawl.scrape(
                pdf_url, formats=["markdown"], parsers=["pdf"]
            )

            # Extract markdown content
            markdown_content = getattr(result, "markdown", "")

            if not markdown_content or len(markdown_content) < 50:
                print(" Warning: Extracted content is very short")
                return None

            # Count words for info
            word_count = len(markdown_content.split())

            print(f"‚úÖ Extracted {word_count:,} words")

            return {
                "markdown": markdown_content,
                "word_count": word_count,
                "raw_result": result,
                "method": "firecrawl",
            }

        except Exception as e:
            print(f"‚ùå Error parsing PDF from URL: {e}")
            return None

    def _parse_url_xml(self, url: str) -> Optional[Dict[str, object]]:
        """
        Parse XML from URL.
        
        Args:
            url: URL to XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            import requests
            
            # Fetch XML from URL
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # Parse XML
            root = ET.fromstring(response.content)
            
            # Convert to pretty-printed string
            xml_string = ET.tostring(root, encoding='unicode')
            pretty_xml = minidom.parseString(xml_string).toprettyxml(indent="  ")
            
            # Extract text content
            text_content = self._extract_xml_text(root)
            
            # Combine structure and content
            markdown_text = f"# XML Document Structure\n\n```xml\n{pretty_xml}\n```\n\n# Extracted Text Content\n\n{text_content}"
            
            word_count = len(text_content.split())
            
            return {
                "markdown": markdown_text,
                "word_count": word_count,
                "method": "xml_url_parser"
            }
            
        except Exception as e:
            print(f"‚ùå Error parsing XML URL: {e}")
            return None

    def _extract_xml_text(self, element, level: int = 0) -> str:
        """
        Recursively extract text content from XML elements.
        
        Args:
            element: XML element
            level: Indentation level
            
        Returns:
            Formatted text content
        """
        text_parts = []
        indent = "  " * level
        
        # Add element tag and attributes
        if getattr(element, 'tag', None):
            attrs = " ".join([f"{k}='{v}'" for k, v in getattr(element, 'attrib', {}).items()])
            text_parts.append(f"{indent}<{element.tag}{' ' + attrs if attrs else ''}>")
        
        # Add text content if present
        if getattr(element, 'text', None) and element.text.strip():
            text_parts.append(f"{indent}  {element.text.strip()}")
        
        # Process children
        for child in list(element):
            text_parts.append(self._extract_xml_text(child, level + 1))
        
        return "\n".join(text_parts)

    def parse_document(self, file_path: str, file_type: str = 'pdf') -> Optional[Dict[str, object]]:
        """
        Parse document (PDF or XML) based on type and location.

        Args:
            file_path: Path to local file OR URL to file
            file_type: 'pdf' or 'xml' (default: 'pdf')

        Returns:
            Dict with parsed content or None if failed
        """
        is_url = self._is_url(file_path)

        # Route to appropriate parser
        if file_type == 'pdf':
            if is_url:
                print("üåê Detected PDF URL - using Firecrawl")
                return self._parse_url_pdf(file_path)
            else:
                print("üíæ Detected local PDF - using PyMuPDF")
                return self._parse_local_pdf(file_path)
        elif file_type == 'xml':
            if is_url:
                print("üåê Detected XML URL - using XML parser")
                return self._parse_url_xml(file_path)
            else:
                print("üíæ Detected local XML - using XML parser")
                return self._parse_local_xml(file_path)
        else:
            print(f"‚ùå Unsupported file type: {file_type}")
            return None

    def get_parsed_content(self, parse_result: Dict) -> str:
        """
        Extract markdown text from parse result.
        
        Args:
            parse_result: Result from parse_document()
        
        Returns:
            Markdown text content
        """
        if not parse_result:
            return ""
        return parse_result.get("markdown", "")

------------------------------------------------- ./doc_processor/src/user_prompt.py --------------------------------------------------

"""Terminal user interaction for collecting context."""

from typing import Dict, List


class UserPrompt:
    """Handles interactive terminal questions to collect user context."""
    
    def __init__(self, questions: List[str]):
        """
        Initialize with list of questions.
        
        Args:
            questions: List of question strings
        """
        self.questions = questions
    
    def ask_questions(self) -> Dict[str, str]:
        """
        Present questions to user in terminal and collect answers.
        
        Returns:
            Dict mapping question index to answer
        """
        print("\n" + "="*60)
        print("üìù Additional Context Needed")
        print("="*60)
        print("(Press Enter to skip optional questions)\n")
        
        answers = {}
        total = len(self.questions)
        
        for i, question in enumerate(self.questions, 1):
            # Display question with progress
            print(f"Question {i}/{total}:")
            print(f"  {question}")
            
            # Get user input
            try:
                answer = input("> ").strip()
                
                # Store answer (even if empty)
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": answer if answer else "N/A"
                }
                
                print()  # Blank line for readability
                
            except KeyboardInterrupt:
                print("\n\n‚ö†Ô∏è  Input interrupted. Skipping remaining questions.")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading input: {e}")
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": "N/A"
                }
        
        print("="*60)
        print("‚úÖ Context collection complete\n")
        
        return answers
    
    def format_for_json(self, answers: Dict) -> Dict[str, str]:
        """
        Format answers for JSON output structure.
        
        Args:
            answers: Raw answers dict from ask_questions()
            
        Returns:
            Cleaned dict for user_context section
        """
        formatted = {}
        
        for key, value in answers.items():
            # Create simple key from question
            question_text = value["question"]
            answer_text = value["answer"]
            
            # Use simplified key
            clean_key = question_text.split("?")[0].lower().replace(" ", "_")[:30]
            formatted[clean_key] = answer_text
        
        return formatted

------------------------------------------------- ./doc_processor/src/json_generator.py --------------------------------------------------

"""JSON output file generation."""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional


class JSONGenerator:
    """Creates structured JSON output files."""
    
    def __init__(self, output_dir: str = "outputs/"):
        """
        Initialize generator.
        
        Args:
            output_dir: Directory for output files
        """
        self.output_dir = output_dir
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """Create output directory if it doesn't exist."""
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def create_output(
        self,
        ai_data: Dict,
        user_answers: Dict,
        file_metadata: Dict,
        processing_metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Combine all data into final JSON structure.
        
        Args:
            ai_data: AI analysis results
            user_answers: User context answers
            file_metadata: Source file info
            processing_metadata: Optional processing stats
            
        Returns:
            Complete output dict
        """
        timestamp = datetime.now().isoformat()
        
        output = {
            "document_info": {
                "source_file": file_metadata.get("filename", "unknown"),
                "file_path": file_metadata.get("file_path", ""),
                "processed_at": timestamp,
                "file_size_mb": file_metadata.get("size_mb", 0),
                "word_count": file_metadata.get("word_count", 0)
            },
            "ai_analysis": ai_data,
            "user_context": user_answers,
            "processing_metadata": processing_metadata or {}
        }
        
        return output
    
    def save_to_file(self, data: Dict, original_filename: str) -> str:
        """
        Save JSON data to timestamped file.
        
        Args:
            data: JSON data to save
            original_filename: Original PDF filename
            
        Returns:
            Path to saved file
        """
        try:
            # Generate filename
            base_name = Path(original_filename).stem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"{base_name}_summary_{timestamp}.json"
            output_path = os.path.join(self.output_dir, output_filename)
            
            # Write JSON file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Saved to: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error saving file: {e}")
            return ""
    
    def display_summary(self, data: Dict):
        """
        Display key information in terminal.
        
        Args:
            data: Complete output data
        """
        print("\n" + "="*60)
        print("üìä Document Summary")
        print("="*60)
        
        # Executive summary
        if "ai_analysis" in data:
            summary = data["ai_analysis"].get("executive_summary", "N/A")
            print(f"\nSummary:\n{summary[:200]}...")
            
            # Key topics
            topics = data["ai_analysis"].get("key_topics", [])
            if topics:
                print(f"\nKey Topics: {', '.join(topics[:5])}")
            
            # Complexity
            complexity = data["ai_analysis"].get("complexity_assessment", "N/A")
            print(f"Complexity: {complexity}")
        
        print("\n" + "="*60 + "\n")

------------------------------------------------- ./doc_processor/src/llm_processor.py --------------------------------------------------

"""Generic LLM API integration for document analysis (xAI Grok-compatible)."""

import json
from typing import Dict, Optional
import requests


class LLMProcessor:
    """Processes documents using xAI's Grok API via direct HTTP calls."""

    def __init__(self, api_key: str, model_name: str = "grok-4-fast-reasoning", base_url: str = "https://api.x.ai/v1"):
        """Initialize LLM processor.

        Args:
            api_key: xAI API key
            model_name: Grok model to use
            base_url: xAI API base URL
        """
        if not api_key:
            raise ValueError("LLM API key is required")

        self.api_key = api_key
        self.model = model_name
        self.base_url = base_url.rstrip('/')
    
    def create_analysis_prompt(self, text_content: str, template: Dict) -> str:
        """
        Create prompt for the LLM to analyze the document and fill the template.
        """
        template_str = json.dumps(template, indent=2)
        
        prompt = f"""Analyze the following document and fill the JSON template with accurate information extracted from the text.

INSTRUCTIONS:
- Be concise but thorough
- Extract only information present in the document
- Use "N/A" for fields where information is not available
- For lists, provide the most relevant items
- Estimate complexity as: Simple, Medium, or Complex

DOCUMENT CONTENT:
{text_content[:15000]}  

JSON TEMPLATE TO FILL:
{template_str}

Return ONLY the filled JSON, no other text."""
        
        return prompt
    
    def process_document(self, markdown_text: str, template: Dict) -> Optional[Dict]:
        """
        Send document to the LLM for analysis and return the filled JSON template.
        """
        try:
            print("ü§ñ Processing with LLM (xAI Grok)...")

            # Create prompt
            prompt = self.create_analysis_prompt(markdown_text, template)

            # Build request
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a document analysis AI that extracts structured information and fills JSON templates accurately.",
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ],
                "temperature": 0.3,
                "max_tokens": 4000,
            }

            resp = requests.post(url, headers=headers, json=payload, timeout=60)

            if resp.status_code != 200:
                print(f"‚ùå Grok API error: {resp.status_code} - {resp.text[:500]}")
                return None

            data = resp.json()

            # Extract assistant message content
            try:
                content = data["choices"][0]["message"]["content"]
            except Exception:
                print("‚ùå Unexpected response format from Grok API")
                return None
            
            # Parse JSON response
            try:
                filled_template = json.loads(content)
                print("‚úÖ AI analysis complete")
                return filled_template
            except json.JSONDecodeError:
                print("‚ö†Ô∏è  Warning: Response was not valid JSON, attempting to extract...")
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = content[start:end]
                    filled_template = json.loads(json_str)
                    print("‚úÖ AI analysis complete (extracted JSON)")
                    return filled_template
                else:
                    print("‚ùå Could not parse AI response as JSON")
                    return None
        except Exception as e:
            print(f"‚ùå Error processing with LLM: {e}")
            return None

------------------------------------------------- ./easy_rich/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

------------------------------------------------- ./easy_rich/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    ‚îî‚îÄ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("ü•∑ Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./easy_rich/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
from firecrawl import Firecrawl
from dotenv import load_dotenv
import re
from urllib.parse import urlparse, urljoin

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        DEFAULT_PROXY_MODE, BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    DEFAULT_PROXY_MODE = "auto"
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize Firecrawl client."""
        load_dotenv()
        api_key = os.getenv("FIRECRAWL_API_KEY")
        if not api_key:
            raise ValueError("FIRECRAWL_API_KEY not found in environment variables")
        self.firecrawl = Firecrawl(api_key=api_key)

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_proxy(self, url: str, proxy_mode: str = DEFAULT_PROXY_MODE) -> Optional[Dict[str, str]]:
        """
        Scrape content with specified proxy mode.
        
        Args:
            url: URL to scrape
            proxy_mode: Proxy mode ("auto", "basic", "stealth")
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            result = self.firecrawl.scrape(
                url,
                proxy=proxy_mode,
                formats=[
                    "markdown",
                    {
                        "type": "json",
                        "prompt": "Extract key information from this page including title, main content summary, key points, and any important data like prices, dates, or contact information.",
                    },
                ],
            )

            # Access attributes from Firecrawl Document object safely
            has_metadata = hasattr(result, "metadata") and result.metadata is not None
            title = getattr(result.metadata, "title", "No title found") if has_metadata else "No title found"
            status_code = getattr(result.metadata, "statusCode", "Unknown") if has_metadata else "Unknown"
            markdown_content = getattr(result, "markdown", "")
            structured_data = getattr(result, "json", {})

            return {
                "url": url,
                "title": title,
                "markdown_content": markdown_content,
                "structured_data": structured_data,
                "status_code": str(status_code),
            }

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Firecrawl with bot detection and stealth fallback.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # First attempt with default proxy
        scraped_content = self.scrape_with_proxy(url, DEFAULT_PROXY_MODE)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # Prompt for stealth retry
            if self.prompt_stealth_retry():
                print(STEALTH_TRYING_MSG)
                stealth_content = self.scrape_with_proxy(url, "stealth")
                if stealth_content:
                    print("‚úÖ Success with stealth mode!")
                    return stealth_content
                else:
                    print("‚ùå Stealth mode also failed")
                    return None
            else:
                print("‚è≠Ô∏è  Skipping stealth mode")
                return None
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Save scraped content to both markdown and JSON files.

        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            # Save markdown content
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./easy_rich/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None

------------------------------------------------- ./manual_scrape/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

# Browser Configuration
BROWSER = {
    'default': 'firefox',  # Options: 'chromium', 'firefox'
    'engines': {
        'chromium': {
            'timeout': 30000,
            'stealth_mode': True
        },
        'firefox': {
            'timeout': 30000,
            'stealth_mode': True
        }
    }
}

------------------------------------------------- ./manual_scrape/install_browsers.sh --------------------------------------------------

#!/bin/bash
echo "üöÄ Installing Playwright browsers..."
playwright install
playwright install chromium firefox
echo "‚úÖ Both Chromium and Firefox installed successfully!"

------------------------------------------------- ./manual_scrape/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    ‚îî‚îÄ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("ü•∑ Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./manual_scrape/src/content_extractor.py --------------------------------------------------

"""Extract article content from webpages using Playwright."""

import time
from bs4 import BeautifulSoup
from .browser_utils import handle_any_popups, add_human_pause, extract_readable_text

class ContentExtractor:
    """Extract article content from a webpage."""
    
    def __init__(self):
        pass
    
    def extract_article_content(self, page, url, article, domain, source, user_email=None):
        """Extract article content from the page.
        
        Args:
            page: The Playwright page object
            url: The URL of the article
            article: The article metadata
            domain: The domain of the article
            source: The source of the article
            user_email: Optional user email for validation context
            
        Returns:
            tuple: (success, article_content, title, rejection_reason)
        """
        print("Extracting article content...")
                
        try:
            # Start with aggressive popup handling first
            print("Applying aggressive popup/cookie handling approach...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Perform progressive scrolling for content exposure
            print("Performing progressive scrolling for content exposure...")
            self._perform_progressive_scrolling(page)
            
            # Handle any popups that appeared after scrolling
            print("Handling any new popups after scrolling...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Wait for page to be fully loaded with a timeout
            print("Waiting for page ready state before content extraction...")
            try:
                page.wait_for_load_state("networkidle", timeout=10000)
                print("Page ready state reached")
            except Exception as e:
                print(f"Error waiting for page ready state (continuing anyway): {e}")
            
            # Extract content with BeautifulSoup
            article_content = self._extract_content_with_soup(page)
            
            # Get character count
            chars_count = len(article_content) if article_content else 0
            print(f"Extracted {chars_count} chars with content extraction")
            
            # Include the full content in logs when it's less than 100 characters
            if article_content and chars_count < 100:
                print(f"Content (under 100 chars): {article_content}")
            
            # If we have substantial content (>5500 chars), consider it valid
            if article_content and len(article_content) > 5500:
                title = self._extract_title(page, article)
                print(f"Article has substantial content ({len(article_content)} chars), accepting")
                return True, article_content, title, None
            
            # Continue with validation for articles with less content
            if article_content and len(article_content) > 150:
                title = self._extract_title(page, article)
            
                # Return the content for validation
                if len(article_content) < 1600:
                    print(f"Content too short: {len(article_content)} chars (minimum 1600)")
                    
                    # Try a more aggressive approach as fallback
                    try:
                        print("Attempting more aggressive content extraction as fallback...")
                        
                        fallback_content = page.evaluate("""() => {
                            const bodyText = document.body.innerText;
                            return bodyText || '';
                        }""")
                        
                        if fallback_content and len(fallback_content) > 1600:
                            print(f"Fallback extraction succeeded, got {len(fallback_content)} chars")
                            return True, fallback_content, title, None
                    except Exception as fallback_error:
                        print(f"Fallback extraction failed: {fallback_error}")
                    
                    rejection_reason = f"Content too short: {len(article_content)} chars (minimum 1600)"
                    return False, "", title, rejection_reason
                
                return True, article_content, title, None
            else:
                rejection_reason = "Content too short: 0 chars"
                print("Content extraction failed: Empty or very short content")
                return False, "", article.get("title", ""), rejection_reason
            
        except Exception as e:
            print(f"Error extracting content: {e}")
            import traceback
            print(f"Extraction error traceback: {traceback.format_exc()}")
            rejection_reason = f"Error: {str(e)}"
            return False, "", article.get("title", ""), rejection_reason

    def _perform_progressive_scrolling(self, page):
        """Perform progressive scrolling to expose all content."""
        try:
            page_height = page.evaluate("document.body.scrollHeight")
            view_height = page.evaluate("window.innerHeight")
            
            scroll_steps = min(4, max(2, int(page_height / view_height)))
            print(f"Performing {scroll_steps} scroll steps to load all content...")
            
            for i in range(1, scroll_steps + 1):
                scroll_position = (i * page_height) / scroll_steps
                page.evaluate(f"window.scrollTo(0, {scroll_position})")
                page.wait_for_timeout(1000)
            
            # Go back to top before extraction
            page.evaluate("window.scrollTo(0, 0)")
            page.wait_for_timeout(1000)
            
        except Exception as e:
            print(f"Error during progressive scrolling: {e}")

    def _extract_content_with_soup(self, page):
        """Extract content using BeautifulSoup parsing."""
        try:
            # Get page HTML and parse with BeautifulSoup
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract readable text
            return extract_readable_text(soup)
            
        except Exception as e:
            print(f"Error during content extraction: {e}")
            return ""

    def _extract_title(self, page, article):
        """Extract title from the page."""
        try:
            # Try to get title from page
            page_title = page.title()
            if page_title and len(page_title.strip()) > 0:
                return page_title.strip()
                
            # Fallback to original title
            return article.get("title", "")
            
        except Exception as e:
            print(f"Error extracting title: {e}")
            return article.get("title", "")

------------------------------------------------- ./manual_scrape/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
import re
from urllib.parse import urlparse, urljoin
from playwright.sync_api import sync_playwright

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "üí∞ Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "‚ùå Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "ü§î Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "ü•∑ Trying stealth mode..."

from .browsers.browser_factory import BrowserFactory
from .content_extractor import ContentExtractor


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize web scraper with Playwright."""
        self.content_extractor = ContentExtractor()

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_playwright(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content with Playwright browser.
        
        Args:
            url: URL to scrape
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            with sync_playwright() as p:
                # Create browser instance
                browser_config = BrowserFactory.create()
                browser = browser_config.launch(p, headless=True)
                
                # Create context with stealth settings
                user_agents = browser_config.get_user_agents()
                import random
                context = browser.new_context(
                    user_agent=random.choice(user_agents),
                    viewport={"width": 1280, "height": 1200}
                )
                page = context.new_page()
                
                # Set timeout
                page.set_default_navigation_timeout(60000)
                
                try:
                    # Navigate to URL
                    response = page.goto(url, wait_until="domcontentloaded")
                    
                    if response and response.status >= 400:
                        print(f"HTTP error {response.status} while accessing content")
                        return None
                    
                    # Extract domain and create article dict
                    domain = urlparse(url).netloc
                    article = {"title": f"Article from {domain}"}
                    
                    # Extract content
                    success, article_content, title, rejection_reason = self.content_extractor.extract_article_content(
                        page, url, article, domain, domain
                    )
                    
                    if not success:
                        print(f"Content extraction failed: {rejection_reason}")
                        return None
                    
                    return {
                        "url": url,
                        "title": title,
                        "markdown_content": article_content,
                        "structured_data": {},
                        "status_code": str(response.status if response else 200),
                    }
                    
                finally:
                    page.close()
                    context.close()
                    browser.close()

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Playwright with bot detection.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # Scrape with Playwright
        scraped_content = self.scrape_with_playwright(url)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # For now, just return None - stealth retry could be implemented later
            print("‚≠ê Bot detection encountered")
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./manual_scrape/src/browser_utils.py --------------------------------------------------

"""Browser utilities for popup handling, data extraction, and stealth operations."""

import time
import random
from bs4 import BeautifulSoup

def get_human_delay(min_ms=800, max_ms=2500):
    """Get a random delay that mimics human behavior."""
    return random.randint(min_ms, max_ms)

def add_human_pause(page, min_ms=1000, max_ms=3000):
    """Add a human-like pause with mouse movement."""
    try:
        viewport = page.viewport_size
        if viewport:
            x = random.randint(100, viewport['width'] - 100)
            y = random.randint(100, viewport['height'] - 100)
            page.mouse.move(x, y)
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))
    except Exception:
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))

def handle_cookies_and_consent(page, timeout=15000):
    """Handle cookies and consent dialogs with Playwright selectors."""
    print("Handling cookies and consent with Playwright selectors...")
    
    start_time = time.time()
    max_time_ms = timeout
    
    try:
        if (time.time() - start_time) * 1000 > max_time_ms:
            print(f"Cookie handling timed out after {timeout}ms")
            return False
        
        # Priority 1: Explicit accept buttons
        accept_buttons = [
            "button:has-text('I accept all cookies')",
            "button:has-text('Accept all cookies')", 
            "button:has-text('Accept All')",
            "button:has-text('Accept')",
            "button:has-text('Accetta tutto')",  # Italian
            "button:has-text('Accetto')",       # Italian
        ]
        
        # Priority 2: Close/dismiss buttons  
        close_buttons = [
            "button:has-text('Close')",
            "button:has-text('Chiudi')",        # Italian
            "button:has-text('√ó')",
            "button[aria-label*='close']",
            "button[class*='close']",
        ]
        
        # Priority 3: Generic attribute-based selectors
        generic_buttons = [
            "[id*='accept'] button:not([href]):not(a)",
            "[class*='accept'] button:not([href]):not(a)", 
            "[id*='cookie'] button:not([href]):not(a)",
            "[class*='cookie'] button:not([href]):not(a)",
            "[data-testid*='accept'] button:not([href]):not(a)",
        ]
        
        # Combine all selectors in priority order
        button_selectors = accept_buttons + close_buttons + generic_buttons
        
        clicked = False
        
        # Try each selector with timeout check
        for selector in button_selectors:
            if (time.time() - start_time) * 1000 > max_time_ms:
                print(f"Cookie handling timed out during selector iteration")
                return False
            
            try:
                if page.query_selector(selector):
                    print(f"Found popup button with selector: {selector}")
                    page.click(selector, timeout=2000)
                    print(f"Successfully clicked popup button with selector: {selector}")
                    clicked = True
                    break
            except Exception:
                continue
        
        if clicked:
            print("Cookie/consent button clicked, waiting for page to stabilize...")
            remaining_time = max(0, max_time_ms - (time.time() - start_time) * 1000)
            if remaining_time > 1000:
                page.wait_for_timeout(min(2000, int(remaining_time)))
            return True
        
        return False
        
    except Exception as e:
        print(f"Error handling cookies and consent: {e}")
        return False

def handle_any_popups(page, aggressive=False):
    """Handle any type of popup, consent dialog, or cookie banner."""
    print(f"Trying {'aggressive ' if aggressive else ''}popup handling...")
    
    # Step 1: Try button clicking with human delays
    add_human_pause(page, 500, 1200)
    
    button_clicked = handle_cookies_and_consent(page, timeout=8000 if aggressive else 5000)
    
    if button_clicked:
        print("‚úÖ Successfully handled popup with button clicking")
        add_human_pause(page, 1000, 2000)
        return True
    
    # Step 2: DOM removal fallback if aggressive
    if aggressive:
        print("Using DOM removal fallback...")
        add_human_pause(page, 1000, 2000)
        
        try:
            removed_count = page.evaluate("""() => {
                const removeElements = (selector) => {
                    const elements = document.querySelectorAll(selector);
                    let removed = 0;
                    elements.forEach(el => {
                        el.remove();
                        removed++;
                    });
                    return removed;
                };
                
                let removedCount = 0;
                
                // Remove elements with high z-index that could be overlays
                document.querySelectorAll('*').forEach(el => {
                    const style = window.getComputedStyle(el);
                    const zIndex = parseInt(style.zIndex);
                    if (zIndex > 999) {
                        const position = style.position;
                        if (position === 'fixed' || position === 'absolute') {
                            el.remove();
                            removedCount++;
                        }
                    }
                });
                
                // Remove common cookie/consent banners
                removedCount += removeElements('[class*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[class*="consent"]:not(html):not(body)');
                removedCount += removeElements('[class*="popup"]:not(html):not(body)');
                removedCount += removeElements('[class*="banner"]:not(html):not(body)');
                removedCount += removeElements('[class*="overlay"]:not(html):not(body)');
                removedCount += removeElements('[class*="modal"]:not(html):not(body)');
                removedCount += removeElements('[id*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[id*="consent"]:not(html):not(body)');
                
                document.body.style.overflow = 'auto';
                document.documentElement.style.overflow = 'auto';
                
                return removedCount;
            }""")
            
            if removed_count > 0:
                print("DOM removal: removed ${removed_count} elements")
                add_human_pause(page, 1500, 3000)
                return True
        except Exception as e:
            print(f"DOM removal failed: {e}")
    
    return False

def extract_readable_text(soup):
    """Extract readable text from BeautifulSoup object."""
    # Remove unwanted elements
    for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
        element.decompose()
    
    # Try to find main content areas
    content_selectors = [
        'article', 
        '[role="main"]', 
        '.content', 
        '#content',
        '.post-content',
        '.article-body',
        '.entry-content'
    ]
    
    for selector in content_selectors:
        content = soup.select_one(selector)
        if content:
            text = content.get_text(separator=' ', strip=True)
            if len(text) > 500:  # Only return if substantial content
                return text
    
    # Fallback to body text
    return soup.get_text(separator=' ', strip=True)

------------------------------------------------- ./manual_scrape/src/browsers/browser_firefox.py --------------------------------------------------

"""Firefox browser implementation."""

from typing import List

class FirefoxBrowser:
    """Firefox browser configuration and launch handler."""

    name = "firefox"

    def get_launch_args(self) -> List[str]:
        """Get Firefox-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--no-first-run',
            '--disable-default-browser-check',
            '--disable-infobars'
        ]

    def launch(self, playwright_instance, headless: bool = False):
        """Launch Firefox browser with stealth configuration."""
        print(f"üöÄ Launching Firefox browser (headless: {headless})")
        print(f"üîß Firefox launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.firefox.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"‚úÖ Firefox browser launched successfully")
            return browser
        except Exception as e:
            print(f"‚ùå Failed to launch Firefox browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Firefox-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/browsers/__init__.py --------------------------------------------------

"""Browser factory and configuration modules."""

------------------------------------------------- ./manual_scrape/src/browsers/browser_factory.py --------------------------------------------------

"""Factory class for creating browser instances."""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import BROWSER

from .browser_chromium import ChromiumBrowser
from .browser_firefox import FirefoxBrowser

class BrowserFactory:
    """Factory class for creating browser instances."""

    _browsers = {
        'chromium': ChromiumBrowser,
        'firefox': FirefoxBrowser
    }

    @classmethod
    def create(cls, browser_name=None):
        """Create a browser instance based on configuration or specified name."""
        if browser_name is None:
            browser_name = BROWSER.get('default', 'firefox')
            print(f"üåê Browser auto-selected from config: {browser_name}")
        else:
            print(f"üåê Browser explicitly requested: {browser_name}")

        browser_class = cls._browsers.get(browser_name)
        if not browser_class:
            raise ValueError(f"Unsupported browser: {browser_name}. Available: {list(cls._browsers.keys())}")

        browser_instance = browser_class()
        print(f"‚úÖ Browser instance created: {browser_instance.name}")
        return browser_instance

    @classmethod
    def get_available_browsers(cls):
        """Get list of available browser names."""
        return list(cls._browsers.keys())
    
    @classmethod
    def verify_browser_config(cls):
        """Verify current browser configuration and return details."""
        try:
            config_browser = BROWSER.get('default', 'firefox')
            print(f"üìã Config browser setting: {config_browser}")
            
            browser_instance = cls.create()
            print(f"üìã Actual browser created: {browser_instance.name}")
            
            return {
                'config_browser': config_browser,
                'actual_browser': browser_instance.name,
                'available_browsers': cls.get_available_browsers()
            }
        except Exception as e:
            print(f"‚ùå Browser config verification failed: {e}")
            return None

------------------------------------------------- ./manual_scrape/src/browsers/browser_chromium.py --------------------------------------------------

"""Chromium browser implementation."""

from typing import List

class ChromiumBrowser:
    """Chromium browser configuration and launch handler."""

    name = "chromium"

    def get_launch_args(self) -> List[str]:
        """Get Chromium-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-dev-shm-usage',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--disable-features=TranslateUI',
            '--disable-ipc-flooding-protection',
            '--no-first-run',
            '--force-device-scale-factor=1',
            '--disable-default-apps'
        ]

    def launch(self, playwright_instance, headless: bool = True):
        """Launch Chromium browser with stealth configuration."""
        print(f"üöÄ Launching Chromium browser (headless: {headless})")
        print(f"üîß Chromium launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.chromium.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"‚úÖ Chromium browser launched successfully")
            return browser
        except Exception as e:
            print(f"‚ùå Failed to launch Chromium browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Chromium-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None
