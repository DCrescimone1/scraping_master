Folder structure:
.
├── BMEcat_transformer
│   ├── config.py
│   ├── example_supplier_ids.json
│   ├── extract_features.py
│   ├── main.py
│   ├── README.md
│   └── src
│       ├── __init__.py
│       ├── dabag_scraper.py
│       ├── feature_extractor.py
│       ├── input_handler.py
│       ├── master_json_manager.py
│       ├── output_formatter.py
│       ├── table_extractor.py
│       ├── user_prompt.py
│       └── xml_reader.py
├── dabag_ch_20251016_161607
│   ├── www_dabag_ch__data.json
│   └── www_dabag_ch_.md
├── doc_processor
│   ├── config.py
│   ├── main.py
│   ├── outputs
│   ├── README.md
│   ├── requirements.txt
│   └── src
│       ├── __init__.py
│       ├── bmecat_parser.py
│       ├── firecrawl_parser.py
│       ├── json_generator.py
│       ├── llm_processor.py
│       ├── pdf_handler.py
│       └── user_prompt.py
├── easy_rich
│   ├── config.py
│   ├── main.py
│   ├── README.md
│   └── src
│       ├── __init__.py
│       ├── serp_api_client.py
│       └── web_scraper.py
├── manual_scrape
│   ├── config.py
│   ├── install_browsers.sh
│   ├── main.py
│   ├── README.md
│   └── src
│       ├── __init__.py
│       ├── browser_utils.py
│       ├── browsers
│       │   ├── __init__.py
│       │   ├── browser_chromium.py
│       │   ├── browser_factory.py
│       │   └── browser_firefox.py
│       ├── content_extractor.py
│       ├── serp_api_client.py
│       └── web_scraper.py
├── outputs
│   ├── bmecat_dabag_results_20251016_104334.json
│   ├── bmecat_dabag_results_20251016_105350.json
│   ├── bmecat_dabag_results_20251016_113936.json
│   ├── bmecat_dabag_results_20251016_114927.json
│   ├── master_bmecat_dabag.json
│   ├── master_bmecat_dabag.json.backup1
│   ├── Stihl_Version DABAG_summary_20251010_183123.json
│   ├── Stihl_Version DABAG_summary_20251013_100932.json
│   └── unique_features.csv
├── project_dump.sh
├── requirements.txt
├── scraping_master_dump.txt
└── store_steampowered_com_spider_man_20250926_171735
    ├── spider_man_results_data.json
    └── spider_man_results.md

14 directories, 60 files


--- File Contents ---


------------------------------------------------- ./doc_processor/config.py --------------------------------------------------

"""
Configuration settings for Document Processor.
Follows same pattern as easy_rich and manual_scrape configs.
"""

import os
from dotenv import load_dotenv

# Load environment variables from root .env
load_dotenv()

# API Configuration
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")  # Used for URL-based PDF parsing
GROK_API_KEY = os.getenv("GROK_API_KEY")  # xAI API key
GROK_MODEL = os.getenv("GROK_MODEL", "grok-beta")  # Default model

# Output Settings
OUTPUT_DIR = "outputs/"
FILENAME_PATTERN = "{original_name}_summary_{timestamp}.json"

# JSON Template - AI fills these fields
DOCUMENT_TEMPLATE = {
    "executive_summary": "",
    "document_type": "",
    "key_topics": [],
    "technical_details": {
        "technologies_mentioned": [],
        "requirements": [],
        "constraints": []
    },
    "entities": {
        "people": [],
        "organizations": [],
        "products": [],
        "dates": []
    },
    "action_items": [],
    "decisions_made": [],
    "open_questions": [],
    "complexity_assessment": "",
    "estimated_read_time_minutes": 0,
    "critical_sections": []
}

# User Questions - asked in terminal after AI processing
USER_QUESTIONS = [
    "What is the primary purpose of this document?",
    "Who is the intended audience? (e.g., developers, executives)",
    "Project or client name (if applicable):",
    "Any critical deadlines or milestones mentioned?",
    "Specific areas you want highlighted or tracked:",
    "Additional context or notes:"
]

# Validation
if not FIRECRAWL_API_KEY:
    raise ValueError("FIRECRAWL_API_KEY not found in environment variables")

if not GROK_API_KEY:
    raise ValueError("GROK_API_KEY not found in environment variables")


------------------------------------------------- ./doc_processor/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Document Processor - PDF & XML Analysis Tool
Parses PDFs/XMLs, analyzes with AI, collects user context, outputs structured JSON.
"""

import sys
import time
from pathlib import Path

# Import configuration
import config

# Import modules
from src.pdf_handler import PDFHandler
from src.firecrawl_parser import FirecrawlParser
from src.llm_processor import LLMProcessor
from src.user_prompt import UserPrompt
from src.json_generator import JSONGenerator


def get_file_path() -> str:
    """Get file path (PDF or XML) from command line or user input."""
    if len(sys.argv) > 1:
        return sys.argv[1]
    else:
        print("📄 Document Processor (PDF & XML)")
        print("="*60)
        file_path = input("Enter path to PDF or XML file: ").strip()
        return file_path


def main():
    """Main orchestration function."""
    start_time = time.time()
    
    try:
        # Get file path
        file_path = get_file_path()
        
        # PHASE 1: Validate file
        print("\n🔍 Validating file...")
        pdf_handler = PDFHandler()
        
        if not pdf_handler.validate_file(file_path):
            print("❌ Validation failed. Exiting.")
            return
        
        file_info = pdf_handler.get_file_info(file_path)
        file_type = file_info.get('file_type', 'pdf')
        print(f"✅ Valid {file_type.upper()}: {file_info['filename']} ({file_info['size_mb']} MB)")
        
        # PHASE 2: Parse document (auto-detects URL vs local, PDF vs XML)
        print(f"\n📄 Extracting content from {file_type.upper()}...")
        parser = FirecrawlParser(config.FIRECRAWL_API_KEY)
        
        parse_result = parser.parse_document(file_path, file_type)
        if not parse_result:
            print(f"❌ Failed to extract content from {file_type.upper()}. Exiting.")
            return
        
        markdown_text = parser.get_parsed_content(parse_result)
        file_info["word_count"] = parse_result.get("word_count", 0)
        print(f"✅ Extracted {file_info['word_count']} words")
        
        # PHASE 3: Process with Grok AI
        print("\n🤖 Analyzing with LLM (xAI Grok by default)...")
        grok = LLMProcessor(config.GROK_API_KEY, config.GROK_MODEL, base_url="https://api.x.ai/v1")
        
        ai_analysis = grok.process_document(markdown_text, config.DOCUMENT_TEMPLATE)
        if not ai_analysis:
            print("❌ AI processing failed. Exiting.")
            return
        
        # PHASE 4: Collect user context
        print("\n📝 Collecting additional context...")
        user_prompt = UserPrompt(config.USER_QUESTIONS)
        
        raw_answers = user_prompt.ask_questions()
        user_context = user_prompt.format_for_json(raw_answers)
        
        # PHASE 5: Generate output
        print("\n💾 Generating JSON output...")
        
        # Calculate processing time
        processing_time = round(time.time() - start_time, 2)
        
        processing_meta = {
            "grok_model": config.GROK_MODEL,
            "parsing_method": parse_result.get("method", "unknown"),
            "file_type": file_type,
            "total_processing_time_seconds": processing_time
        }
        
        generator = JSONGenerator(config.OUTPUT_DIR)
        
        final_output = generator.create_output(
            ai_data=ai_analysis,
            user_answers=user_context,
            file_metadata=file_info,
            processing_metadata=processing_meta
        )
        
        output_path = generator.save_to_file(final_output, file_info["filename"])
        
        if output_path:
            # Display summary
            generator.display_summary(final_output)
            print(f"✅ Processing complete! Saved to: {output_path}")
            print(f"⏱️  Total time: {processing_time}s")
        else:
            print("❌ Failed to save output file.")
        
    except KeyboardInterrupt:
        print("\n\n⚠️  Process interrupted by user. Exiting.")
    except Exception as e:
        print(f"\n❌ Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


------------------------------------------------- ./doc_processor/src/pdf_handler.py --------------------------------------------------

"""PDF and XML file validation and handling."""

import os
from pathlib import Path
from urllib.parse import urlparse
from typing import Dict


class PDFHandler:
    """Validates and prepares PDF and XML files for processing."""
    
    def __init__(self):
        """Initialize file handler for PDF and XML files."""
        pass
    
    # Supported file extensions
    SUPPORTED_EXTENSIONS = {'.pdf', '.xml'}
    
    def validate_file(self, file_path: str) -> bool:
        """
        Validate that input is either a valid local PDF/XML file or a PDF/XML URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
        """
        try:
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Basic sanity: must have netloc and path ending with supported extension
                if not parsed.netloc:
                    print(f"❌ Invalid URL: {file_path}")
                    return False
                path_lower = parsed.path.lower()
                if not any(path_lower.endswith(ext) for ext in self.SUPPORTED_EXTENSIONS):
                    print(f"❌ URL does not point to a supported file (PDF or XML): {file_path}")
                    return False
                return True

            # Local file case
            path = Path(file_path)

            # Check exists
            if not path.exists():
                print(f"❌ File not found: {file_path}")
                return False

            # Check is file (not directory)
            if not path.is_file():
                print(f"❌ Not a file: {file_path}")
                return False

            # Check supported extension
            if path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
                print(f"❌ Not a supported file type (PDF or XML): {file_path}")
                return False

            # Check readable
            if not os.access(file_path, os.R_OK):
                print(f"❌ File not readable: {file_path}")
                return False

            return True

        except Exception as e:
            print(f"❌ Error validating file: {e}")
            return False

    def get_file_type(self, file_path: str) -> str:
        """
        Determine file type (pdf or xml).
        
        Args:
            file_path: Path to file or URL
            
        Returns:
            'pdf' or 'xml' or 'unknown'
        """
        try:
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                path_lower = parsed.path.lower()
            else:
                path_lower = str(file_path).lower()
            
            if path_lower.endswith('.pdf'):
                return 'pdf'
            elif path_lower.endswith('.xml'):
                return 'xml'
            else:
                return 'unknown'
        except Exception:
            return 'unknown'
    
    def get_file_info(self, file_path: str) -> Dict[str, object]:
        """
        Extract file metadata for local PDF/XML or URL.
        
        Args:
            file_path: Path to PDF or XML file
            
        Returns:
            Dict with file info
        """
        try:
            file_type = self.get_file_type(file_path)
            # URL case
            if isinstance(file_path, str) and (file_path.startswith("http://") or file_path.startswith("https://")):
                parsed = urlparse(file_path)
                # Derive filename from URL path
                name = Path(parsed.path).name or f"document.{file_type}"
                return {
                    "filename": name,
                    "file_path": file_path,
                    "size_mb": 0,
                    "file_type": file_type
                }
            
            # Local file case
            path = Path(file_path)
            size_mb = path.stat().st_size / (1024 * 1024)
            
            return {
                "filename": path.name,
                "file_path": str(path.absolute()),
                "size_mb": round(size_mb, 2),
                "file_type": file_type
            }
        except Exception as e:
            print(f"⚠️  Error getting file info: {e}")
            return {
                "filename": "unknown",
                "file_path": file_path,
                "size_mb": 0,
                "file_type": "unknown"
            }

------------------------------------------------- ./doc_processor/src/firecrawl_parser.py --------------------------------------------------

"""Firecrawl PDF/XML parsing integration."""

from typing import Optional, Dict
from firecrawl import Firecrawl
import fitz  # PyMuPDF
import xml.etree.ElementTree as ET
import xml.dom.minidom as minidom
from .bmecat_parser import BMEcatParser

class FirecrawlParser:
    """Handles PDF and XML to text conversion using Firecrawl and local parsers."""

    def __init__(self, api_key: str):
        """
        Initialize Firecrawl client.

        Args:
            api_key: Firecrawl API key
        """
        if not api_key:
            raise ValueError("Firecrawl API key is required")

        self.firecrawl = Firecrawl(api_key=api_key)

    def _is_url(self, path: str) -> bool:
        """
        Check if input is a URL or local file path.

        Args:
            path: Input path or URL

        Returns:
            True if URL, False if local path
        """
        return path.startswith("http://") or path.startswith("https://")

    def _parse_local_pdf(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local PDF file using PyMuPDF.

        Args:
            file_path: Path to local PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("📄 Parsing local PDF with PyMuPDF...")

            # Open PDF
            doc = fitz.open(file_path)

            # Extract text from all pages
            markdown_content = ""
            for page_num, page in enumerate(doc, start=1):
                text = page.get_text()
                markdown_content += f"\n\n## Page {page_num}\n\n{text}"

            page_count = len(doc)
            doc.close()

            if not markdown_content or len(markdown_content) < 50:
                print("⚠️  Warning: Extracted content is very short")
                return None

            # Count words
            word_count = len(markdown_content.split())

            print(f"✅ Extracted {word_count:,} words from {page_count} pages")

            return {
                "markdown": markdown_content.strip(),
                "word_count": word_count,
                "page_count": page_count,
                "method": "pymupdf",
            }

        except Exception as e:
            print(f"❌ Error parsing local PDF: {e}")
            return None

    def _parse_local_xml(self, file_path: str) -> Optional[Dict[str, object]]:
        """
        Parse local XML file using enhanced BMEcat parser.

        Args:
            file_path: Path to local XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("📄 Parsing BMEcat XML with enhanced parser...")
            
            # Use BMEcat-specific parser
            bmecat_parser = BMEcatParser()
            parsed_data = bmecat_parser.parse_bmecat_file(file_path)
            
            if not parsed_data:
                print("⚠️  Failed to parse BMEcat XML")
                return None
            
            # Convert to markdown for AI processing
            markdown_content = bmecat_parser.to_markdown(parsed_data)
            word_count = len(markdown_content.split())
            
            print(f"✅ Extracted {word_count:,} words")
            print(f"   - {parsed_data['metadata']['total_catalog_structures']} catalog structures")
            print(f"   - {parsed_data['metadata']['root_categories']} root categories")
            
            return {
                "markdown": markdown_content,
                "word_count": word_count,
                "method": "bmecat_parser"
            }
        
        except Exception as e:
            print(f"❌ Error parsing BMEcat XML: {e}")
            return None

    def _parse_url_pdf(self, pdf_url: str) -> Optional[Dict[str, object]]:
        """
        Parse PDF from URL using Firecrawl.

        Args:
            pdf_url: URL to PDF file

        Returns:
            Dict with parsed content or None if failed
        """
        try:
            print("📄 Parsing PDF from URL with Firecrawl...")

            # Use Firecrawl with PDF parser explicitly
            result = self.firecrawl.scrape(
                pdf_url, formats=["markdown"], parsers=["pdf"]
            )

            # Extract markdown content
            markdown_content = getattr(result, "markdown", "")

            if not markdown_content or len(markdown_content) < 50:
                print(" Warning: Extracted content is very short")
                return None

            # Count words for info
            word_count = len(markdown_content.split())

            print(f"✅ Extracted {word_count:,} words")

            return {
                "markdown": markdown_content,
                "word_count": word_count,
                "raw_result": result,
                "method": "firecrawl",
            }

        except Exception as e:
            print(f"❌ Error parsing PDF from URL: {e}")
            return None

    def _parse_url_xml(self, url: str) -> Optional[Dict[str, object]]:
        """
        Parse XML from URL.
        
        Args:
            url: URL to XML file
            
        Returns:
            Dict with parsed content or None if failed
        """
        try:
            import requests
            
            # Fetch XML from URL
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # Parse XML
            root = ET.fromstring(response.content)
            
            # Convert to pretty-printed string
            xml_string = ET.tostring(root, encoding='unicode')
            pretty_xml = minidom.parseString(xml_string).toprettyxml(indent="  ")
            
            # Combine structure content (pretty XML only)
            markdown_text = f"# XML Document Structure\n\n```xml\n{pretty_xml}\n```"
            
            word_count = len(pretty_xml.split())
            
            return {
                "markdown": markdown_text,
                "word_count": word_count,
                "method": "xml_url_parser"
            }
            
        except Exception as e:
            print(f"❌ Error parsing XML URL: {e}")
            return None



    def parse_document(self, file_path: str, file_type: str = 'pdf') -> Optional[Dict[str, object]]:
        """
        Parse document (PDF or XML) based on type and location.

        Args:
            file_path: Path to local file OR URL to file
            file_type: 'pdf' or 'xml' (default: 'pdf')

        Returns:
            Dict with parsed content or None if failed
        """
        is_url = self._is_url(file_path)

        # Route to appropriate parser
        if file_type == 'pdf':
            if is_url:
                print("🌐 Detected PDF URL - using Firecrawl")
                return self._parse_url_pdf(file_path)
            else:
                print("💾 Detected local PDF - using PyMuPDF")
                return self._parse_local_pdf(file_path)
        elif file_type == 'xml':
            if is_url:
                print("🌐 Detected XML URL - using XML parser")
                return self._parse_url_xml(file_path)
            else:
                print("💾 Detected local XML - using XML parser")
                return self._parse_local_xml(file_path)
        else:
            print(f"❌ Unsupported file type: {file_type}")
            return None

    def get_parsed_content(self, parse_result: Dict) -> str:
        """
        Extract markdown text from parse result.
        
        Args:
            parse_result: Result from parse_document()
        
        Returns:
            Markdown text content
        """
        if not parse_result:
            return ""
        return parse_result.get("markdown", "")

------------------------------------------------- ./doc_processor/src/user_prompt.py --------------------------------------------------

"""Terminal user interaction for collecting context."""

from typing import Dict, List


class UserPrompt:
    """Handles interactive terminal questions to collect user context."""
    
    def __init__(self, questions: List[str]):
        """
        Initialize with list of questions.
        
        Args:
            questions: List of question strings
        """
        self.questions = questions
    
    def ask_questions(self) -> Dict[str, str]:
        """
        Present questions to user in terminal and collect answers.
        
        Returns:
            Dict mapping question index to answer
        """
        print("\n" + "="*60)
        print("📝 Additional Context Needed")
        print("="*60)
        print("(Press Enter to skip optional questions)\n")
        
        answers = {}
        total = len(self.questions)
        
        for i, question in enumerate(self.questions, 1):
            # Display question with progress
            print(f"Question {i}/{total}:")
            print(f"  {question}")
            
            # Get user input
            try:
                answer = input("> ").strip()
                
                # Store answer (even if empty)
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": answer if answer else "N/A"
                }
                
                print()  # Blank line for readability
                
            except KeyboardInterrupt:
                print("\n\n⚠️  Input interrupted. Skipping remaining questions.")
                break
            except Exception as e:
                print(f"⚠️  Error reading input: {e}")
                answers[f"question_{i}"] = {
                    "question": question,
                    "answer": "N/A"
                }
        
        print("="*60)
        print("✅ Context collection complete\n")
        
        return answers
    
    def format_for_json(self, answers: Dict) -> Dict[str, str]:
        """
        Format answers for JSON output structure.
        
        Args:
            answers: Raw answers dict from ask_questions()
            
        Returns:
            Cleaned dict for user_context section
        """
        formatted = {}
        
        for key, value in answers.items():
            # Create simple key from question
            question_text = value["question"]
            answer_text = value["answer"]
            
            # Use simplified key
            clean_key = question_text.split("?")[0].lower().replace(" ", "_")[:30]
            formatted[clean_key] = answer_text
        
        return formatted

------------------------------------------------- ./doc_processor/src/json_generator.py --------------------------------------------------

"""JSON output file generation."""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional


class JSONGenerator:
    """Creates structured JSON output files."""
    
    def __init__(self, output_dir: str = "outputs/"):
        """
        Initialize generator.
        
        Args:
            output_dir: Directory for output files
        """
        self.output_dir = output_dir
        self._ensure_output_dir()
    
    def _ensure_output_dir(self):
        """Create output directory if it doesn't exist."""
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def create_output(
        self,
        ai_data: Dict,
        user_answers: Dict,
        file_metadata: Dict,
        processing_metadata: Optional[Dict] = None
    ) -> Dict:
        """
        Combine all data into final JSON structure.
        
        Args:
            ai_data: AI analysis results
            user_answers: User context answers
            file_metadata: Source file info
            processing_metadata: Optional processing stats
            
        Returns:
            Complete output dict
        """
        timestamp = datetime.now().isoformat()
        
        output = {
            "document_info": {
                "source_file": file_metadata.get("filename", "unknown"),
                "file_path": file_metadata.get("file_path", ""),
                "processed_at": timestamp,
                "file_size_mb": file_metadata.get("size_mb", 0),
                "word_count": file_metadata.get("word_count", 0)
            },
            "ai_analysis": ai_data,
            "user_context": user_answers,
            "processing_metadata": processing_metadata or {}
        }
        
        return output
    
    def save_to_file(self, data: Dict, original_filename: str) -> str:
        """
        Save JSON data to timestamped file.
        
        Args:
            data: JSON data to save
            original_filename: Original PDF filename
            
        Returns:
            Path to saved file
        """
        try:
            # Generate filename
            base_name = Path(original_filename).stem
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"{base_name}_summary_{timestamp}.json"
            output_path = os.path.join(self.output_dir, output_filename)
            
            # Write JSON file
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"💾 Saved to: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"❌ Error saving file: {e}")
            return ""
    
    def display_summary(self, data: Dict):
        """
        Display key information in terminal.
        
        Args:
            data: Complete output data
        """
        print("\n" + "="*60)
        print("📊 Document Summary")
        print("="*60)
        
        # Executive summary
        if "ai_analysis" in data:
            summary = data["ai_analysis"].get("executive_summary", "N/A")
            print(f"\nSummary:\n{summary[:200]}...")
            
            # Key topics
            topics = data["ai_analysis"].get("key_topics", [])
            if topics:
                print(f"\nKey Topics: {', '.join(topics[:5])}")
            
            # Complexity
            complexity = data["ai_analysis"].get("complexity_assessment", "N/A")
            print(f"Complexity: {complexity}")
        
        print("\n" + "="*60 + "\n")

------------------------------------------------- ./doc_processor/src/bmecat_parser.py --------------------------------------------------

"""
BMEcat XML Parser - Specialized parser for BMEcat catalog format.
Captures all attributes, relationships, and multi-language content.
"""

import xml.etree.ElementTree as ET
import codecs
from typing import Dict, List, Optional, Any


class BMEcatParser:
    """Enhanced parser specifically designed for BMEcat format."""
    
    def __init__(self):
        """Initialize the BMEcat parser."""
        self.namespaces = {
            'bme': 'http://www.bmecat.org/bmecat/2005fd'
        }
    
    def parse_bmecat_file(self, file_path: str) -> Dict[str, Any]:
        """
        Parse BMEcat XML file and extract all structured data.
        
        Args:
            file_path: Path to BMEcat XML file
            
        Returns:
            Dictionary with complete structured data or None if failed
        """
        try:
            # Read and sanitize XML to handle BOM and leading whitespace before declaration
            with open(file_path, 'rb') as f:
                raw = f.read()

            # Remove UTF-8 BOM if present
            if raw.startswith(codecs.BOM_UTF8):
                raw = raw[len(codecs.BOM_UTF8):]

            # Decode and strip leading whitespace/newlines before the first '<'
            text = raw.decode('utf-8', errors='replace')
            text = text.lstrip()  # remove leading spaces/newlines/tabs

            # Parse from string (handles cases where declaration isn't at pos 0 originally)
            root = ET.fromstring(text)
            
            # Handle namespace
            if root.tag.startswith('{'):
                ns = root.tag.split('}')[0] + '}'
            else:
                ns = ''
            
            result = {
                'header': self._parse_header(root, ns),
                'catalog_groups': self._parse_catalog_groups(root, ns),
                'metadata': {
                    'total_catalog_structures': 0,
                    'root_categories': 0,
                    'node_categories': 0,
                    'leaf_categories': 0
                }
            }
            
            # Count structures
            for group in result['catalog_groups']:
                for struct in group['structures']:
                    result['metadata']['total_catalog_structures'] += 1
                    struct_type = struct.get('type', '')
                    if struct_type == 'root':
                        result['metadata']['root_categories'] += 1
                    elif struct_type == 'node':
                        result['metadata']['node_categories'] += 1
                    elif struct_type == 'leaf':
                        result['metadata']['leaf_categories'] += 1
            
            return result
            
        except Exception as e:
            print(f"❌ Error parsing BMEcat file: {e}")
            return None
    
    def _parse_header(self, root: ET.Element, ns: str) -> Dict[str, Any]:
        """Extract header information."""
        header = {}
        header_elem = root.find(f'.//{ns}HEADER')
        
        if header_elem is not None:
            # Catalog info
            catalog_elem = header_elem.find(f'{ns}CATALOG')
            if catalog_elem is not None:
                header['catalog'] = {
                    'languages': [lang.text for lang in catalog_elem.findall(f'{ns}LANGUAGE')],
                    'catalog_id': self._get_text(catalog_elem, f'{ns}CATALOG_ID'),
                    'catalog_version': self._get_text(catalog_elem, f'{ns}CATALOG_VERSION')
                }
            
            # Supplier info
            supplier_elem = header_elem.find(f'{ns}SUPPLIER')
            if supplier_elem is not None:
                header['supplier'] = {
                    'name': self._get_text(supplier_elem, f'{ns}SUPPLIER_NAME')
                }
            
            # Parties info
            parties = []
            for party in header_elem.findall(f'.//{ns}PARTY'):
                party_id = party.find(f'{ns}PARTY_ID')
                if party_id is not None:
                    parties.append({
                        'id': party_id.text,
                        'type': party_id.get('type')
                    })
            header['parties'] = parties
        
        return header
    
    def _parse_catalog_groups(self, root: ET.Element, ns: str) -> List[Dict[str, Any]]:
        """Extract all catalog group systems."""
        groups = []
        
        for group_system in root.findall(f'.//{ns}CATALOG_GROUP_SYSTEM'):
            group_data = {
                'group_system_id': self._get_text(group_system, f'{ns}GROUP_SYSTEM_ID'),
                'group_system_name': self._get_text(group_system, f'{ns}GROUP_SYSTEM_NAME'),
                'structures': []
            }
            
            # Parse all catalog structures
            for struct in group_system.findall(f'{ns}CATALOG_STRUCTURE'):
                structure_data = self._parse_catalog_structure(struct, ns)
                group_data['structures'].append(structure_data)
            
            groups.append(group_data)
        
        return groups
    
    def _parse_catalog_structure(self, struct: ET.Element, ns: str) -> Dict[str, Any]:
        """Parse a single CATALOG_STRUCTURE element with all details."""
        data = {
            'type': struct.get('type'),
            'group_id': self._get_text(struct, f'{ns}GROUP_ID'),
            'parent_id': self._get_text(struct, f'{ns}PARENT_ID'),
            'group_order': self._get_text(struct, f'{ns}GROUP_ORDER'),
            'names': {},
            'descriptions': {}
        }
        
        # Get all GROUP_NAME elements (multi-language)
        for name_elem in struct.findall(f'{ns}GROUP_NAME'):
            lang = name_elem.get('lang', 'default')
            data['names'][lang] = name_elem.text
        
        # Get all GROUP_DESCRIPTION elements (multi-language)
        for desc_elem in struct.findall(f'{ns}GROUP_DESCRIPTION'):
            lang = desc_elem.get('lang', 'default')
            # Preserve full description text including line breaks
            desc_text = desc_elem.text or ''
            data['descriptions'][lang] = desc_text.strip()
        
        return data
    
    def _get_text(self, element: ET.Element, path: str) -> Optional[str]:
        """Safely extract text from element."""
        elem = element.find(path)
        return elem.text if elem is not None else None
    
    def to_markdown(self, parsed_data: Dict[str, Any]) -> str:
        """
        Convert parsed BMEcat data to well-structured markdown for AI processing.
        
        Args:
            parsed_data: Output from parse_bmecat_file()
            
        Returns:
            Formatted markdown string
        """
        md_lines = []
        
        # Header section
        md_lines.append("# BMEcat Catalog Document\n")
        
        if parsed_data.get('header'):
            md_lines.append("## Catalog Information\n")
            header = parsed_data['header']
            
            if 'catalog' in header:
                cat = header['catalog']
                md_lines.append(f"**Catalog ID:** {cat.get('catalog_id', 'N/A')}")
                md_lines.append(f"**Version:** {cat.get('catalog_version', 'N/A')}")
                md_lines.append(f"**Languages:** {', '.join(cat.get('languages', []))}\n")
            
            if 'supplier' in header:
                md_lines.append(f"**Supplier:** {header['supplier'].get('name', 'N/A')}\n")
        
        # Metadata
        if parsed_data.get('metadata'):
            meta = parsed_data['metadata']
            md_lines.append("## Catalog Statistics\n")
            md_lines.append(f"- Total catalog structures: {meta['total_catalog_structures']}")
            md_lines.append(f"- Root categories: {meta['root_categories']}")
            md_lines.append(f"- Node categories: {meta['node_categories']}")
            md_lines.append(f"- Leaf categories: {meta['leaf_categories']}\n")
        
        # Catalog structures
        md_lines.append("## Catalog Structure Details\n")
        
        for group in parsed_data.get('catalog_groups', []):
            md_lines.append(f"### Group System: {group.get('group_system_name', 'N/A')}\n")
            md_lines.append(f"**System ID:** {group.get('group_system_id', 'N/A')}\n")
            
            for idx, struct in enumerate(group.get('structures', []), 1):
                md_lines.append(f"#### Structure {idx}: {struct.get('type', 'unknown').upper()}\n")
                md_lines.append(f"**Group ID:** `{struct.get('group_id', 'N/A')}` ")
                md_lines.append(f"**Parent ID:** `{struct.get('parent_id', 'N/A')}` ")
                md_lines.append(f"**Order:** {struct.get('group_order', 'N/A')}\n")
                
                # Names (all languages)
                if struct.get('names'):
                    md_lines.append("**Names:**")
                    for lang, name in struct['names'].items():
                        md_lines.append(f"- [{lang}] {name}")
                    md_lines.append("")
                
                # Descriptions (all languages)
                if struct.get('descriptions'):
                    md_lines.append("**Descriptions:**")
                    for lang, desc in struct['descriptions'].items():
                        if desc:  # Only show if description exists
                            md_lines.append(f"\n*[{lang}]*")
                            md_lines.append(desc)
                    md_lines.append("\n---\n")
        
        return "\n".join(md_lines)

------------------------------------------------- ./doc_processor/src/llm_processor.py --------------------------------------------------

"""Generic LLM API integration for document analysis (xAI Grok-compatible)."""

import json
from typing import Dict, Optional
import requests


class LLMProcessor:
    """Processes documents using xAI's Grok API via direct HTTP calls."""

    def __init__(self, api_key: str, model_name: str = "grok-4-fast-reasoning", base_url: str = "https://api.x.ai/v1"):
        """Initialize LLM processor.

        Args:
            api_key: xAI API key
            model_name: Grok model to use
            base_url: xAI API base URL
        """
        if not api_key:
            raise ValueError("LLM API key is required")

        self.api_key = api_key
        self.model = model_name
        self.base_url = base_url.rstrip('/')
    
    def create_analysis_prompt(self, text_content: str, template: Dict) -> str:
        """
        Create prompt for the LLM to analyze the document and fill the template.
        """
        template_str = json.dumps(template, indent=2)
        
        prompt = f"""Analyze the following document and fill the JSON template with accurate information extracted from the text.

INSTRUCTIONS:
- Be concise but thorough
- Extract only information present in the document
- Use "N/A" for fields where information is not available
- For lists, provide the most relevant items
- Estimate complexity as: Simple, Medium, or Complex

DOCUMENT CONTENT:
{text_content[:15000]}  

JSON TEMPLATE TO FILL:
{template_str}

Return ONLY the filled JSON, no other text."""
        
        return prompt
    
    def process_document(self, markdown_text: str, template: Dict) -> Optional[Dict]:
        """
        Send document to the LLM for analysis and return the filled JSON template.
        """
        try:
            print("🤖 Processing with LLM (xAI Grok)...")

            # Create prompt
            prompt = self.create_analysis_prompt(markdown_text, template)

            # Build request
            url = f"{self.base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are a document analysis AI that extracts structured information and fills JSON templates accurately.",
                    },
                    {
                        "role": "user",
                        "content": prompt,
                    },
                ],
                "temperature": 0.3,
                "max_tokens": 4000,
            }

            resp = requests.post(url, headers=headers, json=payload, timeout=60)

            if resp.status_code != 200:
                print(f"❌ Grok API error: {resp.status_code} - {resp.text[:500]}")
                return None

            data = resp.json()

            # Extract assistant message content
            try:
                content = data["choices"][0]["message"]["content"]
            except Exception:
                print("❌ Unexpected response format from Grok API")
                return None
            
            # Parse JSON response
            try:
                filled_template = json.loads(content)
                print("✅ AI analysis complete")
                return filled_template
            except json.JSONDecodeError:
                print("⚠️  Warning: Response was not valid JSON, attempting to extract...")
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != 0:
                    json_str = content[start:end]
                    filled_template = json.loads(json_str)
                    print("✅ AI analysis complete (extracted JSON)")
                    return filled_template
                else:
                    print("❌ Could not parse AI response as JSON")
                    return None
        except Exception as e:
            print(f"❌ Error processing with LLM: {e}")
            return None

------------------------------------------------- ./easy_rich/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "💰 Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "❌ Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "🤔 Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "🥷 Trying stealth mode..."

------------------------------------------------- ./easy_rich/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    └─ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("🥷 Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./easy_rich/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
from firecrawl import Firecrawl
from dotenv import load_dotenv
import re
from urllib.parse import urlparse, urljoin

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        DEFAULT_PROXY_MODE, BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    DEFAULT_PROXY_MODE = "auto"
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "💰 Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "❌ Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "🤔 Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "🥷 Trying stealth mode..."


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize Firecrawl client."""
        load_dotenv()
        api_key = os.getenv("FIRECRAWL_API_KEY")
        if not api_key:
            raise ValueError("FIRECRAWL_API_KEY not found in environment variables")
        self.firecrawl = Firecrawl(api_key=api_key)

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_proxy(self, url: str, proxy_mode: str = DEFAULT_PROXY_MODE) -> Optional[Dict[str, str]]:
        """
        Scrape content with specified proxy mode.
        
        Args:
            url: URL to scrape
            proxy_mode: Proxy mode ("auto", "basic", "stealth")
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            result = self.firecrawl.scrape(
                url,
                proxy=proxy_mode,
                formats=[
                    "markdown",
                    {
                        "type": "json",
                        "prompt": "Extract key information from this page including title, main content summary, key points, and any important data like prices, dates, or contact information.",
                    },
                ],
            )

            # Access attributes from Firecrawl Document object safely
            has_metadata = hasattr(result, "metadata") and result.metadata is not None
            title = getattr(result.metadata, "title", "No title found") if has_metadata else "No title found"
            status_code = getattr(result.metadata, "statusCode", "Unknown") if has_metadata else "Unknown"
            markdown_content = getattr(result, "markdown", "")
            structured_data = getattr(result, "json", {})

            return {
                "url": url,
                "title": title,
                "markdown_content": markdown_content,
                "structured_data": structured_data,
                "status_code": str(status_code),
            }

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Firecrawl with bot detection and stealth fallback.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # First attempt with default proxy
        scraped_content = self.scrape_with_proxy(url, DEFAULT_PROXY_MODE)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # Prompt for stealth retry
            if self.prompt_stealth_retry():
                print(STEALTH_TRYING_MSG)
                stealth_content = self.scrape_with_proxy(url, "stealth")
                if stealth_content:
                    print("✅ Success with stealth mode!")
                    return stealth_content
                else:
                    print("❌ Stealth mode also failed")
                    return None
            else:
                print("⏭️  Skipping stealth mode")
                return None
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Save scraped content to both markdown and JSON files.

        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            # Save markdown content
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./easy_rich/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None

------------------------------------------------- ./manual_scrape/config.py --------------------------------------------------

"""
Configuration settings for the Generic Web Scraper.
"""

# Proxy Settings
DEFAULT_PROXY_MODE = "auto"  # "auto", "basic", "stealth"
MANUAL_STEALTH_OVERRIDE = False  # Force stealth from start if True

# Cost Management  
STEALTH_COST_WARNING = True
STEALTH_CREDITS_COST = 5

# Bot Detection
BOT_DETECTION_CODES = [401, 403, 500]

# Terminal Messages
STEALTH_WARNING_MSG = "💰 Stealth mode costs {} credits per request"
BOT_DETECTED_MSG = "❌ Bot detected (Status: {})"
STEALTH_PROMPT_MSG = "🤔 Try stealth mode? [y/N]: "
STEALTH_TRYING_MSG = "🥷 Trying stealth mode..."

# Browser Configuration
BROWSER = {
    'default': 'firefox',  # Options: 'chromium', 'firefox'
    'engines': {
        'chromium': {
            'timeout': 30000,
            'stealth_mode': True
        },
        'firefox': {
            'timeout': 30000,
            'stealth_mode': True
        }
    }
}

------------------------------------------------- ./manual_scrape/install_browsers.sh --------------------------------------------------

#!/bin/bash
echo "🚀 Installing Playwright browsers..."
playwright install
playwright install chromium firefox
echo "✅ Both Chromium and Firefox installed successfully!"

------------------------------------------------- ./manual_scrape/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from typing import Tuple, Optional, List
from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from config import MANUAL_STEALTH_OVERRIDE, DEFAULT_PROXY_MODE
import config


def get_scraping_mode() -> str:
    """Get user's preferred scraping mode."""
    print("\n=== Generic Web Scraper ===")
    print("Choose your option:")
    print("1. Search the web for something")
    print("2. Enter a direct URL to scrape")
    
    while True:
        choice = input("Your choice (1, 2): ").strip()
        if choice == "1":
            return "search"
        elif choice == "2":
            return "url"
        else:
            print("Please enter a valid choice")


def get_search_input() -> Tuple[str, Optional[str]]:
    """Get search text and optional website from user."""
    search_text = input("Enter search text (required): ").strip()
    if not search_text:
        raise ValueError("Search text cannot be empty!")
    
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()
    return search_text, website if website else None


def get_direct_url() -> str:
    """Get direct URL from user."""
    url = input("Enter the URL to scrape: ").strip()
    if not url:
        raise ValueError("URL cannot be empty!")
    if not (url.startswith('http://') or url.startswith('https://')):
        url = 'https://' + url
    return url


def display_subpage_menu(links: List[Tuple[str, str]]) -> None:
    """Display available subpages in a numbered menu with improved formatting."""
    if not links:
        print("\nNo additional subpages found on this domain.")
        return
    
    print(f"\nFound {len(links)} subpages on the same domain:")
    print("=" * 80)
    for i, (title, url) in enumerate(links, 1):
        # Truncate long titles for readability
        display_title = title[:60] + "..." if len(title) > 60 else title
        print(f"{i:2}. {display_title}")
        print(f"    └─ {url}")
        print()  # Add blank line between items for better readability
    print("=" * 80)


def handle_subpage_choice(links: List[Tuple[str, str]]) -> Optional[str]:
    """Handle user's subpage selection."""
    if not links:
        return None
        
    while True:
        print("\nEnter number (1-{0}), 'n' for new search, or 'q' to quit:".format(len(links)))
        choice = input("> ").strip().lower()
        
        if choice == 'q':
            return 'quit'
        elif choice == 'n':
            return 'new'
        elif choice.isdigit():
            num = int(choice)
            if 1 <= num <= len(links):
                return links[num - 1][1]  # Return the URL
            else:
                print(f"\nPlease enter a number between 1 and {len(links)}")
        else:
            print("\nPlease enter a valid number, 'n', or 'q'")


def main() -> None:
    """Main application entry point with enhanced subpage support."""
    print("Starting Enhanced Generic Web Scraper...")
    
    try:
        serp_client = SerpAPIClient()
        session_folder = None
        original_proxy_mode = config.DEFAULT_PROXY_MODE
        stealth_session_applied = False
        
        while True:
            try:
                # Get scraping mode
                mode = get_scraping_mode()

                # Handle stealth session mode
                stealth_session = False
                if mode == "stealth_session":
                    stealth_session = True
                    print("🥷 Stealth mode enabled for this session")
                    mode = get_scraping_mode()  # Get the actual scraping mode

                # Before scraping, if stealth_session is True, temporarily override the proxy mode
                if stealth_session and not stealth_session_applied:
                    # Modify the WebScraper to use stealth mode by default
                    # Keep import style consistent and safe
                    _ = DEFAULT_PROXY_MODE  # referenced to satisfy explicit import
                    config.DEFAULT_PROXY_MODE = "stealth"
                    stealth_session_applied = True

                # Create WebScraper instance after applying any session overrides
                web_scraper = WebScraper()
                
                if mode == "search":
                    # Original search workflow
                    search_text, website = get_search_input()
                    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))
                    
                    # Search the web
                    search_results = serp_client.search_web(search_text, website)
                    if not search_results:
                        print("Failed to get search results")
                        continue
                    
                    # Extract target URL
                    target_url = serp_client.extract_first_url(search_results, website)
                    if not target_url:
                        print("No relevant URL found in search results")
                        continue
                        
                    print(f"Found URL: {target_url}")
                    filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
                    
                else:  # mode == "url"
                    # Direct URL workflow
                    target_url = get_direct_url()
                    print(f"\nPreparing to scrape: {target_url}")
                    
                    # Generate filename from URL
                    from urllib.parse import urlparse
                    parsed_url = urlparse(target_url)
                    filename = f"{parsed_url.netloc.replace('.', '_')}_{parsed_url.path.replace('/', '_').strip('_')}"
                    filename = filename or "direct_scrape"

                # Create session folder on first scrape
                if session_folder is None:
                    if mode == "search":
                        session_folder = web_scraper.create_session_folder(target_url, search_text)
                    else:
                        session_folder = web_scraper.create_session_folder(target_url)
                    print(f"Created session folder: {session_folder}")

                # Scrape the page
                print("Scraping page content...")
                scraped_content = web_scraper.scrape_page(target_url)
                
                if not scraped_content:
                    print("Failed to scrape page content")
                    continue

                print(f"Successfully scraped: {scraped_content['title']}")
                
                # Save content to session folder
                web_scraper.save_content_to_session(scraped_content, filename, session_folder)
                
                # Extract and display subpage options
                links = web_scraper.extract_links_from_markdown(
                    scraped_content.get('markdown_content', ''), 
                    target_url
                )
                
                display_subpage_menu(links)
                
                # Handle subpage choice
                while links:
                    choice = handle_subpage_choice(links)
                    
                    if choice == 'quit':
                        print("Thanks for using the Enhanced Web Scraper!")
                        return
                    elif choice == 'new':
                        break  # Break inner loop to start new search
                    elif choice:  # It's a URL
                        # Find the title corresponding to the chosen URL
                        chosen_title = None
                        for title, url in links:
                            if url == choice:
                                chosen_title = title
                                break
                        
                        print(f"\nScraping subpage: {chosen_title or choice}")
                        subpage_content = web_scraper.scrape_page(choice)
                        
                        if subpage_content:
                            # Generate subpage filename using the readable title
                            if chosen_title:
                                subpage_filename = web_scraper.sanitize_filename(chosen_title)
                            else:
                                # Fallback to URL-based naming
                                from urllib.parse import urlparse
                                parsed_subpage = urlparse(choice)
                                subpage_filename = f"subpage_{parsed_subpage.path.replace('/', '_').strip('_')}"
                                subpage_filename = subpage_filename or "subpage"
                            
                            print(f"Successfully scraped subpage: {subpage_content['title']}")
                            web_scraper.save_content_to_session(subpage_content, subpage_filename, session_folder)
                            
                            # Extract links from subpage for further exploration
                            subpage_links = web_scraper.extract_links_from_markdown(
                                subpage_content.get('markdown_content', ''), 
                                choice
                            )
                            
                            if subpage_links:
                                display_subpage_menu(subpage_links)
                                links = subpage_links  # Update links for next iteration
                            else:
                                print("No more subpages found. Returning to main menu.")
                                break
                        else:
                            print("Failed to scrape subpage")
                
                # If no links or user chose 'new', continue to next iteration
                
            except ValueError as e:
                print(f"Input error: {e}")
            except KeyboardInterrupt:
                print("\nOperation cancelled by user")
                break
            except Exception as e:
                print(f"Unexpected error: {e}")
                
    except Exception as e:
        print(f"Application error: {e}")
    finally:
        # Restore proxy mode if it was overridden for stealth session
        try:
            if 'stealth_session_applied' in locals() and stealth_session_applied:
                config.DEFAULT_PROXY_MODE = original_proxy_mode
        except Exception:
            pass
    
    print("Scraping session completed!")


if __name__ == "__main__":
    main()

------------------------------------------------- ./manual_scrape/src/content_extractor.py --------------------------------------------------

"""Extract article content from webpages using Playwright."""

import time
from bs4 import BeautifulSoup
from .browser_utils import handle_any_popups, add_human_pause, extract_readable_text

class ContentExtractor:
    """Extract article content from a webpage."""
    
    def __init__(self):
        pass
    
    def extract_article_content(self, page, url, article, domain, source, user_email=None):
        """Extract article content from the page.
        
        Args:
            page: The Playwright page object
            url: The URL of the article
            article: The article metadata
            domain: The domain of the article
            source: The source of the article
            user_email: Optional user email for validation context
            
        Returns:
            tuple: (success, article_content, title, rejection_reason)
        """
        print("Extracting article content...")
                
        try:
            # Start with aggressive popup handling first
            print("Applying aggressive popup/cookie handling approach...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Perform progressive scrolling for content exposure
            print("Performing progressive scrolling for content exposure...")
            self._perform_progressive_scrolling(page)
            
            # Handle any popups that appeared after scrolling
            print("Handling any new popups after scrolling...")
            handle_any_popups(page, aggressive=True)
            page.wait_for_timeout(1000)
            
            # Wait for page to be fully loaded with a timeout
            print("Waiting for page ready state before content extraction...")
            try:
                page.wait_for_load_state("networkidle", timeout=10000)
                print("Page ready state reached")
            except Exception as e:
                print(f"Error waiting for page ready state (continuing anyway): {e}")
            
            # Extract content with BeautifulSoup
            article_content = self._extract_content_with_soup(page)
            
            # Get character count
            chars_count = len(article_content) if article_content else 0
            print(f"Extracted {chars_count} chars with content extraction")
            
            # Include the full content in logs when it's less than 100 characters
            if article_content and chars_count < 100:
                print(f"Content (under 100 chars): {article_content}")
            
            # If we have substantial content (>5500 chars), consider it valid
            if article_content and len(article_content) > 5500:
                title = self._extract_title(page, article)
                print(f"Article has substantial content ({len(article_content)} chars), accepting")
                return True, article_content, title, None
            
            # Continue with validation for articles with less content
            if article_content and len(article_content) > 150:
                title = self._extract_title(page, article)
            
                # Return the content for validation
                if len(article_content) < 1600:
                    print(f"Content too short: {len(article_content)} chars (minimum 1600)")
                    
                    # Try a more aggressive approach as fallback
                    try:
                        print("Attempting more aggressive content extraction as fallback...")
                        
                        fallback_content = page.evaluate("""() => {
                            const bodyText = document.body.innerText;
                            return bodyText || '';
                        }""")
                        
                        if fallback_content and len(fallback_content) > 1600:
                            print(f"Fallback extraction succeeded, got {len(fallback_content)} chars")
                            return True, fallback_content, title, None
                    except Exception as fallback_error:
                        print(f"Fallback extraction failed: {fallback_error}")
                    
                    rejection_reason = f"Content too short: {len(article_content)} chars (minimum 1600)"
                    return False, "", title, rejection_reason
                
                return True, article_content, title, None
            else:
                rejection_reason = "Content too short: 0 chars"
                print("Content extraction failed: Empty or very short content")
                return False, "", article.get("title", ""), rejection_reason
            
        except Exception as e:
            print(f"Error extracting content: {e}")
            import traceback
            print(f"Extraction error traceback: {traceback.format_exc()}")
            rejection_reason = f"Error: {str(e)}"
            return False, "", article.get("title", ""), rejection_reason

    def _perform_progressive_scrolling(self, page):
        """Perform progressive scrolling to expose all content."""
        try:
            page_height = page.evaluate("document.body.scrollHeight")
            view_height = page.evaluate("window.innerHeight")
            
            scroll_steps = min(4, max(2, int(page_height / view_height)))
            print(f"Performing {scroll_steps} scroll steps to load all content...")
            
            for i in range(1, scroll_steps + 1):
                scroll_position = (i * page_height) / scroll_steps
                page.evaluate(f"window.scrollTo(0, {scroll_position})")
                page.wait_for_timeout(1000)
            
            # Go back to top before extraction
            page.evaluate("window.scrollTo(0, 0)")
            page.wait_for_timeout(1000)
            
        except Exception as e:
            print(f"Error during progressive scrolling: {e}")

    def _extract_content_with_soup(self, page):
        """Extract content using BeautifulSoup parsing."""
        try:
            # Get page HTML and parse with BeautifulSoup
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract readable text
            return extract_readable_text(soup)
            
        except Exception as e:
            print(f"Error during content extraction: {e}")
            return ""

    def _extract_title(self, page, article):
        """Extract title from the page."""
        try:
            # Try to get title from page
            page_title = page.title()
            if page_title and len(page_title.strip()) > 0:
                return page_title.strip()
                
            # Fallback to original title
            return article.get("title", "")
            
        except Exception as e:
            print(f"Error extracting title: {e}")
            return article.get("title", "")

------------------------------------------------- ./manual_scrape/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict, List, Tuple, Set
import os
import sys
import re
from urllib.parse import urlparse, urljoin
from playwright.sync_api import sync_playwright

# Add config imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from config import (
        BOT_DETECTION_CODES, STEALTH_COST_WARNING, 
        STEALTH_CREDITS_COST, STEALTH_WARNING_MSG, BOT_DETECTED_MSG, 
        STEALTH_PROMPT_MSG, STEALTH_TRYING_MSG
    )
except ImportError:
    # Fallback values if config.py doesn't exist yet
    BOT_DETECTION_CODES = [401, 403, 500]
    STEALTH_COST_WARNING = True
    STEALTH_CREDITS_COST = 5
    STEALTH_WARNING_MSG = "💰 Stealth mode costs {} credits per request"
    BOT_DETECTED_MSG = "❌ Bot detected (Status: {})"
    STEALTH_PROMPT_MSG = "🤔 Try stealth mode? [y/N]: "
    STEALTH_TRYING_MSG = "🥷 Trying stealth mode..."

from .browsers.browser_factory import BrowserFactory
from .content_extractor import ContentExtractor


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize web scraper with Playwright."""
        self.content_extractor = ContentExtractor()

    def is_bot_detected(self, status_code: str) -> bool:
        """Check if the response indicates bot detection."""
        try:
            return int(status_code) in BOT_DETECTION_CODES
        except (ValueError, TypeError):
            return False

    def prompt_stealth_retry(self) -> bool:
        """Prompt user for stealth mode retry with cost warning."""
        if STEALTH_COST_WARNING:
            print(STEALTH_WARNING_MSG.format(STEALTH_CREDITS_COST))
        
        while True:
            choice = input(STEALTH_PROMPT_MSG).strip().lower()
            if choice in ['y', 'yes']:
                return True
            elif choice in ['n', 'no', '']:
                return False
            else:
                print("Please enter 'y' or 'n'")

    def scrape_with_playwright(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content with Playwright browser.
        
        Args:
            url: URL to scrape
        
        Returns:
            Scraped content dict or None if failed
        """
        try:
            with sync_playwright() as p:
                # Create browser instance
                browser_config = BrowserFactory.create()
                browser = browser_config.launch(p, headless=True)
                
                # Create context with stealth settings
                user_agents = browser_config.get_user_agents()
                import random
                context = browser.new_context(
                    user_agent=random.choice(user_agents),
                    viewport={"width": 1280, "height": 1200}
                )
                page = context.new_page()
                
                # Set timeout
                page.set_default_navigation_timeout(60000)
                
                try:
                    # Navigate to URL
                    response = page.goto(url, wait_until="domcontentloaded")
                    
                    if response and response.status >= 400:
                        print(f"HTTP error {response.status} while accessing content")
                        return None
                    
                    # Extract domain and create article dict
                    domain = urlparse(url).netloc
                    article = {"title": f"Article from {domain}"}
                    
                    # Extract content
                    success, article_content, title, rejection_reason = self.content_extractor.extract_article_content(
                        page, url, article, domain, domain
                    )
                    
                    if not success:
                        print(f"Content extraction failed: {rejection_reason}")
                        return None
                    
                    return {
                        "url": url,
                        "title": title,
                        "markdown_content": article_content,
                        "structured_data": {},
                        "status_code": str(response.status if response else 200),
                    }
                    
                finally:
                    page.close()
                    context.close()
                    browser.close()

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Playwright with bot detection.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        # Scrape with Playwright
        scraped_content = self.scrape_with_playwright(url)
        
        if not scraped_content:
            return None
        
        # Check for bot detection
        status_code = scraped_content.get("status_code", "Unknown")
        if self.is_bot_detected(status_code):
            print(BOT_DETECTED_MSG.format(status_code))
            
            # For now, just return None - stealth retry could be implemented later
            print("⭐ Bot detection encountered")
        
        return scraped_content

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")

    def extract_links_from_markdown(self, markdown_content: str, base_url: str) -> List[Tuple[str, str]]:
        """
        Extract all markdown links from content and filter by same domain.
        
        Args:
            markdown_content: The markdown content to parse
            base_url: The original URL to determine the base domain
            
        Returns:
            List of tuples (title, url) for same-domain links
        """
        try:
            # Parse base domain
            base_domain = urlparse(base_url).netloc.lower()
            
            # Extract markdown links using regex
            link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
            matches = re.findall(link_pattern, markdown_content)
            
            same_domain_links: List[Tuple[str, str]] = []
            seen_urls: Set[str] = set()
            
            for title, url in matches:
                # Convert relative URLs to absolute
                if url.startswith('/'):
                    url = urljoin(base_url, url)
                elif not url.startswith('http'):
                    continue
                
                # Check if same domain
                try:
                    link_domain = urlparse(url).netloc.lower()
                    if base_domain in link_domain or link_domain in base_domain:
                        # Avoid duplicates and self-references
                        if url not in seen_urls and url != base_url:
                            same_domain_links.append((title.strip(), url))
                            seen_urls.add(url)
                except Exception:
                    continue
                
            return same_domain_links[:100]
            
        except Exception as e:
            print(f"Error extracting links: {e}")
            return []

    def sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a string to be safe for use as a filename.
        
        Args:
            filename: The raw filename string
            
        Returns:
            A filesystem-safe filename
        """
        import re
        
        # Remove or replace problematic characters
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)  # Remove invalid chars
        filename = re.sub(r'\s+', '_', filename.strip())  # Replace spaces with underscores
        filename = re.sub(r'_+', '_', filename)  # Remove multiple underscores
        filename = filename.strip('_')  # Remove leading/trailing underscores
        
        # Limit length and ensure it's not empty
        filename = filename[:50] if filename else "unnamed"
        
        return filename

    def create_session_folder(self, base_url: str, search_term: str = None) -> str:
        """Create a session folder based on domain, search term, and timestamp."""
        try:
            domain = urlparse(base_url).netloc.replace('www.', '').replace('.', '_')
            timestamp = __import__('datetime').datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Include search term if available
            if search_term:
                clean_search = self.sanitize_filename(search_term)
                folder_name = f"{domain}_{clean_search}_{timestamp}"
            else:
                folder_name = f"{domain}_{timestamp}"
            
            import os
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
            return folder_name
        except Exception as e:
            print(f"Error creating session folder: {e}")
            return "."

    def save_content_to_session(self, content: Dict[str, str], filename: str, session_folder: str) -> None:
        """Save content to session folder."""
        try:
            import json
            
            # Save markdown content
            md_path = os.path.join(session_folder, f"{filename}.md")
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown saved to {md_path}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_path = os.path.join(session_folder, f"{filename}_data.json")
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump({
                        "url": content.get("url", ""),
                        "title": content.get("title", ""),
                        "status_code": content.get("status_code", ""),
                        "structured_data": content.get("structured_data", {}),
                    }, f, indent=2, ensure_ascii=False)
                print(f"Structured data saved to {json_path}")
            
        except Exception as e:
            print(f"Error saving content to session: {e}")
------------------------------------------------- ./manual_scrape/src/browser_utils.py --------------------------------------------------

"""Browser utilities for popup handling, data extraction, and stealth operations."""

import time
import random
from bs4 import BeautifulSoup

def get_human_delay(min_ms=800, max_ms=2500):
    """Get a random delay that mimics human behavior."""
    return random.randint(min_ms, max_ms)

def add_human_pause(page, min_ms=1000, max_ms=3000):
    """Add a human-like pause with mouse movement."""
    try:
        viewport = page.viewport_size
        if viewport:
            x = random.randint(100, viewport['width'] - 100)
            y = random.randint(100, viewport['height'] - 100)
            page.mouse.move(x, y)
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))
    except Exception:
        page.wait_for_timeout(get_human_delay(min_ms, max_ms))

def handle_cookies_and_consent(page, timeout=15000):
    """Handle cookies and consent dialogs with Playwright selectors."""
    print("Handling cookies and consent with Playwright selectors...")
    
    start_time = time.time()
    max_time_ms = timeout
    
    try:
        if (time.time() - start_time) * 1000 > max_time_ms:
            print(f"Cookie handling timed out after {timeout}ms")
            return False
        
        # Priority 1: Explicit accept buttons
        accept_buttons = [
            "button:has-text('I accept all cookies')",
            "button:has-text('Accept all cookies')", 
            "button:has-text('Accept All')",
            "button:has-text('Accept')",
            "button:has-text('Accetta tutto')",  # Italian
            "button:has-text('Accetto')",       # Italian
        ]
        
        # Priority 2: Close/dismiss buttons  
        close_buttons = [
            "button:has-text('Close')",
            "button:has-text('Chiudi')",        # Italian
            "button:has-text('×')",
            "button[aria-label*='close']",
            "button[class*='close']",
        ]
        
        # Priority 3: Generic attribute-based selectors
        generic_buttons = [
            "[id*='accept'] button:not([href]):not(a)",
            "[class*='accept'] button:not([href]):not(a)", 
            "[id*='cookie'] button:not([href]):not(a)",
            "[class*='cookie'] button:not([href]):not(a)",
            "[data-testid*='accept'] button:not([href]):not(a)",
        ]
        
        # Combine all selectors in priority order
        button_selectors = accept_buttons + close_buttons + generic_buttons
        
        clicked = False
        
        # Try each selector with timeout check
        for selector in button_selectors:
            if (time.time() - start_time) * 1000 > max_time_ms:
                print(f"Cookie handling timed out during selector iteration")
                return False
            
            try:
                if page.query_selector(selector):
                    print(f"Found popup button with selector: {selector}")
                    page.click(selector, timeout=2000)
                    print(f"Successfully clicked popup button with selector: {selector}")
                    clicked = True
                    break
            except Exception:
                continue
        
        if clicked:
            print("Cookie/consent button clicked, waiting for page to stabilize...")
            remaining_time = max(0, max_time_ms - (time.time() - start_time) * 1000)
            if remaining_time > 1000:
                page.wait_for_timeout(min(2000, int(remaining_time)))
            return True
        
        return False
        
    except Exception as e:
        print(f"Error handling cookies and consent: {e}")
        return False

def handle_any_popups(page, aggressive=False):
    """Handle any type of popup, consent dialog, or cookie banner."""
    print(f"Trying {'aggressive ' if aggressive else ''}popup handling...")
    
    # Step 1: Try button clicking with human delays
    add_human_pause(page, 500, 1200)
    
    button_clicked = handle_cookies_and_consent(page, timeout=8000 if aggressive else 5000)
    
    if button_clicked:
        print("✅ Successfully handled popup with button clicking")
        add_human_pause(page, 1000, 2000)
        return True
    
    # Step 2: DOM removal fallback if aggressive
    if aggressive:
        print("Using DOM removal fallback...")
        add_human_pause(page, 1000, 2000)
        
        try:
            removed_count = page.evaluate("""() => {
                const removeElements = (selector) => {
                    const elements = document.querySelectorAll(selector);
                    let removed = 0;
                    elements.forEach(el => {
                        el.remove();
                        removed++;
                    });
                    return removed;
                };
                
                let removedCount = 0;
                
                // Remove elements with high z-index that could be overlays
                document.querySelectorAll('*').forEach(el => {
                    const style = window.getComputedStyle(el);
                    const zIndex = parseInt(style.zIndex);
                    if (zIndex > 999) {
                        const position = style.position;
                        if (position === 'fixed' || position === 'absolute') {
                            el.remove();
                            removedCount++;
                        }
                    }
                });
                
                // Remove common cookie/consent banners
                removedCount += removeElements('[class*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[class*="consent"]:not(html):not(body)');
                removedCount += removeElements('[class*="popup"]:not(html):not(body)');
                removedCount += removeElements('[class*="banner"]:not(html):not(body)');
                removedCount += removeElements('[class*="overlay"]:not(html):not(body)');
                removedCount += removeElements('[class*="modal"]:not(html):not(body)');
                removedCount += removeElements('[id*="cookie"]:not(html):not(body)');
                removedCount += removeElements('[id*="consent"]:not(html):not(body)');
                
                document.body.style.overflow = 'auto';
                document.documentElement.style.overflow = 'auto';
                
                return removedCount;
            }""")
            
            if removed_count > 0:
                print("DOM removal: removed ${removed_count} elements")
                add_human_pause(page, 1500, 3000)
                return True
        except Exception as e:
            print(f"DOM removal failed: {e}")
    
    return False

def extract_readable_text(soup):
    """Extract readable text from BeautifulSoup object."""
    # Remove unwanted elements
    for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
        element.decompose()
    
    # Try to find main content areas
    content_selectors = [
        'article', 
        '[role="main"]', 
        '.content', 
        '#content',
        '.post-content',
        '.article-body',
        '.entry-content'
    ]
    
    for selector in content_selectors:
        content = soup.select_one(selector)
        if content:
            text = content.get_text(separator=' ', strip=True)
            if len(text) > 500:  # Only return if substantial content
                return text
    
    # Fallback to body text
    return soup.get_text(separator=' ', strip=True)

------------------------------------------------- ./manual_scrape/src/browsers/browser_firefox.py --------------------------------------------------

"""Firefox browser implementation."""

from typing import List

class FirefoxBrowser:
    """Firefox browser configuration and launch handler."""

    name = "firefox"

    def get_launch_args(self) -> List[str]:
        """Get Firefox-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--no-first-run',
            '--disable-default-browser-check',
            '--disable-infobars'
        ]

    def launch(self, playwright_instance, headless: bool = False):
        """Launch Firefox browser with stealth configuration."""
        print(f"🚀 Launching Firefox browser (headless: {headless})")
        print(f"🔧 Firefox launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.firefox.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"✅ Firefox browser launched successfully")
            return browser
        except Exception as e:
            print(f"❌ Failed to launch Firefox browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Firefox-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:119.0) Gecko/20100101 Firefox/119.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/browsers/__init__.py --------------------------------------------------

"""Browser factory and configuration modules."""

------------------------------------------------- ./manual_scrape/src/browsers/browser_factory.py --------------------------------------------------

"""Factory class for creating browser instances."""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import BROWSER

from .browser_chromium import ChromiumBrowser
from .browser_firefox import FirefoxBrowser

class BrowserFactory:
    """Factory class for creating browser instances."""

    _browsers = {
        'chromium': ChromiumBrowser,
        'firefox': FirefoxBrowser
    }

    @classmethod
    def create(cls, browser_name=None):
        """Create a browser instance based on configuration or specified name."""
        if browser_name is None:
            browser_name = BROWSER.get('default', 'firefox')
            print(f"🌐 Browser auto-selected from config: {browser_name}")
        else:
            print(f"🌐 Browser explicitly requested: {browser_name}")

        browser_class = cls._browsers.get(browser_name)
        if not browser_class:
            raise ValueError(f"Unsupported browser: {browser_name}. Available: {list(cls._browsers.keys())}")

        browser_instance = browser_class()
        print(f"✅ Browser instance created: {browser_instance.name}")
        return browser_instance

    @classmethod
    def get_available_browsers(cls):
        """Get list of available browser names."""
        return list(cls._browsers.keys())
    
    @classmethod
    def verify_browser_config(cls):
        """Verify current browser configuration and return details."""
        try:
            config_browser = BROWSER.get('default', 'firefox')
            print(f"📋 Config browser setting: {config_browser}")
            
            browser_instance = cls.create()
            print(f"📋 Actual browser created: {browser_instance.name}")
            
            return {
                'config_browser': config_browser,
                'actual_browser': browser_instance.name,
                'available_browsers': cls.get_available_browsers()
            }
        except Exception as e:
            print(f"❌ Browser config verification failed: {e}")
            return None

------------------------------------------------- ./manual_scrape/src/browsers/browser_chromium.py --------------------------------------------------

"""Chromium browser implementation."""

from typing import List

class ChromiumBrowser:
    """Chromium browser configuration and launch handler."""

    name = "chromium"

    def get_launch_args(self) -> List[str]:
        """Get Chromium-specific launch arguments optimized for stealth."""
        return [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-dev-shm-usage',
            '--disable-background-timer-throttling',
            '--disable-backgrounding-occluded-windows',
            '--disable-renderer-backgrounding',
            '--disable-features=TranslateUI',
            '--disable-ipc-flooding-protection',
            '--no-first-run',
            '--force-device-scale-factor=1',
            '--disable-default-apps'
        ]

    def launch(self, playwright_instance, headless: bool = True):
        """Launch Chromium browser with stealth configuration."""
        print(f"🚀 Launching Chromium browser (headless: {headless})")
        print(f"🔧 Chromium launch args: {self.get_launch_args()}")
        
        try:
            browser = playwright_instance.chromium.launch(
                headless=headless,
                args=self.get_launch_args()
            )
            print(f"✅ Chromium browser launched successfully")
            return browser
        except Exception as e:
            print(f"❌ Failed to launch Chromium browser: {e}")
            raise

    def get_user_agents(self) -> List[str]:
        """Get Chromium-compatible user agents."""
        return [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]

    def add_human_behavior(self, page):
        """Add realistic human behavior including mouse movement."""
        import random
        # Random mouse movement
        page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
        page.wait_for_timeout(random.randint(800, 2000))

------------------------------------------------- ./manual_scrape/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None

------------------------------------------------- ./outputs/unique_features.csv --------------------------------------------------

fname_de,fvalue_de,fname_fr,fvalue_fr,fname_it,fvalue_it
Akku,-- Stück,accu,-- pièce,batteria,-- pezzo
Akku-Kapazität,-- Ah,capacité de l’accu,-- Ah,capacità batteria,-- Ah
Akku-Typ,--,type d’accu,--,tipo di batteria,--
Aussen-Ø,216 mm,Ø ex­té­rieur,216 mm,Ø esterno,216 mm
Bohrleistung Beton,24 mm,cap. béton,24 mm,cap. calcestruzzo,24 mm
Boh­rungs-Ø,30 mm,Ø perçage,30 mm,Ø foro,30 mm
Drehstopp,ja,arrêt de frappe,oui,arresto di rotazione,sì
Drehzahl,5500 min-1,vitesse,5500 min-1,numero di giri,5500 min-1
Einzelschlagenergie,2.1 J (EPTA),energie de frappe,2.1 J (EPTA),energia percussione singola,2.1 J (EPTA)
Gewicht,2.50 kg,poids,2.50 kg,peso,2.50 kg
LED-Leuchte,mit,lampe DEL,avec,lampada LED,con
Ladegerät,--,chargeur,--,caricabatterie,--
Leerlaufdrehzahl,0-1100 min-1,nombre de tours à vide,0-1100 min-1,numero di giri a vuoto,0-1100 min-1
Scheiben-Ø,125 mm,Ø disques,125 mm,Ø dischi,125 mm
Schlagzahl,0-4600 min-1,fréquence de frappe,0-4600 min-1,frequenza colpo al minuto,0-4600 min-1
Schnittleistung 45°,51 mm,profondeur de coupe 45°,51 mm,profondità taglio 45°,51 mm
Schnittleistung 90°,67 mm,profondeur de coupe à 90°,67 mm,profondità taglio a 90°,67 mm
Schnitt­breite,2.6 mm,largeur de coupe,2.6 mm,larghezza del taglio,2.6 mm
Spannung,18 V,tension,18 V,tensione,18 V
Span­winkel,-5 °,angle,-5 °,angolo di spoglia superiore,-5 °
Stamm­blatt­stärke,1.7 mm,épaisseur du corps de lame,1.7 mm,spessore lama,1.7 mm
Sägeblatt-Ø,190 mm,Ø de la lame de scie,190 mm,Ø della lama,190 mm
Sägeblatt-Ø Bohrung,30 mm,Ø de forage de la lame de scie,30 mm,"Ø lama da sega, foro",30 mm
Verpackung,T-STAK,emballage,T-STAK,imballaggio,T-STAK
Zahn­form,WZ,denture,WZ,forma del dente,WZ

------------------------------------------------- ./outputs/master_bmecat_dabag.json.backup1 --------------------------------------------------

{
  "metadata": {
    "created_at": "2025-10-16T11:38:43.726045",
    "last_updated": "2025-10-16T11:39:36.676459",
    "total_products": 6
  },
  "products": {
    "DCH273NT-XJ": {
      "SUPPLIER_PID": "DCH273NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCH273NT-XJ&markId=DCH273NT-XJ&artId=100292296&trefferUUID=73B4DDF0-869B-48B5-86D42A4407354CF8",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazität": "-- Ah",
          "Leerlaufdrehzahl": "0-1100 min-1",
          "Drehstopp": "ja",
          "Schlagzahl": "0-4600 min-1",
          "Einzelschlagenergie": "2.1 J (EPTA)",
          "Bohrleistung Beton": "24 mm",
          "LED-Leuchte": "mit",
          "Akku": "-- Stück",
          "Ladegerät": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "2.50 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d’accu": "--",
          "capacité de l’accu": "-- Ah",
          "nombre de tours à vide": "0-1100 min-1",
          "arrêt de frappe": "oui",
          "fréquence de frappe": "0-4600 min-1",
          "energie de frappe": "2.1 J (EPTA)",
          "cap. béton": "24 mm",
          "lampe DEL": "avec",
          "accu": "-- pièce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "2.50 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacità batteria": "-- Ah",
          "numero di giri a vuoto": "0-1100 min-1",
          "arresto di rotazione": "sì",
          "frequenza colpo al minuto": "0-4600 min-1",
          "energia percussione singola": "2.1 J (EPTA)",
          "cap. calcestruzzo": "24 mm",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "2.50 kg"
        }
      },
      "scraped_at": "2025-10-16T11:38:56.544330"
    },
    "DCG405NT-XJ": {
      "SUPPLIER_PID": "DCG405NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCG405NT-XJ&markId=DCG405NT-XJ&artId=100321765&trefferUUID=D315C16A-D4EA-4D1B-8FF9BF9F8F6D43E4",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazität": "-- Ah",
          "Scheiben-Ø": "125 mm",
          "Leerlaufdrehzahl": "9000 min-1",
          "Akku": "-- Stück",
          "Ladegerät": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "1.80 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d’accu": "--",
          "capacité de l’accu": "-- Ah",
          "Ø disques": "125 mm",
          "nombre de tours à vide": "9000 min-1",
          "accu": "-- pièce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "1.80 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacità batteria": "-- Ah",
          "Ø dischi": "125 mm",
          "numero di giri a vuoto": "9000 min-1",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "1.80 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:03.420664"
    },
    "DCK329P2T-QW": {
      "SUPPLIER_PID": "DCK329P2T-QW",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCK329P2T-QW&markId=DCK329P2T-QW&artId=100683390&trefferUUID=15A26530-F3A5-4A78-884602D12ABD18D2",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "Li-Ion",
          "Akku-Kapazität": "5 Ah",
          "Akku": "2 Stück",
          "Ladegerät": "DCB1104",
          "Verpackung": "T STAK-Box VI / T STAK-Box II"
        },
        "fr": {
          "tension": "18 V",
          "type d’accu": "Li-Ion",
          "capacité de l’accu": "5 Ah",
          "accu": "2 pièce",
          "chargeur": "DCB1104",
          "emballage": "T STAK-Box VI / T STAK-Box II"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "Li-Ion",
          "capacità batteria": "5 Ah",
          "batteria": "2 pezzo",
          "caricabatterie": "DCB1104",
          "imballaggio": "T STAK-Box VI / T STAK-Box II"
        }
      },
      "scraped_at": "2025-10-16T11:39:10.157359"
    },
    "DCS573NT-XJ": {
      "SUPPLIER_PID": "DCS573NT-XJ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DCS573NT-XJ&markId=DCS573NT-XJ&artId=100571860&trefferUUID=46DDD712-4A88-4D51-88021A3495BD79D7",
      "languages": {
        "de": {
          "Spannung": "18 V",
          "Akku-Typ": "--",
          "Akku-Kapazität": "-- Ah",
          "Sägeblatt-Ø": "190 mm",
          "Sägeblatt-Ø Bohrung": "30 mm",
          "Schnittleistung 45°": "51 mm",
          "Schnittleistung 90°": "67 mm",
          "Drehzahl": "5500 min-1",
          "LED-Leuchte": "mit",
          "Akku": "-- Stück",
          "Ladegerät": "--",
          "Verpackung": "T-STAK",
          "Gewicht": "3.60 kg"
        },
        "fr": {
          "tension": "18 V",
          "type d’accu": "--",
          "capacité de l’accu": "-- Ah",
          "Ø de la lame de scie": "190 mm",
          "Ø de forage de la lame de scie": "30 mm",
          "profondeur de coupe 45°": "51 mm",
          "profondeur de coupe à 90°": "67 mm",
          "vitesse": "5500 min-1",
          "lampe DEL": "avec",
          "accu": "-- pièce",
          "chargeur": "--",
          "emballage": "T-STAK",
          "poids": "3.60 kg"
        },
        "it": {
          "tensione": "18 V",
          "tipo di batteria": "--",
          "capacità batteria": "-- Ah",
          "Ø della lama": "190 mm",
          "Ø lama da sega, foro": "30 mm",
          "profondità taglio 45°": "51 mm",
          "profondità taglio a 90°": "67 mm",
          "numero di giri": "5500 min-1",
          "lampada LED": "con",
          "batteria": "-- pezzo",
          "caricabatterie": "--",
          "imballaggio": "T-STAK",
          "peso": "3.60 kg"
        }
      },
      "scraped_at": "2025-10-16T11:39:20.815164"
    },
    "DT1953-QZ": {
      "SUPPLIER_PID": "DT1953-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1953-QZ&markId=DT1953-QZ&artId=100255912&trefferUUID=A626F45F-87B8-44F1-93F95608C46F4DF7",
      "languages": {
        "de": {
          "Aussen-Ø": "216 mm",
          "Boh­rungs-Ø": "30 mm",
          "Schnitt­breite": "2.6 mm",
          "Stamm­blatt­stärke": "1.7 mm",
          "Span­winkel": "-5 °",
          "Zahn­form": "WZ"
        },
        "fr": {
          "Ø ex­té­rieur": "216 mm",
          "Ø perçage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "épaisseur du corps de lame": "1.7 mm",
          "angle": "-5 °",
          "denture": "WZ"
        },
        "it": {
          "Ø esterno": "216 mm",
          "Ø foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 °",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:29.230573"
    },
    "DT1952-QZ": {
      "SUPPLIER_PID": "DT1952-QZ",
      "product_url": "https://www.dabag.ch/?srv=search&pg=det&q=DT1952-QZ&markId=DT1952-QZ&artId=100255912&trefferUUID=06749BE5-CA8A-4501-81644005B0BBFB13",
      "languages": {
        "de": {
          "Aussen-Ø": "216 mm",
          "Boh­rungs-Ø": "30 mm",
          "Schnitt­breite": "2.6 mm",
          "Stamm­blatt­stärke": "1.7 mm",
          "Span­winkel": "-5 °",
          "Zahn­form": "WZ"
        },
        "fr": {
          "Ø ex­té­rieur": "216 mm",
          "Ø perçage": "30 mm",
          "largeur de coupe": "2.6 mm",
          "épaisseur du corps de lame": "1.7 mm",
          "angle": "-5 °",
          "denture": "WZ"
        },
        "it": {
          "Ø esterno": "216 mm",
          "Ø foro": "30 mm",
          "larghezza del taglio": "2.6 mm",
          "spessore lama": "1.7 mm",
          "angolo di spoglia superiore": "-5 °",
          "forma del dente": "WZ"
        }
      },
      "scraped_at": "2025-10-16T11:39:36.676448"
    }
  }
}
------------------------------------------------- ./BMEcat_transformer/config.py --------------------------------------------------

from __future__ import annotations

"""
Configuration settings for BMEcat_transformer.
Follows the style of existing configs in `doc_processor/` and `easy_rich/`.
"""

import os
from dotenv import load_dotenv

# Load environment variables from root .env
load_dotenv()

# Scraping method selection: "firecrawl" or "playwright"
SCRAPING_METHOD: str = os.getenv("SCRAPING_METHOD", "firecrawl").strip().lower()

# DABAG settings
DABAG_BASE_URL: str = os.getenv("DABAG_BASE_URL", "https://www.dabag.ch")

# Language mapping for DABAG (de=1, fr=2, it=3)
LANGUAGES: dict[str, int] = {
    "de": 1,
    "fr": 2,
    "it": 3,
}

# Output directory for saved JSON
OUTPUT_DIR: str = os.getenv("BME_OUTPUT_DIR", "outputs/")

# Master JSON settings
MASTER_JSON_FILENAME: str = os.getenv("MASTER_JSON_FILENAME", "master_bmecat_dabag.json")
MASTER_JSON_BACKUP_COUNT: int = int(os.getenv("MASTER_JSON_BACKUP_COUNT", "2"))

# Optional keys depending on method
FIRECRAWL_API_KEY: str | None = os.getenv("FIRECRAWL_API_KEY")

# Validation
if SCRAPING_METHOD not in {"firecrawl", "playwright"}:
    raise ValueError(
        "SCRAPING_METHOD must be either 'firecrawl' or 'playwright'"
    )

if SCRAPING_METHOD == "firecrawl" and not FIRECRAWL_API_KEY:
    raise ValueError("FIRECRAWL_API_KEY not found in environment variables for firecrawl mode")

# Normalize OUTPUT_DIR to have trailing slash
if not OUTPUT_DIR.endswith("/"):
    OUTPUT_DIR = OUTPUT_DIR + "/"


------------------------------------------------- ./BMEcat_transformer/extract_features.py --------------------------------------------------

"""Standalone script to extract unique feature names from master JSON.

Usage:
    python3 extract_features.py
"""

from __future__ import annotations

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

import config
from src.feature_extractor import FeatureExtractor


def main() -> None:
    """Run feature extraction."""
    # Construct master JSON path
    master_json_path = Path(config.OUTPUT_DIR) / config.MASTER_JSON_FILENAME
    
    if not master_json_path.exists():
        print(f"❌ Error: Master JSON not found at {master_json_path}")
        print(f"Please run the main scraper first to generate {config.MASTER_JSON_FILENAME}")
        sys.exit(1)
    
    # Run extractor
    extractor = FeatureExtractor(
        master_json_path=str(master_json_path),
        output_dir=config.OUTPUT_DIR
    )
    
    success = extractor.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

------------------------------------------------- ./BMEcat_transformer/main.py --------------------------------------------------

"""Main entry point for BMEcat_transformer.

Pipeline:
- Parse XML path from args/user input
- Extract SUPPLIER_PIDs
- For each SUPPLIER_PID, search and scrape DABAG in DE/FR/IT
- Print tables, save JSON, display summary
"""

from __future__ import annotations

import sys
import time
from pathlib import Path
from typing import Dict, Any

import config
from src.dabag_scraper import DABAGScraper
from src.output_formatter import OutputFormatter
from src.input_handler import InputHandler
from src.master_json_manager import MasterJSONManager
from src.user_prompt import UserPrompt


def get_input_path() -> str:
    """Get input file path from command-line args or prompt user.

    Returns:
        Validated path string to input file.
    """
    path: str | None = None
    if len(sys.argv) > 1:
        path = sys.argv[1]
    else:
        path = input("Enter path to BMEcat XML file: ").strip()

    if not path:
        print("⚠️ No input file path provided.")
        sys.exit(1)

    p = Path(path)
    if not p.exists() or not p.is_file():
        print(f"⚠️ Input file path does not exist or is not a file: {path}")
        sys.exit(1)
    return str(p)


def main() -> None:
    start = time.time()
    print("=" * 80)
    print("BMEcat_transformer - DABAG Spec Scraper")
    print(f"Scraping method: {config.SCRAPING_METHOD}")
    print("=" * 80)

    input_path = get_input_path()

    # Extract product IDs
    SUPPLIER_PIDs = InputHandler.load_supplier_ids(input_path)
    print(f"Found {len(SUPPLIER_PIDs)} SUPPLIER_PID(s) to process.")
    if not SUPPLIER_PIDs:
        print("Nothing to do. Exiting.")
        return

    # Initialize master JSON manager
    master_manager = MasterJSONManager(
        master_filename=config.MASTER_JSON_FILENAME,
        output_dir=config.OUTPUT_DIR,
        backup_count=config.MASTER_JSON_BACKUP_COUNT
    )
    master_manager.load()

    # Show master JSON statistics
    stats = master_manager.get_statistics()
    print(f"\n📊 Master JSON Status:")
    print(f"  Total products: {stats['total_products']}")
    print(f"  Last updated: {stats['last_updated']}")
    print()

    scraper = DABAGScraper()
    results: Dict[str, Dict[str, Any]] = {}
    
    # Counters for summary
    new_count = 0
    updated_count = 0
    skipped_count = 0

    for idx, pid in enumerate(SUPPLIER_PIDs, 1):
        print("-" * 80)
        print(f"[{idx}/{len(SUPPLIER_PIDs)}] Processing SUPPLIER_PID: {pid}")

        # Check if ID exists in master JSON
        exists, existing_data = master_manager.check_id_exists(pid)

        if exists:
            # Show existing data WITHOUT scraping
            UserPrompt.show_existing_data(pid, existing_data)

            # Ask user for decision BEFORE scraping
            decision = UserPrompt.prompt_update_decision(pid)

            if decision == "update":
                # Only scrape if user wants to update
                try:
                    new_data = scraper.process_product(pid)
                    master_manager.update_product(pid, new_data)
                    results[pid] = new_data
                    updated_count += 1
                except Exception as e:
                    print(f"⚠️  Warning: Error processing {pid}: {e}")
                    results[pid] = existing_data  # Keep existing on error
            else:
                print(f"⏭️  Skipping {pid} (keeping existing data)")
                results[pid] = existing_data
                skipped_count += 1

        else:
            # New ID - scrape and append
            try:
                data = scraper.process_product(pid)
                master_manager.append_product(pid, data)
                results[pid] = data
                new_count += 1
            except Exception as e:
                print(f"⚠️  Warning: Error processing {pid}: {e}")
                results[pid] = {"SUPPLIER_PID": pid, "product_url": None, "languages": {}}

    # Save master JSON
    master_manager.save()

    # Display results and save timestamped export
    formatter = OutputFormatter(output_dir=config.OUTPUT_DIR)
    formatter.print_results(results)
    formatter.save_to_json(results)  # Keeps timestamped export functionality
    formatter.display_summary(results)

    # Display master JSON update summary
    print("\n" + "=" * 80)
    print("Master JSON Update Summary")
    print("-" * 80)
    print(f"New products added: {new_count}")
    print(f"Products updated: {updated_count}")
    print(f"Products skipped: {skipped_count}")
    print(f"Total in master: {master_manager.get_statistics()['total_products']}")
    print("-" * 80)

    elapsed = time.time() - start
    print(f"Elapsed time: {elapsed:.2f}s")


if __name__ == "__main__":
    main()

------------------------------------------------- ./BMEcat_transformer/src/master_json_manager.py --------------------------------------------------

"""Master JSON manager for BMEcat_transformer.

Manages a persistent master JSON file that tracks all scraped products.
Provides functionality to check existence, append new entries, update existing ones,
and maintain backup versions.
"""

from __future__ import annotations

import json
import os
import shutil
from datetime import datetime
from typing import Dict, Any, Optional, Tuple


class MasterJSONManager:
    """Manage the master JSON file for scraped product data."""

    def __init__(self, master_filename: str, output_dir: str, backup_count: int = 2) -> None:
        """Initialize the master JSON manager.

        Args:
            master_filename: Name of the master JSON file.
            output_dir: Directory where master JSON is stored.
            backup_count: Number of backup versions to keep.
        """
        self.master_filename = master_filename
        self.output_dir = output_dir
        self.backup_count = backup_count
        self.master_path = os.path.join(output_dir, master_filename)
        self.data: Dict[str, Any] = {"metadata": {}, "products": {}}
        os.makedirs(output_dir, exist_ok=True)

    def load(self) -> None:
        """Load the master JSON file. Creates new if doesn't exist."""
        if os.path.exists(self.master_path):
            try:
                with open(self.master_path, "r", encoding="utf-8") as f:
                    self.data = json.load(f)
                print(f"✓ Loaded master JSON from: {self.master_path}")
            except (json.JSONDecodeError, IOError) as e:
                print(f"⚠️  Warning: Failed to load master JSON ({e}). Starting fresh.")
                self._initialize_fresh()
        else:
            print(f"✓ Master JSON not found. Starting fresh: {self.master_path}")
            self._initialize_fresh()

    def _initialize_fresh(self) -> None:
        """Initialize a fresh master JSON structure."""
        self.data = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "total_products": 0,
            },
            "products": {},
        }

    def check_id_exists(self, supplier_pid: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
        """Check if a SUPPLIER_PID exists in master JSON.

        Args:
            supplier_pid: The product ID to check.

        Returns:
            Tuple of (exists: bool, existing_data: dict or None)
        """
        exists = supplier_pid in self.data.get("products", {})
        existing_data = self.data["products"].get(supplier_pid) if exists else None
        return exists, existing_data

    def append_product(self, supplier_pid: str, product_data: Dict[str, Any]) -> None:
        """Append a new product to the master JSON.

        Args:
            supplier_pid: The product ID.
            product_data: The scraped product data.
        """
        enriched_data = product_data.copy()
        enriched_data["scraped_at"] = datetime.now().isoformat()
        self.data["products"][supplier_pid] = enriched_data
        self.data["metadata"]["total_products"] = len(self.data["products"])
        self.data["metadata"]["last_updated"] = datetime.now().isoformat()
        print(f"✓ Appended {supplier_pid} to master JSON")

    def update_product(self, supplier_pid: str, product_data: Dict[str, Any]) -> None:
        """Update an existing product in the master JSON.

        Args:
            supplier_pid: The product ID.
            product_data: The new scraped product data.
        """
        enriched_data = product_data.copy()
        enriched_data["scraped_at"] = datetime.now().isoformat()
        enriched_data["updated_at"] = datetime.now().isoformat()
        self.data["products"][supplier_pid] = enriched_data
        self.data["metadata"]["last_updated"] = datetime.now().isoformat()
        print(f"✓ Updated {supplier_pid} in master JSON")

    def save(self) -> None:
        """Save the master JSON with backup rotation."""
        self._rotate_backups()
        try:
            with open(self.master_path, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            print(f"✓ Saved master JSON to: {self.master_path}")
        except IOError as e:
            print(f"❌ Error saving master JSON: {e}")

    def _rotate_backups(self) -> None:
        """Rotate backup files, keeping only the specified number of backups."""
        if not os.path.exists(self.master_path):
            return

        # Shift existing backups
        for i in range(self.backup_count - 1, 0, -1):
            old_backup = f"{self.master_path}.backup{i}"
            new_backup = f"{self.master_path}.backup{i+1}"
            if os.path.exists(old_backup):
                if i == self.backup_count - 1:
                    # Remove oldest backup if at limit
                    if os.path.exists(new_backup):
                        os.remove(new_backup)
                shutil.move(old_backup, new_backup)

        # Create backup of current master
        backup_path = f"{self.master_path}.backup1"
        shutil.copy2(self.master_path, backup_path)

    def get_statistics(self) -> Dict[str, Any]:
        """Get statistics about the master JSON.

        Returns:
            Dictionary with statistics.
        """
        return {
            "total_products": len(self.data.get("products", {})),
            "created_at": self.data.get("metadata", {}).get("created_at", "N/A"),
            "last_updated": self.data.get("metadata", {}).get("last_updated", "N/A"),
        }

------------------------------------------------- ./BMEcat_transformer/src/xml_reader.py --------------------------------------------------

"""XML Reader for BMEcat_transformer.

Provides `XMLReader` that extracts unique `SUPPLIER_PID`s from a BMEcat XML file.
Uses multiple fallback strategies to handle malformed XML.
"""

from __future__ import annotations

from typing import List, Set
import sys
import re
from pathlib import Path
import xml.etree.ElementTree as ET


# Ensure we can import the BMEcatParser from the sibling module `doc_processor`
DOC_PROCESSOR_PATH = Path(__file__).parent.parent.parent / "doc_processor"
sys.path.append(str(DOC_PROCESSOR_PATH))
try:
    from src.bmecat_parser import BMEcatParser  # type: ignore
except Exception as e:  # pragma: no cover - import guard
    BMEcatParser = None  # Fallback if import fails; we'll handle at runtime


class XMLReader:
    """Read BMEcat XML and extract product IDs.

    Attributes:
        xml_path: Path to the BMEcat XML file.
    """

    def __init__(self, xml_path: str) -> None:
        """Initialize with the XML file path.

        Args:
            xml_path: Path to the BMEcat XML file.
        """
        self.xml_path = xml_path

    def extract_SUPPLIER_PIDs(self) -> List[str]:
        """Extract a list of unique SUPPLIER_PID strings from the XML file.

        Uses multiple strategies, prioritizing regex for malformed XML:
        1. Regex extraction (most reliable for malformed XML)
        2. Try with lxml (if available)
        3. Try standard XML parsing

        Returns:
            List of unique product IDs (order preserved by first appearance).
        """
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        # Strategy 1: Regex-based extraction (most reliable for malformed XML)
        try:
            print("🔍 Using regex-based extraction (most reliable)...")
            SUPPLIER_PIDs = self._extract_via_regex()
            if SUPPLIER_PIDs:
                print(f"✅ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via regex")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except Exception as e:
            print(f"⚠️  Regex extraction failed: {e}")

        # Strategy 2: Try lxml with recovery mode (more lenient)
        try:
            print("🔍 Attempting lxml parsing with recovery mode...")
            from lxml import etree as LXML_ET
            SUPPLIER_PIDs = self._extract_via_lxml()
            if SUPPLIER_PIDs:
                print(f"✅ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via lxml recovery")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except ImportError:
            print("⚠️  lxml not available, skipping...")
        except Exception as e:
            print(f"⚠️  lxml parsing failed: {e}")

        # Strategy 3: Try standard XML parsing
        try:
            print("🔍 Attempting standard XML parsing...")
            SUPPLIER_PIDs = self._extract_via_xml_parsing()
            if SUPPLIER_PIDs:
                print(f"✅ Successfully extracted {len(SUPPLIER_PIDs)} SUPPLIER_PIDs via XML parsing")
                print(f"   Product IDs: {', '.join(SUPPLIER_PIDs)}")
                return SUPPLIER_PIDs
        except Exception as e:
            print(f"⚠️  XML parsing failed: {e}")

        if not SUPPLIER_PIDs:
            print("⚠️  Warning: No SUPPLIER_PID elements found in the provided XML.")

        return SUPPLIER_PIDs

    def _extract_via_xml_parsing(self) -> List[str]:
        """Extract SUPPLIER_PIDs using standard XML parsing."""
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'rb') as f:
            raw = f.read()

        # Remove BOM and strip leading whitespace
        if raw.startswith(b'\xef\xbb\xbf'):
            raw = raw[3:]
        text = raw.decode('utf-8', errors='replace').lstrip()
        
        root = ET.fromstring(text)

        # Determine namespace
        if root.tag.startswith('{'):
            ns = root.tag.split('}')[0] + '}'
        else:
            ns = ''

        # Search for SUPPLIER_PID elements
        for elem in root.findall(f'.//{ns}SUPPLIER_PID'):
            if elem is not None and elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        # Also try without namespace
        for elem in root.findall('.//SUPPLIER_PID'):
            if elem is not None and elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        return SUPPLIER_PIDs

    def _extract_via_lxml(self) -> List[str]:
        """Extract SUPPLIER_PIDs using lxml with recovery mode."""
        from lxml import etree as LXML_ET
        
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'rb') as f:
            raw = f.read()

        # Create parser with recovery mode (tolerates malformed XML)
        parser = LXML_ET.XMLParser(recover=True, encoding='utf-8')
        root = LXML_ET.fromstring(raw, parser)

        # Find all SUPPLIER_PID elements
        for elem in root.xpath('.//SUPPLIER_PID | .//*[local-name()="SUPPLIER_PID"]'):
            if elem.text:
                val = elem.text.strip()
                if val and val not in seen:
                    seen.add(val)
                    SUPPLIER_PIDs.append(val)

        return SUPPLIER_PIDs

    def _extract_via_regex(self) -> List[str]:
        """Extract SUPPLIER_PIDs using regex pattern matching.
        
        This method is resilient to XML structure errors and will work
        even with malformed XML, as long as the tags exist.
        """
        SUPPLIER_PIDs: List[str] = []
        seen: Set[str] = set()

        with open(self.xml_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()

        # Pattern to match <SUPPLIER_PID>VALUE</SUPPLIER_PID>
        # Works with or without namespace, handles whitespace
        pattern = r'<(?:\w+:)?SUPPLIER_PID[^>]*>\s*([^<]+?)\s*</(?:\w+:)?SUPPLIER_PID>'
        
        matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            val = match.strip()
            # Filter out empty or whitespace-only values
            if val and not val.isspace() and val not in seen:
                seen.add(val)
                SUPPLIER_PIDs.append(val)

        return SUPPLIER_PIDs
------------------------------------------------- ./BMEcat_transformer/src/output_formatter.py --------------------------------------------------

"""Output formatter for BMEcat_transformer.

- Pretty-prints per-product, per-language specification tables to terminal.
- Displays stats for each product (specs per language)
- Saves complete results to JSON in configured output directory.
- Displays a processing summary.
"""

from __future__ import annotations

from typing import Dict, Any
import os
import time
import json
import sys
from pathlib import Path
from tabulate import tabulate  # type: ignore

# Import config from the module root (BMEcat_transformer)
MODULE_ROOT = Path(__file__).resolve().parent.parent
if str(MODULE_ROOT) not in sys.path:
    sys.path.append(str(MODULE_ROOT))
import config  # type: ignore


class OutputFormatter:
    """Format and output scraping results."""

    def __init__(self, output_dir: str | None = None) -> None:
        """Initialize with an output directory.

        Args:
            output_dir: Directory to save JSON outputs. Defaults to config.OUTPUT_DIR
        """
        self.output_dir = output_dir or config.OUTPUT_DIR
        os.makedirs(self.output_dir, exist_ok=True)

    def print_results(self, results: Dict[str, Dict[str, Any]]) -> None:
        """Print tables for each product with per-language statistics.

        Args:
            results: Mapping of SUPPLIER_PID -> {"product_url": str|None, "languages": {de|fr|it: specs dict}}
        """
        for SUPPLIER_PID, data in results.items():
            print("\n" + "=" * 80)
            print(f"Product: {SUPPLIER_PID}")
            url = data.get("product_url")
            if url:
                print(f"URL: {url}")
            print("-" * 80)

            # Build row keys from union of all labels across languages
            langs = data.get("languages", {})
            labels = set()
            for lang_dict in langs.values():
                labels.update(lang_dict.keys())
            ordered_labels = sorted(labels)

            table_rows = []
            for label in ordered_labels:
                row = [label]
                row.append(langs.get("de", {}).get(label, ""))
                row.append(langs.get("fr", {}).get(label, ""))
                row.append(langs.get("it", {}).get(label, ""))
                table_rows.append(row)

            print(tabulate(table_rows, headers=["Spec Label", "DE", "FR", "IT"], tablefmt="grid"))
            
            # Add per-product statistics
            de_specs = len([v for v in langs.get("de", {}).values() if v and v.strip()])
            fr_specs = len([v for v in langs.get("fr", {}).values() if v and v.strip()])
            it_specs = len([v for v in langs.get("it", {}).values() if v and v.strip()])
            
            print(f"\n📊 Stats: DE: {de_specs} specs | FR: {fr_specs} specs | IT: {it_specs} specs")

    def save_to_json(self, results: Dict[str, Dict[str, Any]], filename: str | None = None) -> str:
        """Save results to a timestamped JSON file in the output directory.

        Args:
            results: The complete results dict.
            filename: Optional base filename without extension.

        Returns:
            The full path of the saved JSON file.
        """
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"bmecat_dabag_results_{timestamp}.json"

        if not filename.endswith(".json"):
            filename += ".json"

        filepath = os.path.join(self.output_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"Saved JSON: {filepath}")
        return filepath

    def display_summary(self, results: Dict[str, Dict[str, Any]]) -> None:
        """Display a summary of the scraping session.

        Args:
            results: The complete results dict.
        """
        total_products = len(results)
        
        # Count total unique specs per language across all products
        all_de_specs = set()
        all_fr_specs = set()
        all_it_specs = set()
        
        for data in results.values():
            langs = data.get("languages", {})
            all_de_specs.update(langs.get("de", {}).keys())
            all_fr_specs.update(langs.get("fr", {}).keys())
            all_it_specs.update(langs.get("it", {}).keys())

        print("\n" + "=" * 80)
        print("Summary")
        print("-" * 80)
        print(f"Total products processed: {total_products}")
        print(f"Total unique spec fields - DE: {len(all_de_specs)}, FR: {len(all_fr_specs)}, IT: {len(all_it_specs)}")
        print("-" * 80)
------------------------------------------------- ./BMEcat_transformer/src/__init__.py --------------------------------------------------

"""BMEcat_transformer package."""

------------------------------------------------- ./BMEcat_transformer/src/user_prompt.py --------------------------------------------------

"""User prompt handler for BMEcat_transformer.

Handles user interaction when an existing product is found in master JSON.
Shows existing data and prompts for update decision.
"""

from __future__ import annotations

from typing import Dict, Any, Literal


class UserPrompt:
    """Handle user prompts and display existing data."""

    @staticmethod
    def show_existing_data(supplier_pid: str, existing_data: Dict[str, Any]) -> None:
        """Display existing data from master JSON.

        Args:
            supplier_pid: The product ID.
            existing_data: Data currently in master JSON.
        """
        print("\n" + "=" * 80)
        print(f"🔍 Product {supplier_pid} already exists in master JSON")
        print("-" * 80)

        # Show timestamps
        scraped_at = existing_data.get("scraped_at", "N/A")
        updated_at = existing_data.get("updated_at", scraped_at)
        print(f"Last scraped: {scraped_at}")
        if updated_at != scraped_at:
            print(f"Last updated: {updated_at}")

        # Show language coverage
        existing_langs = existing_data.get("languages", {})
        print("\nLanguage Coverage:")
        for lang in ["de", "fr", "it"]:
            spec_count = len(existing_langs.get(lang, {}))
            print(f"  {lang.upper()}: {spec_count} specs")

        # Show URL
        existing_url = existing_data.get("product_url", "N/A")
        print(f"\nURL: {existing_url}")

        print("-" * 80)

    @staticmethod
    def prompt_update_decision(supplier_pid: str) -> Literal["update", "skip"]:
        """Prompt user to decide whether to update or skip.

        Args:
            supplier_pid: The product ID.

        Returns:
            'update' if user wants to update, 'skip' otherwise.
        """
        while True:
            response = input(f"Update {supplier_pid}? [y/n]: ").strip().lower()
            if response in ["y", "yes"]:
                return "update"
            elif response in ["n", "no"]:
                return "skip"
            else:
                print("⚠️  Invalid input. Please enter 'y' or 'n'.")

------------------------------------------------- ./BMEcat_transformer/src/feature_extractor.py --------------------------------------------------

"""Feature extractor for BMEcat_transformer.

Extracts unique feature names (fname) and example values (fvalue) from 
master_bmecat_dabag.json across all languages (de, fr, it).
Creates a 6-column matrix for feature name mapping reference.
"""

from __future__ import annotations

import json
import csv
import os
from typing import Dict, List, Tuple, Any
from pathlib import Path


class FeatureExtractor:
    """Extract and export unique feature names from master JSON."""

    def __init__(self, master_json_path: str, output_dir: str) -> None:
        """Initialize with paths.

        Args:
            master_json_path: Path to master_bmecat_dabag.json
            output_dir: Directory to save output CSV
        """
        self.master_json_path = master_json_path
        self.output_dir = output_dir
        self.data: Dict[str, Any] = {}
        
        # Store unique features as: {(fname_de, fname_fr, fname_it): (fvalue_de, fvalue_fr, fvalue_it)}
        self.unique_features: Dict[Tuple[str, str, str], Tuple[str, str, str]] = {}

    def load_master_json(self) -> bool:
        """Load the master JSON file.

        Returns:
            True if successful, False otherwise.
        """
        try:
            with open(self.master_json_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"✅ Loaded master JSON: {len(self.data.get('products', {}))} products")
            return True
        except FileNotFoundError:
            print(f"❌ Error: Master JSON not found at {self.master_json_path}")
            return False
        except json.JSONDecodeError as e:
            print(f"❌ Error: Invalid JSON: {e}")
            return False

    def extract_features(self) -> None:
        """Extract unique features from all products.
        
        Matches features by position index across languages.
        """
        products = self.data.get('products', {})
        
        for pid, product_data in products.items():
            languages = product_data.get('languages', {})
            
            de_features = languages.get('de', {})
            fr_features = languages.get('fr', {})
            it_features = languages.get('it', {})
            
            # Convert to lists to maintain order
            de_items = list(de_features.items())
            fr_items = list(fr_features.items())
            it_items = list(it_features.items())
            
            # Match by position index
            max_len = max(len(de_items), len(fr_items), len(it_items))
            
            for i in range(max_len):
                fname_de = de_items[i][0] if i < len(de_items) else ""
                fvalue_de = de_items[i][1] if i < len(de_items) else ""
                
                fname_fr = fr_items[i][0] if i < len(fr_items) else ""
                fvalue_fr = fr_items[i][1] if i < len(fr_items) else ""
                
                fname_it = it_items[i][0] if i < len(it_items) else ""
                fvalue_it = it_items[i][1] if i < len(it_items) else ""
                
                # Create unique key and store
                feature_key = (fname_de, fname_fr, fname_it)
                
                # Skip if all empty
                if not any([fname_de, fname_fr, fname_it]):
                    continue
                
                # Store first occurrence as example
                if feature_key not in self.unique_features:
                    self.unique_features[feature_key] = (fvalue_de, fvalue_fr, fvalue_it)
        
        print(f"✅ Extracted {len(self.unique_features)} unique feature mappings")

    def export_to_csv(self, filename: str = "unique_features.csv") -> str:
        """Export unique features to CSV file.

        Args:
            filename: Output CSV filename.

        Returns:
            Full path to the saved CSV file.
        """
        os.makedirs(self.output_dir, exist_ok=True)
        output_path = os.path.join(self.output_dir, filename)
        
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # Write header
            writer.writerow([
                'fname_de', 'fvalue_de',
                'fname_fr', 'fvalue_fr',
                'fname_it', 'fvalue_it'
            ])
            
            # Write data rows
            for feature_key, feature_values in sorted(self.unique_features.items()):
                row = [
                    feature_key[0], feature_values[0],  # de
                    feature_key[1], feature_values[1],  # fr
                    feature_key[2], feature_values[2]   # it
                ]
                writer.writerow(row)
        
        print(f"✅ Saved CSV: {output_path}")
        return output_path

    def run(self) -> bool:
        """Run the complete feature extraction pipeline.

        Returns:
            True if successful, False otherwise.
        """
        print("="*80)
        print("Feature Extractor - Unique Feature Name Database")
        print("="*80)
        
        if not self.load_master_json():
            return False
        
        self.extract_features()
        self.export_to_csv()
        
        print("="*80)
        print("✅ Feature extraction complete!")
        print("="*80)
        
        return True

------------------------------------------------- ./BMEcat_transformer/src/dabag_scraper.py --------------------------------------------------

"""DABAG scraper for BMEcat_transformer.

Selects scraping backend based on config (firecrawl or playwright) to locate
product pages, then fetches product page HTML and extracts specification tables
for multiple languages.
"""

from __future__ import annotations

from typing import Dict, Optional
import sys
from pathlib import Path
from urllib.parse import urljoin
import requests

# Import config from the module root (BMEcat_transformer)
MODULE_ROOT = Path(__file__).resolve().parent.parent
if str(MODULE_ROOT) not in sys.path:
    sys.path.append(str(MODULE_ROOT))
import config  # type: ignore

# Import appropriate scraper based on config
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

if config.SCRAPING_METHOD == "firecrawl":
    from easy_rich.src.web_scraper import WebScraper  # type: ignore
elif config.SCRAPING_METHOD == "playwright":
    from manual_scrape.src.web_scraper import WebScraper  # type: ignore
else:  # Defensive
    from easy_rich.src.web_scraper import WebScraper  # type: ignore

# Local imports
from .table_extractor import TableExtractor


class DABAGScraper:
    """Search and scrape DABAG product pages in multiple languages."""

    def __init__(self) -> None:
        """Initialize scraper backend and table extractor."""
        try:
            self.scraper = WebScraper()
        except Exception as e:
            print(f"⚠️ Warning: Failed to initialize scraper backend: {e}")
            self.scraper = None
        self.table_extractor = TableExtractor()

    def search_product(self, SUPPLIER_PID: str) -> Optional[str]:
        """Search DABAG for a product and return the product detail URL.

        Args:
            SUPPLIER_PID: The product identifier to search.

        Returns:
            Full URL to the product detail page, or None if not found.
        """
        try:
            search_url = f"{config.DABAG_BASE_URL}/?q={SUPPLIER_PID}&srv=search"
            if not self.scraper:
                print("⚠️ Warning: Scraper not initialized; cannot perform search.")
                return None

            result = self.scraper.scrape_page(search_url)
            if not result:
                print(f"⚠️ Warning: Search failed for SUPPLIER_PID={SUPPLIER_PID}")
                return None

            markdown = result.get("markdown_content", "")
            # Reuse link extractor from the backend if available
            try:
                links = self.scraper.extract_links_from_markdown(markdown, search_url)  # type: ignore[attr-defined]
            except Exception:
                links = []

            # Find product detail link with expected pattern
            target: Optional[str] = None
            for _title, href in links:
                if "srv=search&pg=det&q=" in href:
                    target = href
                    break

            if not target:
                print(f"⚠️ Warning: Product detail link not found for SUPPLIER_PID={SUPPLIER_PID}")
                return None

            # Ensure full URL
            full_url = target if target.startswith("http") else urljoin(config.DABAG_BASE_URL, target)
            return full_url

        except Exception as e:
            print(f"⚠️ Warning: Error during product search for {SUPPLIER_PID}: {e}")
            return None

    def scrape_product_languages(self, base_url: str, SUPPLIER_PID: str) -> Dict[str, Dict[str, str]]:
        """Scrape product specs for all configured languages.

        Args:
            base_url: The base product detail page URL (likely in default language).
            SUPPLIER_PID: The product identifier for logging only.

        Returns:
            Mapping of language code (de/fr/it) to specs dict.
        """
        results: Dict[str, Dict[str, str]] = {}
        headers = {"User-Agent": "Mozilla/5.0 (compatible; BMEcatTransformer/1.0)"}

        for lang_code, lang_id in config.LANGUAGES.items():
            try:
                url = f"{base_url}&&&lngId={lang_id}"
                resp = requests.get(url, headers=headers, timeout=30)
                if resp.status_code >= 400:
                    print(f"⚠️ Warning: {lang_code.upper()} page HTTP {resp.status_code} for {SUPPLIER_PID}")
                    results[lang_code] = {}
                    continue

                html = resp.text
                specs = self.table_extractor.extract_specs_table(html)
                results[lang_code] = specs
            except Exception as e:
                print(f"⚠️ Warning: Failed to scrape {lang_code.upper()} for {SUPPLIER_PID}: {e}")
                results[lang_code] = {}

        return results

    def process_product(self, SUPPLIER_PID: str) -> Dict[str, object]:
        """Search and scrape a single product across languages.

        Returns a dict with structure:
        {
            "SUPPLIER_PID": str,
            "product_url": str | None,
            "languages": {"de": {...}, "fr": {...}, "it": {...}}
        }
        """
        product_url = self.search_product(SUPPLIER_PID)
        lang_data: Dict[str, Dict[str, str]] = {}
        if product_url:
            lang_data = self.scrape_product_languages(product_url, SUPPLIER_PID)
        else:
            print(f"⚠️ Warning: Skipping language scrape; no product URL for {SUPPLIER_PID}")

        return {
            "SUPPLIER_PID": SUPPLIER_PID,
            "product_url": product_url,
            "languages": lang_data,
        }


------------------------------------------------- ./BMEcat_transformer/src/input_handler.py --------------------------------------------------

"""Input handler for BMEcat_transformer.

Provides `InputHandler` that auto-detects and loads SUPPLIER_PIDs from
supported input files (XML or JSON).
"""

from __future__ import annotations

from pathlib import Path
from typing import List
import json

from src.xml_reader import XMLReader


class InputHandler:
    """Handle loading SUPPLIER_PIDs from XML or JSON input files.

    This is a stateless utility class that provides a single entry point to
    load product identifiers for downstream processing.
    """

    @staticmethod
    def load_supplier_ids(file_path: str) -> List[str]:
        """Auto-detect file type and return list of SUPPLIER_PIDs.

        Args:
            file_path: Path to input file (.xml or .json).

        Returns:
            List of unique SUPPLIER_PID strings.

        Raises:
            ValueError: If the file does not exist, has an unsupported type,
                or contains invalid JSON/invalid content.
        """
        path = Path(file_path)
        if not path.exists():
            raise ValueError(f"Input file does not exist: {file_path}")

        suffix = path.suffix.lower()
        if suffix == ".xml":
            print("🔍 Detected XML file, extracting SUPPLIER_PIDs...")
            reader = XMLReader(str(path))
            ids = reader.extract_SUPPLIER_PIDs()
            # Ensure uniqueness and cleanliness
            seen = set()
            cleaned: List[str] = []
            for val in ids:
                v = val.strip()
                if v and v not in seen:
                    seen.add(v)
                    cleaned.append(v)
            print(f"✅ Successfully loaded {len(cleaned)} SUPPLIER_PID(s) from XML")
            return cleaned
        elif suffix == ".json":
            print("🔍 Detected JSON file, loading SUPPLIER_PIDs...")
            ids = InputHandler._load_from_json(str(path))
            print(f"✅ Successfully loaded {len(ids)} SUPPLIER_PID(s) from JSON")
            return ids
        else:
            raise ValueError(
                f"Unsupported input format '{suffix}'. Only .xml and .json are supported."
            )

    @staticmethod
    def _load_from_json(file_path: str) -> List[str]:
        """Load SUPPLIER_PIDs from a JSON array file.

        The JSON file must contain a simple array of strings, e.g.:
        ["ID1", "ID2", "ID3"]

        Args:
            file_path: Path to the JSON file.

        Returns:
            List of unique SUPPLIER_PID strings.

        Raises:
            ValueError: If JSON is invalid or content fails validation.
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            print(f"⚠️  Invalid JSON: {e}")
            raise ValueError(f"Invalid JSON in file: {file_path}") from e

        # Validate structure: must be a list
        if not isinstance(data, list):
            print("⚠️  JSON must be an array of strings, e.g. [\"ID1\", \"ID2\"]")
            raise ValueError("JSON must be an array of strings")

        # Validate elements are strings
        if not all(isinstance(x, str) for x in data):
            print("⚠️  All elements in the JSON array must be strings")
            raise ValueError("All elements in the JSON array must be strings")

        # Clean, strip, and filter out empty strings; enforce uniqueness preserving order
        seen = set()
        cleaned: List[str] = []
        for x in data:
            v = x.strip()
            if v and v not in seen:
                seen.add(v)
                cleaned.append(v)

        if not cleaned:
            print("⚠️  JSON array is empty or contains only empty/whitespace strings")
            raise ValueError("JSON must contain at least one non-empty string")

        return cleaned

------------------------------------------------- ./BMEcat_transformer/src/table_extractor.py --------------------------------------------------

"""Table extractor for BMEcat_transformer.

Parses DABAG product pages' specification table into a dictionary mapping
label -> value. Designed to be resilient to minor HTML variations.
"""

from __future__ import annotations

from typing import Dict
from bs4 import BeautifulSoup  # type: ignore


class TableExtractor:
    """Extract specification tables from DABAG HTML content.

    Methods:
        extract_specs_table(html_content): Return dict of spec label -> value.
    """

    def __init__(self) -> None:
        """Initialize the extractor (no state required)."""
        pass

    def extract_specs_table(self, html_content: str) -> Dict[str, str]:
        """Extract specifications table from given HTML.

        The table is expected to have class
        "w-100 table table-striped m-0". Each row should contain two
        <td> cells: label and value.

        Args:
            html_content: Full HTML content of the product page.

        Returns:
            Dictionary mapping label -> value. Empty if table not found
            or on parse errors.
        """
        specs: Dict[str, str] = {}
        try:
            soup = BeautifulSoup(html_content or "", "html.parser")
            table = soup.find("table", class_="w-100 table table-striped m-0")
            if table is None:
                # Attempt a broader match if exact class chain changes order
                # or is partially applied by the site.
                candidate_tables = soup.find_all("table")
                for t in candidate_tables:
                    classes = set((t.get("class") or []))
                    needed = {"w-100", "table", "table-striped", "m-0"}
                    if needed.issubset(classes):
                        table = t
                        break

            if table is None:
                print("⚠️ Warning: Specification table not found in HTML.")
                return specs

            for tr in table.find_all("tr"):
                tds = tr.find_all("td")
                if len(tds) < 2:
                    continue
                label = (tds[0].get_text(strip=True) or "").strip()
                value = (tds[1].get_text(strip=True) or "").strip()
                if label:
                    specs[label] = value

        except Exception as e:
            print(f"⚠️ Warning: Failed to parse specification table: {e}")
            return {}

        return specs

