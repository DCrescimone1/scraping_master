Folder structure:
.
├── easy_rich
│   ├── main.py
│   ├── README.md
│   └── src
│       ├── __init__.py
│       ├── serp_api_client.py
│       └── web_scraper.py
├── ninja_assassin_results.html
├── project_dump.sh
├── requirements.txt
├── scraping_master_dump.txt
├── toy_story_2_results_data.json
└── toy_story_2_results.md

3 directories, 11 files


--- File Contents ---


------------------------------------------------- ./.env --------------------------------------------------

SERP_API_KEY=47c6cb52b4141c5f94ff57a3eece11d84cb6990ddd18486002c941f6f89aeb9b
FIRECRAWL_API_KEY=fc-b2d74ba4c3f64eb38a6569d0db1d1c49

------------------------------------------------- ./easy_rich/main.py --------------------------------------------------

#!/usr/bin/env python3
"""
Generic Web Scraper
Searches for user input on the web and scrapes the page content.
"""

from src.serp_api_client import SerpAPIClient
from src.web_scraper import WebScraper


def main() -> None:
    """Main application entry point."""
    print("Starting Generic Web Scraper...")

    # Collect user input
    print("\n--- Generic Web Scraper ---")
    search_text = input("Enter search text (required): ").strip()
    website = input("Enter website to search on (optional, e.g., 'bbc.com'): ").strip()

    # Validate input
    if not search_text:
        print("Error: Search text cannot be empty!")
        return

    website = website if website else None
    print(f"\nSearching for '{search_text}'" + (f" on {website}" if website else " on the web"))

    # Initialize clients
    try:
        serp_client = SerpAPIClient()
        web_scraper = WebScraper()

        print("Searching the web...")

        # Search the web
        search_results = serp_client.search_web(search_text, website)

        if not search_results:
            print("Failed to get search results")
            return

        # Extract target URL
        target_url = serp_client.extract_first_url(search_results, website)

        if not target_url:
            print("No relevant URL found in search results")
            return

        print(f"Found URL: {target_url}")

        # Scrape the page
        print("Scraping page content...")
        scraped_content = web_scraper.scrape_page(target_url)

        if scraped_content:
            print(f"Successfully scraped: {scraped_content['title']}")

            # Save content
            filename = f"{search_text.replace(' ', '_').replace('/', '_')}_results"
            web_scraper.save_content(scraped_content, filename)

            print("Scraping completed successfully!")
        else:
            print("Failed to scrape page content")

    except Exception as e:
        print(f"Application error: {e}")


if __name__ == "__main__":
    main()

------------------------------------------------- ./easy_rich/src/web_scraper.py --------------------------------------------------

from typing import Optional, Dict

import os
from firecrawl import Firecrawl
from dotenv import load_dotenv


class WebScraper:
    """Web scraper for extracting content from web pages."""

    def __init__(self) -> None:
        """Initialize Firecrawl client."""
        load_dotenv()
        api_key = os.getenv("FIRECRAWL_API_KEY")
        if not api_key:
            raise ValueError("FIRECRAWL_API_KEY not found in environment variables")
        self.firecrawl = Firecrawl(api_key=api_key)

    def scrape_page(self, url: str) -> Optional[Dict[str, str]]:
        """
        Scrape content from a given URL using Firecrawl.

        Args:
            url: URL to scrape.

        Returns:
            A dict containing metadata, markdown content, and structured data, or None if the request fails.
        """
        try:
            result = self.firecrawl.scrape(
                url,
                formats=[
                    "markdown",
                    {
                        "type": "json",
                        "prompt": "Extract key information from this page including title, main content summary, key points, and any important data like prices, dates, or contact information.",
                    },
                ],
            )

            # Access attributes from Firecrawl Document object safely
            has_metadata = hasattr(result, "metadata") and result.metadata is not None
            title = getattr(result.metadata, "title", "No title found") if has_metadata else "No title found"
            status_code = getattr(result.metadata, "statusCode", "Unknown") if has_metadata else "Unknown"
            markdown_content = getattr(result, "markdown", "")
            structured_data = getattr(result, "json", {})

            return {
                "url": url,
                "title": title,
                "markdown_content": markdown_content,
                "structured_data": structured_data,
                "status_code": str(status_code),
            }

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error scraping {url}: {e}")
            return None

    

    def save_content(self, content: Dict[str, str], filename: str = "scraped_content") -> None:
        """
        Save scraped content to both markdown and JSON files.

        Args:
            content: Scraped content dict.
            filename: Base filename (without extension).
        """
        try:
            # Save markdown content
            md_filename = f"{filename}.md"
            with open(md_filename, "w", encoding="utf-8") as f:
                f.write(f"# {content['title']}\n\n")
                f.write(f"**Source:** {content['url']}\n\n")
                f.write(content.get("markdown_content", ""))
            print(f"Markdown content saved to {md_filename}")

            # Save structured data if available
            if content.get("structured_data") is not None:
                json_filename = f"{filename}_data.json"
                import json
                with open(json_filename, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "url": content.get("url", ""),
                            "title": content.get("title", ""),
                            "status_code": content.get("status_code", ""),
                            "structured_data": content.get("structured_data", {}),
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )
                print(f"Structured data saved to {json_filename}")

        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error saving content: {e}")


------------------------------------------------- ./easy_rich/src/serp_api_client.py --------------------------------------------------

"""
Client for handling SerpAPI search requests.
"""

import os
from typing import Dict, Optional, List

import requests
from dotenv import load_dotenv


class SerpAPIClient:
    """Client for handling SerpAPI search requests."""

    def __init__(self) -> None:
        """Initialize client by loading environment variables and base config."""
        # Load environment variables from .env (searched from CWD upward)
        load_dotenv()
        self.api_key: Optional[str] = os.getenv("SERP_API_KEY")
        self.base_url: str = "https://serpapi.com/search.json"

        if not self.api_key:
            raise ValueError("SERP_API_KEY not found in environment variables")

    def search_web(self, query: str, website: Optional[str] = None) -> Optional[Dict]:
        """
        Search the web using SerpAPI with optional site restriction.

        Args:
            query: Search query (e.g., "ninja assassin").
            website: Optional website to restrict search to (e.g., "imdb.com").

        Returns:
            The parsed JSON response as a dict, or None if the request failed.
        """
        # Construct search query
        if website:
            search_query = f"site:{website} {query}"
        else:
            search_query = query

        params: Dict[str, str | int] = {
            "engine": "google",
            "q": search_query,
            "api_key": self.api_key or "",
            "num": 10,
        }

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"Error making API request: {e}")
            return None

    def extract_first_url(self, search_results: Dict, website: Optional[str] = None) -> Optional[str]:
        """
        Extract the first relevant URL from search results.

        Args:
            search_results: SerpAPI response payload.
            website: Optional website domain to filter by.

        Returns:
            First relevant URL if found, otherwise None.
        """
        try:
            organic_results: List[Dict] = search_results.get("organic_results", [])  # type: ignore[assignment]

            for result in organic_results:
                url: str = result.get("link", "")
                if website:
                    if website.lower() in url.lower():
                        return url
                else:
                    if url:
                        return url

            return None
        except Exception as e:  # noqa: BLE001 - broad except with logging for robustness
            print(f"Error extracting URL: {e}")
            return None
